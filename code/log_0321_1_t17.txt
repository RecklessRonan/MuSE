nohup: ignoring input
03/21/2023 11:30:45 - INFO - __main__ -   label_list: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[START]', '[END]'], length: 12
The number of samples: 3373
The number of images: 3373
The number of samples: 723
The number of images: 723
The number of samples: 723
The number of images: 723
03/21/2023 11:30:45 - INFO - root -   Constructing vocabulary for image caption
03/21/2023 11:30:45 - INFO - __main__ -    The size of vocabulary = 3524
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/21/2023 11:31:04 - INFO - __main__ -   Args: Namespace(adam_epsilon=1e-08, alpha=0.0001, bert_type='uncased', beta=0.0001, cls_init=0, crf_dropout=0.5, crf_learning_rate=0.0001, crop_size=224, cross_dropout=0.2, data_dir='../data/twitter2017', device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, drop_last=True, eval_all_checkpoints=False, evaluate_during_training=True, gradient_accumulation_steps=1, hidden_size=768, id2label={0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC', 9: 'X', 10: '[START]', 11: '[END]'}, image_dir='../data/twitter2017_images', image_dropout=0.0, label2id={'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'X': 9, '[START]': 10, '[END]': 11}, learning_rate=0.0001, load_image_checkpoint=False, load_text_checkpoint=False, local_rank=-1, logging_steps=100, markup='bio', max_grad_norm=1.0, max_seq_length=64, n_gpu=1, num_layers=6, num_train_epochs=10, num_workers=8, output_dir='../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/', per_gpu_eval_batch_size=40, per_gpu_train_batch_size=40, predict_checkpoints=0, replace_end=3, replace_start=1, resnet_pretrained_dir='../models/resnet152-b121ed2d.pth', save_steps=100, seed=42, sigma=1.0, skip_connection=True, task='twitter17', test_batch_size=1, text_dropout=0.0, theta=0.1, ti_crop_size=32, train_batch_size=40, use_quantile=True, use_xlmr=False, warmup_proportion=0.1, weight_decay=0.01)
03/21/2023 11:31:04 - INFO - __main__ -   Summary dir: ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/summary_1679369464
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
03/21/2023 11:31:04 - INFO - __main__ -   ***** Running training *****
03/21/2023 11:31:04 - INFO - __main__ -     Num examples = 3373
03/21/2023 11:31:04 - INFO - __main__ -     Num Epochs = 10
03/21/2023 11:31:04 - INFO - __main__ -     Gradient Accumulation steps = 1
03/21/2023 11:31:04 - INFO - __main__ -     Total optimization steps = 840

Epoch: 0/10
/home/ubuntu/multimodal-fusion/MuSE/code/models.py:1235: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[Training] 1/84 [..............................] - ETA: 5:27  [ loss=0.9246 ][Training] 2/84 [..............................] - ETA: 4:26  [ loss=0.8644 ][Training] 3/84 [>.............................] - ETA: 3:45  [ loss=0.8212 ][Training] 4/84 [>.............................] - ETA: 3:32  [ loss=0.8370 ][Training] 5/84 [>.............................] - ETA: 3:19  [ loss=0.8017 ][Training] 6/84 [=>............................] - ETA: 3:08  [ loss=0.6879 ][Training] 7/84 [=>............................] - ETA: 2:59  [ loss=0.5904 ][Training] 8/84 [=>............................] - ETA: 2:52  [ loss=0.4626 ][Training] 9/84 [==>...........................] - ETA: 2:46  [ loss=0.4870 ][Training] 10/84 [==>...........................] - ETA: 2:41  [ loss=0.3826 ][Training] 11/84 [==>...........................] - ETA: 2:37  [ loss=0.3814 ][Training] 12/84 [===>..........................] - ETA: 2:33  [ loss=0.5147 ][Training] 13/84 [===>..........................] - ETA: 2:29  [ loss=0.4040 ][Training] 14/84 [====>.........................] - ETA: 2:26  [ loss=0.4291 ][Training] 15/84 [====>.........................] - ETA: 2:23  [ loss=0.4392 ][Training] 16/84 [====>.........................] - ETA: 2:20  [ loss=0.4336 ][Training] 17/84 [=====>........................] - ETA: 2:17  [ loss=0.3931 ][Training] 18/84 [=====>........................] - ETA: 2:14  [ loss=0.4037 ][Training] 19/84 [=====>........................] - ETA: 2:11  [ loss=0.3687 ][Training] 20/84 [======>.......................] - ETA: 2:08  [ loss=0.3900 ][Training] 21/84 [======>.......................] - ETA: 2:06  [ loss=0.4002 ][Training] 22/84 [======>.......................] - ETA: 2:03  [ loss=0.3715 ][Training] 23/84 [=======>......................] - ETA: 2:01  [ loss=0.3724 ][Training] 24/84 [=======>......................] - ETA: 1:59  [ loss=0.3338 ][Training] 25/84 [=======>......................] - ETA: 1:56  [ loss=0.4201 ][Training] 26/84 [========>.....................] - ETA: 1:54  [ loss=0.3883 ][Training] 27/84 [========>.....................] - ETA: 1:52  [ loss=0.3782 ][Training] 28/84 [=========>....................] - ETA: 1:50  [ loss=0.3754 ][Training] 29/84 [=========>....................] - ETA: 1:48  [ loss=0.3406 ][Training] 30/84 [=========>....................] - ETA: 1:46  [ loss=0.3406 ][Training] 31/84 [==========>...................] - ETA: 1:43  [ loss=0.3474 ][Training] 32/84 [==========>...................] - ETA: 1:41  [ loss=0.3849 ][Training] 33/84 [==========>...................] - ETA: 1:39  [ loss=0.3429 ][Training] 34/84 [===========>..................] - ETA: 1:37  [ loss=0.3952 ][Training] 35/84 [===========>..................] - ETA: 1:35  [ loss=0.2807 ][Training] 36/84 [===========>..................] - ETA: 1:33  [ loss=0.2823 ][Training] 37/84 [============>.................] - ETA: 1:30  [ loss=0.3253 ][Training] 38/84 [============>.................] - ETA: 1:28  [ loss=0.2298 ][Training] 39/84 [============>.................] - ETA: 1:26  [ loss=0.2384 ][Training] 40/84 [=============>................] - ETA: 1:24  [ loss=0.2380 ][Training] 41/84 [=============>................] - ETA: 1:22  [ loss=0.2464 ][Training] 42/84 [==============>...............] - ETA: 1:20  [ loss=0.2120 ][Training] 43/84 [==============>...............] - ETA: 1:18  [ loss=0.1975 ][Training] 44/84 [==============>...............] - ETA: 1:16  [ loss=0.1632 ][Training] 45/84 [===============>..............] - ETA: 1:14  [ loss=0.2104 ][Training] 46/84 [===============>..............] - ETA: 1:12  [ loss=0.1782 ][Training] 47/84 [===============>..............] - ETA: 1:10  [ loss=0.1912 ][Training] 48/84 [================>.............] - ETA: 1:08  [ loss=0.1947 ][Training] 49/84 [================>.............] - ETA: 1:06  [ loss=0.1659 ][Training] 50/84 [================>.............] - ETA: 1:04  [ loss=0.1824 ][Training] 51/84 [=================>............] - ETA: 1:03  [ loss=0.1576 ][Training] 52/84 [=================>............] - ETA: 1:01  [ loss=0.1531 ][Training] 53/84 [=================>............] - ETA: 59s  [ loss=0.1387 ][Training] 54/84 [==================>...........] - ETA: 57s  [ loss=0.1062 ][Training] 55/84 [==================>...........] - ETA: 55s  [ loss=0.1282 ][Training] 56/84 [===================>..........] - ETA: 53s  [ loss=0.1353 ][Training] 57/84 [===================>..........] - ETA: 51s  [ loss=0.1207 ][Training] 58/84 [===================>..........] - ETA: 49s  [ loss=0.1348 ][Training] 59/84 [====================>.........] - ETA: 47s  [ loss=0.1131 ][Training] 60/84 [====================>.........] - ETA: 45s  [ loss=0.1263 ][Training] 61/84 [====================>.........] - ETA: 43s  [ loss=0.1087 ][Training] 62/84 [=====================>........] - ETA: 41s  [ loss=0.1571 ][Training] 63/84 [=====================>........] - ETA: 39s  [ loss=0.1403 ][Training] 64/84 [=====================>........] - ETA: 38s  [ loss=0.1224 ][Training] 65/84 [======================>.......] - ETA: 36s  [ loss=0.1383 ][Training] 66/84 [======================>.......] - ETA: 34s  [ loss=0.1154 ][Training] 67/84 [======================>.......] - ETA: 32s  [ loss=0.1036 ][Training] 68/84 [=======================>......] - ETA: 30s  [ loss=0.1125 ][Training] 69/84 [=======================>......] - ETA: 28s  [ loss=0.0848 ][Training] 70/84 [========================>.....] - ETA: 26s  [ loss=0.1426 ][Training] 71/84 [========================>.....] - ETA: 24s  [ loss=0.0823 ][Training] 72/84 [========================>.....] - ETA: 22s  [ loss=0.0921 ][Training] 73/84 [=========================>....] - ETA: 20s  [ loss=0.1095 ][Training] 74/84 [=========================>....] - ETA: 18s  [ loss=0.1156 ][Training] 75/84 [=========================>....] - ETA: 16s  [ loss=0.1012 ][Training] 76/84 [==========================>...] - ETA: 15s  [ loss=0.1204 ][Training] 77/84 [==========================>...] - ETA: 13s  [ loss=0.0938 ][Training] 78/84 [==========================>...] - ETA: 11s  [ loss=0.0944 ][Training] 79/84 [===========================>..] - ETA: 9s  [ loss=0.1094 ][Training] 80/84 [===========================>..] - ETA: 7s  [ loss=0.0835 ][Training] 81/84 [===========================>..] - ETA: 5s  [ loss=0.0905 ][Training] 82/84 [============================>.] - ETA: 3s  [ loss=0.0723 ][Training] 83/84 [============================>.] - ETA: 1s  [ loss=0.1021 ][Training] 84/84 [==============================] 1.9s/step  [ loss=0.0979 ]
03/21/2023 11:33:42 - INFO - __main__ -   


Epoch: 1/10
[Training] 1/84 [..............................] - ETA: 4:17  [ loss=0.0971 ][Training] 2/84 [..............................] - ETA: 3:30  [ loss=0.0893 ][Training] 3/84 [>.............................] - ETA: 3:08  [ loss=0.0738 ][Training] 4/84 [>.............................] - ETA: 2:56  [ loss=0.1003 ][Training] 5/84 [>.............................] - ETA: 2:51  [ loss=0.0950 ][Training] 6/84 [=>............................] - ETA: 2:45  [ loss=0.0890 ][Training] 7/84 [=>............................] - ETA: 2:40  [ loss=0.0848 ][Training] 8/84 [=>............................] - ETA: 2:35  [ loss=0.0685 ][Training] 9/84 [==>...........................] - ETA: 2:31  [ loss=0.0875 ][Training] 10/84 [==>...........................] - ETA: 2:28  [ loss=0.0648 ][Training] 11/84 [==>...........................] - ETA: 2:25  [ loss=0.0828 ][Training] 12/84 [===>..........................] - ETA: 2:22  [ loss=0.0683 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0748 ][Training] 14/84 [====>.........................] - ETA: 2:17  [ loss=0.0863 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0590 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0776 ] 
03/21/2023 11:34:14 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:34:14 - INFO - __main__ -     Num examples = 723
03/21/2023 11:34:14 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 25s  [ loss=0.0719 ][Evaluating] 1/18 [>.............................] - ETA: 25s[Evaluating] 2/18 [==>...........................] - ETA: 23s  [ loss=0.0884 ][Evaluating] 2/18 [==>...........................] - ETA: 23s[Evaluating] 3/18 [====>.........................] - ETA: 22s  [ loss=0.0789 ][Evaluating] 3/18 [====>.........................] - ETA: 22s[Evaluating] 4/18 [=====>........................] - ETA: 21s  [ loss=0.0687 ][Evaluating] 4/18 [=====>........................] - ETA: 21s[Evaluating] 5/18 [=======>......................] - ETA: 20s  [ loss=0.0889 ][Evaluating] 5/18 [=======>......................] - ETA: 20s[Evaluating] 6/18 [=========>....................] - ETA: 19s  [ loss=0.0964 ][Evaluating] 6/18 [=========>....................] - ETA: 19s[Evaluating] 7/18 [==========>...................] - ETA: 17s  [ loss=0.1024 ][Evaluating] 7/18 [==========>...................] - ETA: 17s[Evaluating] 8/18 [============>.................] - ETA: 15s  [ loss=0.0609 ][Evaluating] 8/18 [============>.................] - ETA: 15s[Evaluating] 9/18 [==============>...............] - ETA: 14s  [ loss=0.0783 ][Evaluating] 9/18 [==============>...............] - ETA: 14s[Evaluating] 10/18 [===============>..............] - ETA: 12s  [ loss=0.0814 ][Evaluating] 10/18 [===============>..............] - ETA: 12s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0840 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 9s  [ loss=0.0776 ][Evaluating] 12/18 [===================>..........] - ETA: 9s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0746 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0942 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 4s  [ loss=0.0964 ][Evaluating] 15/18 [========================>.....] - ETA: 4s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0752 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0630 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0798 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:34:44 - INFO - __main__ -   

03/21/2023 11:34:44 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.7610 - recall: 0.7754 - f1: 0.7681 - loss: 0.0812 
03/21/2023 11:34:44 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:34:44 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.7348 - recall: 0.5673 - f1: 0.6403 
03/21/2023 11:34:44 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.6406 - recall: 0.2733 - f1: 0.3832 
03/21/2023 11:34:44 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.7139 - recall: 0.7807 - f1: 0.7458 
03/21/2023 11:34:44 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.8022 - recall: 0.9502 - f1: 0.8699 
03/21/2023 11:34:46 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-100
03/21/2023 11:34:49 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-100
03/21/2023 11:34:49 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:34:54 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:34:54 - INFO - __main__ -   Saving best eval loss model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:34:59 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 17/84 [=====>........................] - ETA: 5:07  [ loss=0.0554 ][Training] 18/84 [=====>........................] - ETA: 4:53  [ loss=0.0746 ][Training] 19/84 [=====>........................] - ETA: 4:39  [ loss=0.0652 ][Training] 20/84 [======>.......................] - ETA: 4:27  [ loss=0.0817 ][Training] 21/84 [======>.......................] - ETA: 4:16  [ loss=0.0903 ][Training] 22/84 [======>.......................] - ETA: 4:06  [ loss=0.0788 ][Training] 23/84 [=======>......................] - ETA: 3:56  [ loss=0.0796 ][Training] 24/84 [=======>......................] - ETA: 3:47  [ loss=0.0845 ][Training] 25/84 [=======>......................] - ETA: 3:39  [ loss=0.0851 ][Training] 26/84 [========>.....................] - ETA: 3:31  [ loss=0.0556 ][Training] 27/84 [========>.....................] - ETA: 3:24  [ loss=0.0626 ][Training] 28/84 [=========>....................] - ETA: 3:17  [ loss=0.0681 ][Training] 29/84 [=========>....................] - ETA: 3:10  [ loss=0.1084 ][Training] 30/84 [=========>....................] - ETA: 3:04  [ loss=0.0602 ][Training] 31/84 [==========>...................] - ETA: 2:58  [ loss=0.0775 ][Training] 32/84 [==========>...................] - ETA: 2:52  [ loss=0.0651 ][Training] 33/84 [==========>...................] - ETA: 2:47  [ loss=0.0779 ][Training] 34/84 [===========>..................] - ETA: 2:41  [ loss=0.0908 ][Training] 35/84 [===========>..................] - ETA: 2:36  [ loss=0.0507 ][Training] 36/84 [===========>..................] - ETA: 2:31  [ loss=0.0659 ][Training] 37/84 [============>.................] - ETA: 2:26  [ loss=0.0783 ][Training] 38/84 [============>.................] - ETA: 2:22  [ loss=0.0569 ][Training] 39/84 [============>.................] - ETA: 2:17  [ loss=0.0587 ][Training] 40/84 [=============>................] - ETA: 2:13  [ loss=0.0690 ][Training] 41/84 [=============>................] - ETA: 2:09  [ loss=0.0468 ][Training] 42/84 [==============>...............] - ETA: 2:05  [ loss=0.0585 ][Training] 43/84 [==============>...............] - ETA: 2:01  [ loss=0.0512 ][Training] 44/84 [==============>...............] - ETA: 1:57  [ loss=0.0859 ][Training] 45/84 [===============>..............] - ETA: 1:54  [ loss=0.0762 ][Training] 46/84 [===============>..............] - ETA: 1:50  [ loss=0.0736 ][Training] 47/84 [===============>..............] - ETA: 1:46  [ loss=0.0760 ][Training] 48/84 [================>.............] - ETA: 1:43  [ loss=0.0557 ][Training] 49/84 [================>.............] - ETA: 1:39  [ loss=0.0737 ][Training] 50/84 [================>.............] - ETA: 1:36  [ loss=0.0872 ][Training] 51/84 [=================>............] - ETA: 1:32  [ loss=0.0656 ][Training] 52/84 [=================>............] - ETA: 1:29  [ loss=0.0577 ][Training] 53/84 [=================>............] - ETA: 1:25  [ loss=0.0831 ][Training] 54/84 [==================>...........] - ETA: 1:22  [ loss=0.0558 ][Training] 55/84 [==================>...........] - ETA: 1:19  [ loss=0.0666 ][Training] 56/84 [===================>..........] - ETA: 1:16  [ loss=0.0537 ][Training] 57/84 [===================>..........] - ETA: 1:12  [ loss=0.0525 ][Training] 58/84 [===================>..........] - ETA: 1:09  [ loss=0.0758 ][Training] 59/84 [====================>.........] - ETA: 1:06  [ loss=0.0890 ][Training] 60/84 [====================>.........] - ETA: 1:03  [ loss=0.0435 ][Training] 61/84 [====================>.........] - ETA: 1:00  [ loss=0.0702 ][Training] 62/84 [=====================>........] - ETA: 57s  [ loss=0.0562 ][Training] 63/84 [=====================>........] - ETA: 54s  [ loss=0.0603 ][Training] 64/84 [=====================>........] - ETA: 52s  [ loss=0.0686 ][Training] 65/84 [======================>.......] - ETA: 49s  [ loss=0.0572 ][Training] 66/84 [======================>.......] - ETA: 46s  [ loss=0.0655 ][Training] 67/84 [======================>.......] - ETA: 43s  [ loss=0.0649 ][Training] 68/84 [=======================>......] - ETA: 40s  [ loss=0.0576 ][Training] 69/84 [=======================>......] - ETA: 38s  [ loss=0.0545 ][Training] 70/84 [========================>.....] - ETA: 35s  [ loss=0.0595 ][Training] 71/84 [========================>.....] - ETA: 32s  [ loss=0.0659 ][Training] 72/84 [========================>.....] - ETA: 30s  [ loss=0.0631 ][Training] 73/84 [=========================>....] - ETA: 27s  [ loss=0.0525 ][Training] 74/84 [=========================>....] - ETA: 25s  [ loss=0.0519 ][Training] 75/84 [=========================>....] - ETA: 22s  [ loss=0.0868 ][Training] 76/84 [==========================>...] - ETA: 19s  [ loss=0.0579 ][Training] 77/84 [==========================>...] - ETA: 17s  [ loss=0.0837 ][Training] 78/84 [==========================>...] - ETA: 14s  [ loss=0.0507 ][Training] 79/84 [===========================>..] - ETA: 12s  [ loss=0.0561 ][Training] 80/84 [===========================>..] - ETA: 9s  [ loss=0.0562 ][Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0704 ][Training] 82/84 [============================>.] - ETA: 4s  [ loss=0.0578 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0663 ][Training] 84/84 [==============================] 2.4s/step  [ loss=0.0469 ]
03/21/2023 11:37:07 - INFO - __main__ -   


Epoch: 2/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:17  [ loss=0.0432 ][Training] 2/84 [..............................] - ETA: 3:25  [ loss=0.0381 ][Training] 3/84 [>.............................] - ETA: 3:04  [ loss=0.0431 ][Training] 4/84 [>.............................] - ETA: 2:53  [ loss=0.0531 ][Training] 5/84 [>.............................] - ETA: 2:46  [ loss=0.0484 ][Training] 6/84 [=>............................] - ETA: 2:41  [ loss=0.0619 ][Training] 7/84 [=>............................] - ETA: 2:37  [ loss=0.0392 ][Training] 8/84 [=>............................] - ETA: 2:33  [ loss=0.0400 ][Training] 9/84 [==>...........................] - ETA: 2:30  [ loss=0.0361 ][Training] 10/84 [==>...........................] - ETA: 2:27  [ loss=0.0520 ][Training] 11/84 [==>...........................] - ETA: 2:24  [ loss=0.0463 ][Training] 12/84 [===>..........................] - ETA: 2:21  [ loss=0.0376 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0339 ][Training] 14/84 [====>.........................] - ETA: 2:16  [ loss=0.0401 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0372 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0653 ][Training] 17/84 [=====>........................] - ETA: 2:09  [ loss=0.0480 ][Training] 18/84 [=====>........................] - ETA: 2:07  [ loss=0.0453 ][Training] 19/84 [=====>........................] - ETA: 2:05  [ loss=0.0450 ][Training] 20/84 [======>.......................] - ETA: 2:03  [ loss=0.0512 ][Training] 21/84 [======>.......................] - ETA: 2:00  [ loss=0.0417 ][Training] 22/84 [======>.......................] - ETA: 1:58  [ loss=0.0459 ][Training] 23/84 [=======>......................] - ETA: 1:56  [ loss=0.0437 ][Training] 24/84 [=======>......................] - ETA: 1:54  [ loss=0.0439 ][Training] 25/84 [=======>......................] - ETA: 1:52  [ loss=0.0522 ][Training] 26/84 [========>.....................] - ETA: 1:50  [ loss=0.0429 ][Training] 27/84 [========>.....................] - ETA: 1:48  [ loss=0.0421 ][Training] 28/84 [=========>....................] - ETA: 1:46  [ loss=0.0342 ][Training] 29/84 [=========>....................] - ETA: 1:44  [ loss=0.0527 ][Training] 30/84 [=========>....................] - ETA: 1:43  [ loss=0.0478 ][Training] 31/84 [==========>...................] - ETA: 1:41  [ loss=0.0537 ][Training] 32/84 [==========>...................] - ETA: 1:39  [ loss=0.0432 ] 
03/21/2023 11:38:08 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:38:08 - INFO - __main__ -     Num examples = 723
03/21/2023 11:38:08 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 30s  [ loss=0.0468 ][Evaluating] 1/18 [>.............................] - ETA: 30s[Evaluating] 2/18 [==>...........................] - ETA: 27s  [ loss=0.0645 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0509 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0438 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.0805 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.0880 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.0890 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0564 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0720 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0593 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0613 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0589 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0559 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0637 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0684 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0490 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0459 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0731 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:38:40 - INFO - __main__ -   

03/21/2023 11:38:40 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.8352 - recall: 0.8422 - f1: 0.8387 - loss: 0.0626 
03/21/2023 11:38:40 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:38:40 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.8092 - recall: 0.7193 - f1: 0.7616 
03/21/2023 11:38:40 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.5824 - recall: 0.7067 - f1: 0.6386 
03/21/2023 11:38:40 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.8041 - recall: 0.8342 - f1: 0.8189 
03/21/2023 11:38:40 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.9374 - recall: 0.9133 - f1: 0.9252 
03/21/2023 11:38:41 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-200
03/21/2023 11:38:44 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-200
03/21/2023 11:38:44 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:39:03 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:39:03 - INFO - __main__ -   Saving best eval loss model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:39:36 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 33/84 [==========>...................] - ETA: 3:52  [ loss=0.0448 ][Training] 34/84 [===========>..................] - ETA: 3:44  [ loss=0.0343 ][Training] 35/84 [===========>..................] - ETA: 3:35  [ loss=0.0495 ][Training] 36/84 [===========>..................] - ETA: 3:27  [ loss=0.0455 ][Training] 37/84 [============>.................] - ETA: 3:20  [ loss=0.0547 ][Training] 38/84 [============>.................] - ETA: 3:13  [ loss=0.0391 ][Training] 39/84 [============>.................] - ETA: 3:06  [ loss=0.0485 ][Training] 40/84 [=============>................] - ETA: 2:59  [ loss=0.0481 ][Training] 41/84 [=============>................] - ETA: 2:53  [ loss=0.0423 ][Training] 42/84 [==============>...............] - ETA: 2:46  [ loss=0.0575 ][Training] 43/84 [==============>...............] - ETA: 2:40  [ loss=0.0394 ][Training] 44/84 [==============>...............] - ETA: 2:35  [ loss=0.0376 ][Training] 45/84 [===============>..............] - ETA: 2:29  [ loss=0.0556 ][Training] 46/84 [===============>..............] - ETA: 2:23  [ loss=0.0465 ][Training] 47/84 [===============>..............] - ETA: 2:18  [ loss=0.0413 ][Training] 48/84 [================>.............] - ETA: 2:13  [ loss=0.0372 ][Training] 49/84 [================>.............] - ETA: 2:08  [ loss=0.0412 ][Training] 50/84 [================>.............] - ETA: 2:03  [ loss=0.0450 ][Training] 51/84 [=================>............] - ETA: 1:58  [ loss=0.0392 ][Training] 52/84 [=================>............] - ETA: 1:54  [ loss=0.0391 ][Training] 53/84 [=================>............] - ETA: 1:49  [ loss=0.0490 ][Training] 54/84 [==================>...........] - ETA: 1:45  [ loss=0.0408 ][Training] 55/84 [==================>...........] - ETA: 1:40  [ loss=0.0469 ][Training] 56/84 [===================>..........] - ETA: 1:36  [ loss=0.0544 ][Training] 57/84 [===================>..........] - ETA: 1:32  [ loss=0.0438 ][Training] 58/84 [===================>..........] - ETA: 1:28  [ loss=0.0398 ][Training] 59/84 [====================>.........] - ETA: 1:24  [ loss=0.0476 ][Training] 60/84 [====================>.........] - ETA: 1:20  [ loss=0.0426 ][Training] 61/84 [====================>.........] - ETA: 1:16  [ loss=0.0539 ][Training] 62/84 [=====================>........] - ETA: 1:12  [ loss=0.0443 ][Training] 63/84 [=====================>........] - ETA: 1:08  [ loss=0.0414 ][Training] 64/84 [=====================>........] - ETA: 1:04  [ loss=0.0365 ][Training] 65/84 [======================>.......] - ETA: 1:01  [ loss=0.0433 ][Training] 66/84 [======================>.......] - ETA: 57s  [ loss=0.0387 ][Training] 67/84 [======================>.......] - ETA: 54s  [ loss=0.0478 ][Training] 68/84 [=======================>......] - ETA: 50s  [ loss=0.0312 ][Training] 69/84 [=======================>......] - ETA: 47s  [ loss=0.0450 ][Training] 70/84 [========================>.....] - ETA: 43s  [ loss=0.0456 ][Training] 71/84 [========================>.....] - ETA: 40s  [ loss=0.0461 ][Training] 72/84 [========================>.....] - ETA: 37s  [ loss=0.0369 ][Training] 73/84 [=========================>....] - ETA: 33s  [ loss=0.0462 ][Training] 74/84 [=========================>....] - ETA: 30s  [ loss=0.0425 ][Training] 75/84 [=========================>....] - ETA: 27s  [ loss=0.0487 ][Training] 76/84 [==========================>...] - ETA: 24s  [ loss=0.0450 ][Training] 77/84 [==========================>...] - ETA: 21s  [ loss=0.0438 ][Training] 78/84 [==========================>...] - ETA: 18s  [ loss=0.0495 ][Training] 79/84 [===========================>..] - ETA: 14s  [ loss=0.0539 ][Training] 80/84 [===========================>..] - ETA: 11s  [ loss=0.0511 ][Training] 81/84 [===========================>..] - ETA: 8s  [ loss=0.0402 ][Training] 82/84 [============================>.] - ETA: 5s  [ loss=0.0411 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0337 ][Training] 84/84 [==============================] 2.9s/step  [ loss=0.0410 ]
03/21/2023 11:41:12 - INFO - __main__ -   


Epoch: 3/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:14  [ loss=0.0381 ][Training] 2/84 [..............................] - ETA: 3:22  [ loss=0.0420 ][Training] 3/84 [>.............................] - ETA: 3:03  [ loss=0.0319 ][Training] 4/84 [>.............................] - ETA: 2:52  [ loss=0.0412 ][Training] 5/84 [>.............................] - ETA: 2:44  [ loss=0.0345 ][Training] 6/84 [=>............................] - ETA: 2:39  [ loss=0.0315 ][Training] 7/84 [=>............................] - ETA: 2:35  [ loss=0.0358 ][Training] 8/84 [=>............................] - ETA: 2:31  [ loss=0.0391 ][Training] 9/84 [==>...........................] - ETA: 2:29  [ loss=0.0359 ][Training] 10/84 [==>...........................] - ETA: 2:26  [ loss=0.0346 ][Training] 11/84 [==>...........................] - ETA: 2:23  [ loss=0.0297 ][Training] 12/84 [===>..........................] - ETA: 2:21  [ loss=0.0327 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0401 ][Training] 14/84 [====>.........................] - ETA: 2:16  [ loss=0.0367 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0295 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0415 ][Training] 17/84 [=====>........................] - ETA: 2:10  [ loss=0.0336 ][Training] 18/84 [=====>........................] - ETA: 2:08  [ loss=0.0311 ][Training] 19/84 [=====>........................] - ETA: 2:06  [ loss=0.0330 ][Training] 20/84 [======>.......................] - ETA: 2:04  [ loss=0.0297 ][Training] 21/84 [======>.......................] - ETA: 2:02  [ loss=0.0375 ][Training] 22/84 [======>.......................] - ETA: 2:00  [ loss=0.0382 ][Training] 23/84 [=======>......................] - ETA: 1:58  [ loss=0.0355 ][Training] 24/84 [=======>......................] - ETA: 1:56  [ loss=0.0380 ][Training] 25/84 [=======>......................] - ETA: 1:54  [ loss=0.0390 ][Training] 26/84 [========>.....................] - ETA: 1:52  [ loss=0.0344 ][Training] 27/84 [========>.....................] - ETA: 1:50  [ loss=0.0344 ][Training] 28/84 [=========>....................] - ETA: 1:48  [ loss=0.0319 ][Training] 29/84 [=========>....................] - ETA: 1:46  [ loss=0.0380 ][Training] 30/84 [=========>....................] - ETA: 1:44  [ loss=0.0347 ][Training] 31/84 [==========>...................] - ETA: 1:42  [ loss=0.0305 ][Training] 32/84 [==========>...................] - ETA: 1:40  [ loss=0.0342 ][Training] 33/84 [==========>...................] - ETA: 1:38  [ loss=0.0298 ][Training] 34/84 [===========>..................] - ETA: 1:36  [ loss=0.0292 ][Training] 35/84 [===========>..................] - ETA: 1:34  [ loss=0.0331 ][Training] 36/84 [===========>..................] - ETA: 1:32  [ loss=0.0334 ][Training] 37/84 [============>.................] - ETA: 1:30  [ loss=0.0352 ][Training] 38/84 [============>.................] - ETA: 1:28  [ loss=0.0337 ][Training] 39/84 [============>.................] - ETA: 1:26  [ loss=0.0355 ][Training] 40/84 [=============>................] - ETA: 1:24  [ loss=0.0291 ][Training] 41/84 [=============>................] - ETA: 1:22  [ loss=0.0353 ][Training] 42/84 [==============>...............] - ETA: 1:20  [ loss=0.0291 ][Training] 43/84 [==============>...............] - ETA: 1:18  [ loss=0.0286 ][Training] 44/84 [==============>...............] - ETA: 1:16  [ loss=0.0285 ][Training] 45/84 [===============>..............] - ETA: 1:14  [ loss=0.0337 ][Training] 46/84 [===============>..............] - ETA: 1:12  [ loss=0.0380 ][Training] 47/84 [===============>..............] - ETA: 1:10  [ loss=0.0319 ][Training] 48/84 [================>.............] - ETA: 1:08  [ loss=0.0361 ] 
03/21/2023 11:42:45 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:42:45 - INFO - __main__ -     Num examples = 723
03/21/2023 11:42:45 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 27s  [ loss=0.0469 ][Evaluating] 1/18 [>.............................] - ETA: 27s[Evaluating] 2/18 [==>...........................] - ETA: 25s  [ loss=0.0623 ][Evaluating] 2/18 [==>...........................] - ETA: 25s[Evaluating] 3/18 [====>.........................] - ETA: 24s  [ loss=0.0484 ][Evaluating] 3/18 [====>.........................] - ETA: 24s[Evaluating] 4/18 [=====>........................] - ETA: 23s  [ loss=0.0414 ][Evaluating] 4/18 [=====>........................] - ETA: 23s[Evaluating] 5/18 [=======>......................] - ETA: 21s  [ loss=0.0824 ][Evaluating] 5/18 [=======>......................] - ETA: 21s[Evaluating] 6/18 [=========>....................] - ETA: 19s  [ loss=0.0933 ][Evaluating] 6/18 [=========>....................] - ETA: 19s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.0981 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 16s  [ loss=0.0558 ][Evaluating] 8/18 [============>.................] - ETA: 16s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0644 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0631 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0589 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0707 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0594 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0770 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0741 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0508 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0482 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.8s/step  [ loss=0.0818 ]
[Evaluating] 18/18 [==============================] 1.8s/step
03/21/2023 11:43:16 - INFO - __main__ -   

03/21/2023 11:43:16 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:43:16 - INFO - __main__ -    acc: 0.8640 - recall: 0.8437 - f1: 0.8537 - loss: 0.0654 
03/21/2023 11:43:16 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:43:16 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:43:16 - INFO - __main__ -    acc: 0.8061 - recall: 0.7778 - f1: 0.7917 
03/21/2023 11:43:16 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:43:16 - INFO - __main__ -    acc: 0.6867 - recall: 0.6867 - f1: 0.6867 
03/21/2023 11:43:16 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:43:16 - INFO - __main__ -    acc: 0.8163 - recall: 0.8316 - f1: 0.8238 
03/21/2023 11:43:16 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:43:16 - INFO - __main__ -    acc: 0.9560 - recall: 0.9069 - f1: 0.9308 
03/21/2023 11:43:18 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-300
03/21/2023 11:43:21 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-300
03/21/2023 11:43:21 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:43:39 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 49/84 [================>.............] - ETA: 1:45  [ loss=0.0322 ][Training] 50/84 [================>.............] - ETA: 1:41  [ loss=0.0269 ][Training] 51/84 [=================>............] - ETA: 1:38  [ loss=0.0341 ][Training] 52/84 [=================>............] - ETA: 1:34  [ loss=0.0364 ][Training] 53/84 [=================>............] - ETA: 1:31  [ loss=0.0312 ][Training] 54/84 [==================>...........] - ETA: 1:27  [ loss=0.0363 ][Training] 55/84 [==================>...........] - ETA: 1:24  [ loss=0.0294 ][Training] 56/84 [===================>..........] - ETA: 1:20  [ loss=0.0319 ][Training] 57/84 [===================>..........] - ETA: 1:17  [ loss=0.0352 ][Training] 58/84 [===================>..........] - ETA: 1:14  [ loss=0.0396 ][Training] 59/84 [====================>.........] - ETA: 1:10  [ loss=0.0268 ][Training] 60/84 [====================>.........] - ETA: 1:07  [ loss=0.0386 ][Training] 61/84 [====================>.........] - ETA: 1:04  [ loss=0.0296 ][Training] 62/84 [=====================>........] - ETA: 1:01  [ loss=0.0306 ][Training] 63/84 [=====================>........] - ETA: 58s  [ loss=0.0316 ][Training] 64/84 [=====================>........] - ETA: 55s  [ loss=0.0366 ][Training] 65/84 [======================>.......] - ETA: 52s  [ loss=0.0376 ][Training] 66/84 [======================>.......] - ETA: 49s  [ loss=0.0338 ][Training] 67/84 [======================>.......] - ETA: 46s  [ loss=0.0389 ][Training] 68/84 [=======================>......] - ETA: 43s  [ loss=0.0371 ][Training] 69/84 [=======================>......] - ETA: 40s  [ loss=0.0333 ][Training] 70/84 [========================>.....] - ETA: 37s  [ loss=0.0320 ][Training] 71/84 [========================>.....] - ETA: 34s  [ loss=0.0367 ][Training] 72/84 [========================>.....] - ETA: 31s  [ loss=0.0325 ][Training] 73/84 [=========================>....] - ETA: 29s  [ loss=0.0356 ][Training] 74/84 [=========================>....] - ETA: 26s  [ loss=0.0498 ][Training] 75/84 [=========================>....] - ETA: 23s  [ loss=0.0305 ][Training] 76/84 [==========================>...] - ETA: 20s  [ loss=0.0326 ][Training] 77/84 [==========================>...] - ETA: 18s  [ loss=0.0306 ][Training] 78/84 [==========================>...] - ETA: 15s  [ loss=0.0347 ][Training] 79/84 [===========================>..] - ETA: 12s  [ loss=0.0332 ][Training] 80/84 [===========================>..] - ETA: 10s  [ loss=0.0369 ][Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0316 ][Training] 82/84 [============================>.] - ETA: 5s  [ loss=0.0313 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0360 ][Training] 84/84 [==============================] 2.5s/step  [ loss=0.0371 ]
03/21/2023 11:44:45 - INFO - __main__ -   


Epoch: 4/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:22  [ loss=0.0297 ][Training] 2/84 [..............................] - ETA: 3:22  [ loss=0.0294 ][Training] 3/84 [>.............................] - ETA: 3:02  [ loss=0.0251 ][Training] 4/84 [>.............................] - ETA: 2:51  [ loss=0.0296 ][Training] 5/84 [>.............................] - ETA: 2:43  [ loss=0.0297 ][Training] 6/84 [=>............................] - ETA: 2:39  [ loss=0.0306 ][Training] 7/84 [=>............................] - ETA: 2:35  [ loss=0.0264 ][Training] 8/84 [=>............................] - ETA: 2:31  [ loss=0.0270 ][Training] 9/84 [==>...........................] - ETA: 2:28  [ loss=0.0339 ][Training] 10/84 [==>...........................] - ETA: 2:25  [ loss=0.0298 ][Training] 11/84 [==>...........................] - ETA: 2:22  [ loss=0.0313 ][Training] 12/84 [===>..........................] - ETA: 2:19  [ loss=0.0269 ][Training] 13/84 [===>..........................] - ETA: 2:17  [ loss=0.0274 ][Training] 14/84 [====>.........................] - ETA: 2:14  [ loss=0.0252 ][Training] 15/84 [====>.........................] - ETA: 2:12  [ loss=0.0299 ][Training] 16/84 [====>.........................] - ETA: 2:10  [ loss=0.0262 ][Training] 17/84 [=====>........................] - ETA: 2:07  [ loss=0.0267 ][Training] 18/84 [=====>........................] - ETA: 2:05  [ loss=0.0311 ][Training] 19/84 [=====>........................] - ETA: 2:03  [ loss=0.0262 ][Training] 20/84 [======>.......................] - ETA: 2:01  [ loss=0.0265 ][Training] 21/84 [======>.......................] - ETA: 1:59  [ loss=0.0276 ][Training] 22/84 [======>.......................] - ETA: 1:57  [ loss=0.0290 ][Training] 23/84 [=======>......................] - ETA: 1:55  [ loss=0.0279 ][Training] 24/84 [=======>......................] - ETA: 1:53  [ loss=0.0326 ][Training] 25/84 [=======>......................] - ETA: 1:51  [ loss=0.0289 ][Training] 26/84 [========>.....................] - ETA: 1:50  [ loss=0.0204 ][Training] 27/84 [========>.....................] - ETA: 1:48  [ loss=0.0328 ][Training] 28/84 [=========>....................] - ETA: 1:46  [ loss=0.0261 ][Training] 29/84 [=========>....................] - ETA: 1:44  [ loss=0.0299 ][Training] 30/84 [=========>....................] - ETA: 1:43  [ loss=0.0266 ][Training] 31/84 [==========>...................] - ETA: 1:41  [ loss=0.0285 ][Training] 32/84 [==========>...................] - ETA: 1:39  [ loss=0.0235 ][Training] 33/84 [==========>...................] - ETA: 1:37  [ loss=0.0263 ][Training] 34/84 [===========>..................] - ETA: 1:35  [ loss=0.0352 ][Training] 35/84 [===========>..................] - ETA: 1:33  [ loss=0.0291 ][Training] 36/84 [===========>..................] - ETA: 1:31  [ loss=0.0252 ][Training] 37/84 [============>.................] - ETA: 1:29  [ loss=0.0289 ][Training] 38/84 [============>.................] - ETA: 1:27  [ loss=0.0308 ][Training] 39/84 [============>.................] - ETA: 1:25  [ loss=0.0275 ][Training] 40/84 [=============>................] - ETA: 1:23  [ loss=0.0277 ][Training] 41/84 [=============>................] - ETA: 1:22  [ loss=0.0275 ][Training] 42/84 [==============>...............] - ETA: 1:20  [ loss=0.0310 ][Training] 43/84 [==============>...............] - ETA: 1:18  [ loss=0.0257 ][Training] 44/84 [==============>...............] - ETA: 1:16  [ loss=0.0279 ][Training] 45/84 [===============>..............] - ETA: 1:14  [ loss=0.0294 ][Training] 46/84 [===============>..............] - ETA: 1:12  [ loss=0.0268 ][Training] 47/84 [===============>..............] - ETA: 1:10  [ loss=0.0249 ][Training] 48/84 [================>.............] - ETA: 1:08  [ loss=0.0286 ][Training] 49/84 [================>.............] - ETA: 1:06  [ loss=0.0265 ][Training] 50/84 [================>.............] - ETA: 1:04  [ loss=0.0304 ][Training] 51/84 [=================>............] - ETA: 1:03  [ loss=0.0267 ][Training] 52/84 [=================>............] - ETA: 1:01  [ loss=0.0305 ][Training] 53/84 [=================>............] - ETA: 59s  [ loss=0.0260 ][Training] 54/84 [==================>...........] - ETA: 57s  [ loss=0.0225 ][Training] 55/84 [==================>...........] - ETA: 55s  [ loss=0.0288 ][Training] 56/84 [===================>..........] - ETA: 53s  [ loss=0.0275 ][Training] 57/84 [===================>..........] - ETA: 51s  [ loss=0.0304 ][Training] 58/84 [===================>..........] - ETA: 49s  [ loss=0.0257 ][Training] 59/84 [====================>.........] - ETA: 47s  [ loss=0.0345 ][Training] 60/84 [====================>.........] - ETA: 45s  [ loss=0.0231 ][Training] 61/84 [====================>.........] - ETA: 43s  [ loss=0.0312 ][Training] 62/84 [=====================>........] - ETA: 41s  [ loss=0.0292 ][Training] 63/84 [=====================>........] - ETA: 40s  [ loss=0.0260 ][Training] 64/84 [=====================>........] - ETA: 38s  [ loss=0.0391 ] 
03/21/2023 11:46:48 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:46:48 - INFO - __main__ -     Num examples = 723
03/21/2023 11:46:48 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 29s  [ loss=0.0541 ][Evaluating] 1/18 [>.............................] - ETA: 30s[Evaluating] 2/18 [==>...........................] - ETA: 27s  [ loss=0.0735 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0487 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0446 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.0953 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.1041 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 19s  [ loss=0.1172 ][Evaluating] 7/18 [==========>...................] - ETA: 19s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0631 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0638 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 14s  [ loss=0.0549 ][Evaluating] 10/18 [===============>..............] - ETA: 14s[Evaluating] 11/18 [=================>............] - ETA: 12s  [ loss=0.0721 ][Evaluating] 11/18 [=================>............] - ETA: 12s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0849 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 9s  [ loss=0.0641 ][Evaluating] 13/18 [====================>.........] - ETA: 9s[Evaluating] 14/18 [======================>.......] - ETA: 7s  [ loss=0.0757 ][Evaluating] 14/18 [======================>.......] - ETA: 7s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0849 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0559 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0471 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.9s/step  [ loss=0.0913 ]
[Evaluating] 18/18 [==============================] 1.9s/step
03/21/2023 11:47:22 - INFO - __main__ -   

03/21/2023 11:47:22 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:47:22 - INFO - __main__ -    acc: 0.8553 - recall: 0.8612 - f1: 0.8582 - loss: 0.0720 
03/21/2023 11:47:22 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:47:22 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:47:22 - INFO - __main__ -    acc: 0.7874 - recall: 0.8012 - f1: 0.7942 
03/21/2023 11:47:22 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:47:22 - INFO - __main__ -    acc: 0.6757 - recall: 0.6667 - f1: 0.6711 
03/21/2023 11:47:22 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:47:22 - INFO - __main__ -    acc: 0.8260 - recall: 0.8503 - f1: 0.8379 
03/21/2023 11:47:22 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:47:22 - INFO - __main__ -    acc: 0.9355 - recall: 0.9310 - f1: 0.9332 
03/21/2023 11:47:23 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-400
03/21/2023 11:47:27 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-400
03/21/2023 11:47:27 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:47:47 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 65/84 [======================>.......] - ETA: 53s  [ loss=0.0252 ][Training] 66/84 [======================>.......] - ETA: 50s  [ loss=0.0288 ][Training] 67/84 [======================>.......] - ETA: 47s  [ loss=0.0297 ][Training] 68/84 [=======================>......] - ETA: 44s  [ loss=0.0275 ][Training] 69/84 [=======================>......] - ETA: 41s  [ loss=0.0325 ][Training] 70/84 [========================>.....] - ETA: 38s  [ loss=0.0291 ][Training] 71/84 [========================>.....] - ETA: 35s  [ loss=0.0283 ][Training] 72/84 [========================>.....] - ETA: 32s  [ loss=0.0291 ][Training] 73/84 [=========================>....] - ETA: 30s  [ loss=0.0246 ][Training] 74/84 [=========================>....] - ETA: 27s  [ loss=0.0284 ][Training] 75/84 [=========================>....] - ETA: 24s  [ loss=0.0298 ][Training] 76/84 [==========================>...] - ETA: 21s  [ loss=0.0276 ][Training] 77/84 [==========================>...] - ETA: 18s  [ loss=0.0280 ][Training] 78/84 [==========================>...] - ETA: 16s  [ loss=0.0300 ][Training] 79/84 [===========================>..] - ETA: 13s  [ loss=0.0293 ][Training] 80/84 [===========================>..] - ETA: 10s  [ loss=0.0288 ][Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0247 ][Training] 82/84 [============================>.] - ETA: 5s  [ loss=0.0310 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0337 ][Training] 84/84 [==============================] 2.6s/step  [ loss=0.0265 ]
03/21/2023 11:48:26 - INFO - __main__ -   


Epoch: 5/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:17  [ loss=0.0305 ][Training] 2/84 [..............................] - ETA: 3:24  [ loss=0.0235 ][Training] 3/84 [>.............................] - ETA: 3:04  [ loss=0.0269 ][Training] 4/84 [>.............................] - ETA: 2:55  [ loss=0.0255 ][Training] 5/84 [>.............................] - ETA: 2:48  [ loss=0.0277 ][Training] 6/84 [=>............................] - ETA: 2:43  [ loss=0.0221 ][Training] 7/84 [=>............................] - ETA: 2:39  [ loss=0.0237 ][Training] 8/84 [=>............................] - ETA: 2:35  [ loss=0.0295 ][Training] 9/84 [==>...........................] - ETA: 2:31  [ loss=0.0281 ][Training] 10/84 [==>...........................] - ETA: 2:28  [ loss=0.0267 ][Training] 11/84 [==>...........................] - ETA: 2:25  [ loss=0.0243 ][Training] 12/84 [===>..........................] - ETA: 2:22  [ loss=0.0239 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0234 ][Training] 14/84 [====>.........................] - ETA: 2:16  [ loss=0.0246 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0277 ][Training] 16/84 [====>.........................] - ETA: 2:11  [ loss=0.0269 ][Training] 17/84 [=====>........................] - ETA: 2:08  [ loss=0.0228 ][Training] 18/84 [=====>........................] - ETA: 2:06  [ loss=0.0253 ][Training] 19/84 [=====>........................] - ETA: 2:04  [ loss=0.0241 ][Training] 20/84 [======>.......................] - ETA: 2:02  [ loss=0.0248 ][Training] 21/84 [======>.......................] - ETA: 2:00  [ loss=0.0256 ][Training] 22/84 [======>.......................] - ETA: 1:58  [ loss=0.0247 ][Training] 23/84 [=======>......................] - ETA: 1:56  [ loss=0.0254 ][Training] 24/84 [=======>......................] - ETA: 1:54  [ loss=0.0283 ][Training] 25/84 [=======>......................] - ETA: 1:52  [ loss=0.0244 ][Training] 26/84 [========>.....................] - ETA: 1:50  [ loss=0.0286 ][Training] 27/84 [========>.....................] - ETA: 1:48  [ loss=0.0247 ][Training] 28/84 [=========>....................] - ETA: 1:46  [ loss=0.0315 ][Training] 29/84 [=========>....................] - ETA: 1:44  [ loss=0.0270 ][Training] 30/84 [=========>....................] - ETA: 1:42  [ loss=0.0256 ][Training] 31/84 [==========>...................] - ETA: 1:40  [ loss=0.0242 ][Training] 32/84 [==========>...................] - ETA: 1:38  [ loss=0.0277 ][Training] 33/84 [==========>...................] - ETA: 1:36  [ loss=0.0259 ][Training] 34/84 [===========>..................] - ETA: 1:34  [ loss=0.0243 ][Training] 35/84 [===========>..................] - ETA: 1:32  [ loss=0.0239 ][Training] 36/84 [===========>..................] - ETA: 1:30  [ loss=0.0283 ][Training] 37/84 [============>.................] - ETA: 1:28  [ loss=0.0307 ][Training] 38/84 [============>.................] - ETA: 1:26  [ loss=0.0233 ][Training] 39/84 [============>.................] - ETA: 1:24  [ loss=0.0307 ][Training] 40/84 [=============>................] - ETA: 1:22  [ loss=0.0254 ][Training] 41/84 [=============>................] - ETA: 1:20  [ loss=0.0301 ][Training] 42/84 [==============>...............] - ETA: 1:19  [ loss=0.0278 ][Training] 43/84 [==============>...............] - ETA: 1:17  [ loss=0.0260 ][Training] 44/84 [==============>...............] - ETA: 1:15  [ loss=0.0286 ][Training] 45/84 [===============>..............] - ETA: 1:13  [ loss=0.0241 ][Training] 46/84 [===============>..............] - ETA: 1:11  [ loss=0.0237 ][Training] 47/84 [===============>..............] - ETA: 1:09  [ loss=0.0265 ][Training] 48/84 [================>.............] - ETA: 1:07  [ loss=0.0252 ][Training] 49/84 [================>.............] - ETA: 1:05  [ loss=0.0281 ][Training] 50/84 [================>.............] - ETA: 1:03  [ loss=0.0276 ][Training] 51/84 [=================>............] - ETA: 1:01  [ loss=0.0256 ][Training] 52/84 [=================>............] - ETA: 1:00  [ loss=0.0262 ][Training] 53/84 [=================>............] - ETA: 58s  [ loss=0.0238 ][Training] 54/84 [==================>...........] - ETA: 56s  [ loss=0.0262 ][Training] 55/84 [==================>...........] - ETA: 54s  [ loss=0.0246 ][Training] 56/84 [===================>..........] - ETA: 52s  [ loss=0.0222 ][Training] 57/84 [===================>..........] - ETA: 50s  [ loss=0.0246 ][Training] 58/84 [===================>..........] - ETA: 48s  [ loss=0.0248 ][Training] 59/84 [====================>.........] - ETA: 46s  [ loss=0.0275 ][Training] 60/84 [====================>.........] - ETA: 45s  [ loss=0.0227 ][Training] 61/84 [====================>.........] - ETA: 43s  [ loss=0.0273 ][Training] 62/84 [=====================>........] - ETA: 41s  [ loss=0.0295 ][Training] 63/84 [=====================>........] - ETA: 39s  [ loss=0.0256 ][Training] 64/84 [=====================>........] - ETA: 37s  [ loss=0.0261 ][Training] 65/84 [======================>.......] - ETA: 35s  [ loss=0.0254 ][Training] 66/84 [======================>.......] - ETA: 33s  [ loss=0.0272 ][Training] 67/84 [======================>.......] - ETA: 31s  [ loss=0.0243 ][Training] 68/84 [=======================>......] - ETA: 30s  [ loss=0.0212 ][Training] 69/84 [=======================>......] - ETA: 28s  [ loss=0.0265 ][Training] 70/84 [========================>.....] - ETA: 26s  [ loss=0.0282 ][Training] 71/84 [========================>.....] - ETA: 24s  [ loss=0.0270 ][Training] 72/84 [========================>.....] - ETA: 22s  [ loss=0.0233 ][Training] 73/84 [=========================>....] - ETA: 20s  [ loss=0.0268 ][Training] 74/84 [=========================>....] - ETA: 18s  [ loss=0.0274 ][Training] 75/84 [=========================>....] - ETA: 16s  [ loss=0.0259 ][Training] 76/84 [==========================>...] - ETA: 14s  [ loss=0.0287 ][Training] 77/84 [==========================>...] - ETA: 13s  [ loss=0.0255 ][Training] 78/84 [==========================>...] - ETA: 11s  [ loss=0.0285 ][Training] 79/84 [===========================>..] - ETA: 9s  [ loss=0.0315 ][Training] 80/84 [===========================>..] - ETA: 7s  [ loss=0.0245 ] 
03/21/2023 11:50:56 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:50:56 - INFO - __main__ -     Num examples = 723
03/21/2023 11:50:56 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 27s  [ loss=0.0584 ][Evaluating] 1/18 [>.............................] - ETA: 28s[Evaluating] 2/18 [==>...........................] - ETA: 26s  [ loss=0.0720 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0516 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0375 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.1056 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.1083 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.1224 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 16s  [ loss=0.0603 ][Evaluating] 8/18 [============>.................] - ETA: 16s[Evaluating] 9/18 [==============>...............] - ETA: 14s  [ loss=0.0693 ][Evaluating] 9/18 [==============>...............] - ETA: 14s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0702 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0704 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0933 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0617 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0774 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0747 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0585 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0507 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0956 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:51:26 - INFO - __main__ -   

03/21/2023 11:51:26 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:51:26 - INFO - __main__ -    acc: 0.8629 - recall: 0.8551 - f1: 0.8590 - loss: 0.0743 
03/21/2023 11:51:26 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:51:26 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:51:26 - INFO - __main__ -    acc: 0.7963 - recall: 0.7544 - f1: 0.7748 
03/21/2023 11:51:26 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:51:26 - INFO - __main__ -    acc: 0.7447 - recall: 0.7000 - f1: 0.7216 
03/21/2023 11:51:26 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:51:26 - INFO - __main__ -    acc: 0.8147 - recall: 0.8583 - f1: 0.8359 
03/21/2023 11:51:26 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:51:26 - INFO - __main__ -    acc: 0.9392 - recall: 0.9181 - f1: 0.9286 
03/21/2023 11:51:28 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-500
03/21/2023 11:51:30 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-500
03/21/2023 11:51:30 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:51:49 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0301 ][Training] 82/84 [============================>.] - ETA: 5s  [ loss=0.0258 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0332 ][Training] 84/84 [==============================] 2.5s/step  [ loss=0.0238 ]
03/21/2023 11:51:56 - INFO - __main__ -   


Epoch: 6/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:45  [ loss=0.0294 ][Training] 2/84 [..............................] - ETA: 3:36  [ loss=0.0282 ][Training] 3/84 [>.............................] - ETA: 3:10  [ loss=0.0244 ][Training] 4/84 [>.............................] - ETA: 2:57  [ loss=0.0283 ][Training] 5/84 [>.............................] - ETA: 2:49  [ loss=0.0272 ][Training] 6/84 [=>............................] - ETA: 2:42  [ loss=0.0234 ][Training] 7/84 [=>............................] - ETA: 2:38  [ loss=0.0215 ][Training] 8/84 [=>............................] - ETA: 2:34  [ loss=0.0219 ][Training] 9/84 [==>...........................] - ETA: 2:31  [ loss=0.0244 ][Training] 10/84 [==>...........................] - ETA: 2:27  [ loss=0.0235 ][Training] 11/84 [==>...........................] - ETA: 2:25  [ loss=0.0238 ][Training] 12/84 [===>..........................] - ETA: 2:21  [ loss=0.0264 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0267 ][Training] 14/84 [====>.........................] - ETA: 2:17  [ loss=0.0244 ][Training] 15/84 [====>.........................] - ETA: 2:15  [ loss=0.0262 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0270 ][Training] 17/84 [=====>........................] - ETA: 2:10  [ loss=0.0229 ][Training] 18/84 [=====>........................] - ETA: 2:07  [ loss=0.0299 ][Training] 19/84 [=====>........................] - ETA: 2:05  [ loss=0.0272 ][Training] 20/84 [======>.......................] - ETA: 2:03  [ loss=0.0266 ][Training] 21/84 [======>.......................] - ETA: 2:01  [ loss=0.0274 ][Training] 22/84 [======>.......................] - ETA: 1:59  [ loss=0.0271 ][Training] 23/84 [=======>......................] - ETA: 1:56  [ loss=0.0264 ][Training] 24/84 [=======>......................] - ETA: 1:54  [ loss=0.0249 ][Training] 25/84 [=======>......................] - ETA: 1:52  [ loss=0.0272 ][Training] 26/84 [========>.....................] - ETA: 1:50  [ loss=0.0222 ][Training] 27/84 [========>.....................] - ETA: 1:48  [ loss=0.0249 ][Training] 28/84 [=========>....................] - ETA: 1:46  [ loss=0.0240 ][Training] 29/84 [=========>....................] - ETA: 1:44  [ loss=0.0259 ][Training] 30/84 [=========>....................] - ETA: 1:42  [ loss=0.0234 ][Training] 31/84 [==========>...................] - ETA: 1:40  [ loss=0.0275 ][Training] 32/84 [==========>...................] - ETA: 1:38  [ loss=0.0267 ][Training] 33/84 [==========>...................] - ETA: 1:36  [ loss=0.0285 ][Training] 34/84 [===========>..................] - ETA: 1:34  [ loss=0.0260 ][Training] 35/84 [===========>..................] - ETA: 1:32  [ loss=0.0255 ][Training] 36/84 [===========>..................] - ETA: 1:30  [ loss=0.0261 ][Training] 37/84 [============>.................] - ETA: 1:28  [ loss=0.0250 ][Training] 38/84 [============>.................] - ETA: 1:26  [ loss=0.0319 ][Training] 39/84 [============>.................] - ETA: 1:24  [ loss=0.0233 ][Training] 40/84 [=============>................] - ETA: 1:23  [ loss=0.0245 ][Training] 41/84 [=============>................] - ETA: 1:21  [ loss=0.0242 ][Training] 42/84 [==============>...............] - ETA: 1:19  [ loss=0.0266 ][Training] 43/84 [==============>...............] - ETA: 1:17  [ loss=0.0267 ][Training] 44/84 [==============>...............] - ETA: 1:15  [ loss=0.0276 ][Training] 45/84 [===============>..............] - ETA: 1:13  [ loss=0.0229 ][Training] 46/84 [===============>..............] - ETA: 1:11  [ loss=0.0227 ][Training] 47/84 [===============>..............] - ETA: 1:09  [ loss=0.0253 ][Training] 48/84 [================>.............] - ETA: 1:07  [ loss=0.0271 ][Training] 49/84 [================>.............] - ETA: 1:05  [ loss=0.0241 ][Training] 50/84 [================>.............] - ETA: 1:04  [ loss=0.0259 ][Training] 51/84 [=================>............] - ETA: 1:02  [ loss=0.0228 ][Training] 52/84 [=================>............] - ETA: 1:00  [ loss=0.0238 ][Training] 53/84 [=================>............] - ETA: 58s  [ loss=0.0266 ][Training] 54/84 [==================>...........] - ETA: 56s  [ loss=0.0283 ][Training] 55/84 [==================>...........] - ETA: 54s  [ loss=0.0223 ][Training] 56/84 [===================>..........] - ETA: 52s  [ loss=0.0266 ][Training] 57/84 [===================>..........] - ETA: 50s  [ loss=0.0287 ][Training] 58/84 [===================>..........] - ETA: 48s  [ loss=0.0221 ][Training] 59/84 [====================>.........] - ETA: 47s  [ loss=0.0242 ][Training] 60/84 [====================>.........] - ETA: 45s  [ loss=0.0210 ][Training] 61/84 [====================>.........] - ETA: 43s  [ loss=0.0253 ][Training] 62/84 [=====================>........] - ETA: 41s  [ loss=0.0301 ][Training] 63/84 [=====================>........] - ETA: 39s  [ loss=0.0233 ][Training] 64/84 [=====================>........] - ETA: 37s  [ loss=0.0254 ][Training] 65/84 [======================>.......] - ETA: 35s  [ loss=0.0271 ][Training] 66/84 [======================>.......] - ETA: 33s  [ loss=0.0230 ][Training] 67/84 [======================>.......] - ETA: 32s  [ loss=0.0272 ][Training] 68/84 [=======================>......] - ETA: 30s  [ loss=0.0257 ][Training] 69/84 [=======================>......] - ETA: 28s  [ loss=0.0248 ][Training] 70/84 [========================>.....] - ETA: 26s  [ loss=0.0306 ][Training] 71/84 [========================>.....] - ETA: 24s  [ loss=0.0224 ][Training] 72/84 [========================>.....] - ETA: 22s  [ loss=0.0212 ][Training] 73/84 [=========================>....] - ETA: 20s  [ loss=0.0223 ][Training] 74/84 [=========================>....] - ETA: 18s  [ loss=0.0249 ][Training] 75/84 [=========================>....] - ETA: 16s  [ loss=0.0232 ][Training] 76/84 [==========================>...] - ETA: 15s  [ loss=0.0225 ][Training] 77/84 [==========================>...] - ETA: 13s  [ loss=0.0264 ][Training] 78/84 [==========================>...] - ETA: 11s  [ loss=0.0239 ][Training] 79/84 [===========================>..] - ETA: 9s  [ loss=0.0239 ][Training] 80/84 [===========================>..] - ETA: 7s  [ loss=0.0244 ][Training] 81/84 [===========================>..] - ETA: 5s  [ loss=0.0237 ][Training] 82/84 [============================>.] - ETA: 3s  [ loss=0.0232 ][Training] 83/84 [============================>.] - ETA: 1s  [ loss=0.0273 ][Training] 84/84 [==============================] 1.9s/step  [ loss=0.0297 ]
03/21/2023 11:54:35 - INFO - __main__ -   


Epoch: 7/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:34  [ loss=0.0262 ][Training] 2/84 [..............................] - ETA: 3:31  [ loss=0.0252 ][Training] 3/84 [>.............................] - ETA: 3:09  [ loss=0.0249 ][Training] 4/84 [>.............................] - ETA: 2:57  [ loss=0.0239 ][Training] 5/84 [>.............................] - ETA: 2:49  [ loss=0.0271 ][Training] 6/84 [=>............................] - ETA: 2:43  [ loss=0.0250 ][Training] 7/84 [=>............................] - ETA: 2:39  [ loss=0.0223 ][Training] 8/84 [=>............................] - ETA: 2:34  [ loss=0.0230 ][Training] 9/84 [==>...........................] - ETA: 2:31  [ loss=0.0253 ][Training] 10/84 [==>...........................] - ETA: 2:28  [ loss=0.0259 ][Training] 11/84 [==>...........................] - ETA: 2:25  [ loss=0.0226 ][Training] 12/84 [===>..........................] - ETA: 2:22  [ loss=0.0224 ] 
03/21/2023 11:54:59 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:54:59 - INFO - __main__ -     Num examples = 723
03/21/2023 11:54:59 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 27s  [ loss=0.0572 ][Evaluating] 1/18 [>.............................] - ETA: 27s[Evaluating] 2/18 [==>...........................] - ETA: 25s  [ loss=0.0745 ][Evaluating] 2/18 [==>...........................] - ETA: 25s[Evaluating] 3/18 [====>.........................] - ETA: 24s  [ loss=0.0529 ][Evaluating] 3/18 [====>.........................] - ETA: 24s[Evaluating] 4/18 [=====>........................] - ETA: 23s  [ loss=0.0473 ][Evaluating] 4/18 [=====>........................] - ETA: 23s[Evaluating] 5/18 [=======>......................] - ETA: 21s  [ loss=0.1082 ][Evaluating] 5/18 [=======>......................] - ETA: 21s[Evaluating] 6/18 [=========>....................] - ETA: 19s  [ loss=0.1131 ][Evaluating] 6/18 [=========>....................] - ETA: 19s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.1321 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 16s  [ loss=0.0641 ][Evaluating] 8/18 [============>.................] - ETA: 16s[Evaluating] 9/18 [==============>...............] - ETA: 14s  [ loss=0.0668 ][Evaluating] 9/18 [==============>...............] - ETA: 14s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0755 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0745 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0925 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0630 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0825 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0706 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0549 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0528 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0937 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:55:30 - INFO - __main__ -   

03/21/2023 11:55:30 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:55:30 - INFO - __main__ -    acc: 0.8581 - recall: 0.8490 - f1: 0.8535 - loss: 0.0764 
03/21/2023 11:55:30 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:55:30 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:55:30 - INFO - __main__ -    acc: 0.7805 - recall: 0.7485 - f1: 0.7642 
03/21/2023 11:55:30 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:55:30 - INFO - __main__ -    acc: 0.7092 - recall: 0.6667 - f1: 0.6873 
03/21/2023 11:55:30 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:55:30 - INFO - __main__ -    acc: 0.8128 - recall: 0.8476 - f1: 0.8298 
03/21/2023 11:55:30 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:55:30 - INFO - __main__ -    acc: 0.9425 - recall: 0.9213 - f1: 0.9318 
03/21/2023 11:55:31 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-600
03/21/2023 11:55:35 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-600
[Training] 13/84 [===>..........................] - ETA: 5:36  [ loss=0.0232 ][Training] 14/84 [====>.........................] - ETA: 5:17  [ loss=0.0269 ][Training] 15/84 [====>.........................] - ETA: 5:00  [ loss=0.0235 ][Training] 16/84 [====>.........................] - ETA: 4:45  [ loss=0.0274 ][Training] 17/84 [=====>........................] - ETA: 4:32  [ loss=0.0239 ][Training] 18/84 [=====>........................] - ETA: 4:20  [ loss=0.0211 ][Training] 19/84 [=====>........................] - ETA: 4:08  [ loss=0.0239 ][Training] 20/84 [======>.......................] - ETA: 3:58  [ loss=0.0232 ][Training] 21/84 [======>.......................] - ETA: 3:49  [ loss=0.0248 ][Training] 22/84 [======>.......................] - ETA: 3:40  [ loss=0.0292 ][Training] 23/84 [=======>......................] - ETA: 3:32  [ loss=0.0248 ][Training] 24/84 [=======>......................] - ETA: 3:25  [ loss=0.0254 ][Training] 25/84 [=======>......................] - ETA: 3:18  [ loss=0.0273 ][Training] 26/84 [========>.....................] - ETA: 3:11  [ loss=0.0214 ][Training] 27/84 [========>.....................] - ETA: 3:04  [ loss=0.0256 ][Training] 28/84 [=========>....................] - ETA: 2:59  [ loss=0.0259 ][Training] 29/84 [=========>....................] - ETA: 2:53  [ loss=0.0237 ][Training] 30/84 [=========>....................] - ETA: 2:48  [ loss=0.0224 ][Training] 31/84 [==========>...................] - ETA: 2:42  [ loss=0.0229 ][Training] 32/84 [==========>...................] - ETA: 2:37  [ loss=0.0236 ][Training] 33/84 [==========>...................] - ETA: 2:33  [ loss=0.0263 ][Training] 34/84 [===========>..................] - ETA: 2:28  [ loss=0.0252 ][Training] 35/84 [===========>..................] - ETA: 2:23  [ loss=0.0220 ][Training] 36/84 [===========>..................] - ETA: 2:19  [ loss=0.0245 ][Training] 37/84 [============>.................] - ETA: 2:15  [ loss=0.0259 ][Training] 38/84 [============>.................] - ETA: 2:11  [ loss=0.0244 ][Training] 39/84 [============>.................] - ETA: 2:07  [ loss=0.0227 ][Training] 40/84 [=============>................] - ETA: 2:03  [ loss=0.0281 ][Training] 41/84 [=============>................] - ETA: 1:59  [ loss=0.0261 ][Training] 42/84 [==============>...............] - ETA: 1:55  [ loss=0.0237 ][Training] 43/84 [==============>...............] - ETA: 1:52  [ loss=0.0224 ][Training] 44/84 [==============>...............] - ETA: 1:48  [ loss=0.0231 ][Training] 45/84 [===============>..............] - ETA: 1:45  [ loss=0.0263 ][Training] 46/84 [===============>..............] - ETA: 1:41  [ loss=0.0227 ][Training] 47/84 [===============>..............] - ETA: 1:38  [ loss=0.0242 ][Training] 48/84 [================>.............] - ETA: 1:35  [ loss=0.0247 ][Training] 49/84 [================>.............] - ETA: 1:32  [ loss=0.0245 ][Training] 50/84 [================>.............] - ETA: 1:28  [ loss=0.0254 ][Training] 51/84 [=================>............] - ETA: 1:25  [ loss=0.0230 ][Training] 52/84 [=================>............] - ETA: 1:22  [ loss=0.0243 ][Training] 53/84 [=================>............] - ETA: 1:19  [ loss=0.0235 ][Training] 54/84 [==================>...........] - ETA: 1:16  [ loss=0.0240 ][Training] 55/84 [==================>...........] - ETA: 1:13  [ loss=0.0229 ][Training] 56/84 [===================>..........] - ETA: 1:10  [ loss=0.0200 ][Training] 57/84 [===================>..........] - ETA: 1:08  [ loss=0.0263 ][Training] 58/84 [===================>..........] - ETA: 1:05  [ loss=0.0250 ][Training] 59/84 [====================>.........] - ETA: 1:02  [ loss=0.0269 ][Training] 60/84 [====================>.........] - ETA: 59s  [ loss=0.0275 ][Training] 61/84 [====================>.........] - ETA: 57s  [ loss=0.0222 ][Training] 62/84 [=====================>........] - ETA: 54s  [ loss=0.0235 ][Training] 63/84 [=====================>........] - ETA: 51s  [ loss=0.0232 ][Training] 64/84 [=====================>........] - ETA: 49s  [ loss=0.0236 ][Training] 65/84 [======================>.......] - ETA: 46s  [ loss=0.0272 ][Training] 66/84 [======================>.......] - ETA: 43s  [ loss=0.0258 ][Training] 67/84 [======================>.......] - ETA: 41s  [ loss=0.0223 ][Training] 68/84 [=======================>......] - ETA: 38s  [ loss=0.0227 ][Training] 69/84 [=======================>......] - ETA: 36s  [ loss=0.0235 ][Training] 70/84 [========================>.....] - ETA: 33s  [ loss=0.0247 ][Training] 71/84 [========================>.....] - ETA: 31s  [ loss=0.0266 ][Training] 72/84 [========================>.....] - ETA: 28s  [ loss=0.0219 ][Training] 73/84 [=========================>....] - ETA: 26s  [ loss=0.0244 ][Training] 74/84 [=========================>....] - ETA: 23s  [ loss=0.0270 ][Training] 75/84 [=========================>....] - ETA: 21s  [ loss=0.0221 ][Training] 76/84 [==========================>...] - ETA: 18s  [ loss=0.0244 ][Training] 77/84 [==========================>...] - ETA: 16s  [ loss=0.0233 ][Training] 78/84 [==========================>...] - ETA: 14s  [ loss=0.0261 ][Training] 79/84 [===========================>..] - ETA: 11s  [ loss=0.0300 ][Training] 80/84 [===========================>..] - ETA: 9s  [ loss=0.0240 ][Training] 81/84 [===========================>..] - ETA: 6s  [ loss=0.0249 ][Training] 82/84 [============================>.] - ETA: 4s  [ loss=0.0277 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0245 ][Training] 84/84 [==============================] 2.3s/step  [ loss=0.0216 ]
03/21/2023 11:57:49 - INFO - __main__ -   


Epoch: 8/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:17  [ loss=0.0242 ][Training] 2/84 [..............................] - ETA: 3:25  [ loss=0.0197 ][Training] 3/84 [>.............................] - ETA: 3:05  [ loss=0.0251 ][Training] 4/84 [>.............................] - ETA: 2:54  [ loss=0.0213 ][Training] 5/84 [>.............................] - ETA: 2:46  [ loss=0.0245 ][Training] 6/84 [=>............................] - ETA: 2:41  [ loss=0.0282 ][Training] 7/84 [=>............................] - ETA: 2:36  [ loss=0.0255 ][Training] 8/84 [=>............................] - ETA: 2:33  [ loss=0.0222 ][Training] 9/84 [==>...........................] - ETA: 2:30  [ loss=0.0276 ][Training] 10/84 [==>...........................] - ETA: 2:27  [ loss=0.0251 ][Training] 11/84 [==>...........................] - ETA: 2:24  [ loss=0.0250 ][Training] 12/84 [===>..........................] - ETA: 2:21  [ loss=0.0243 ][Training] 13/84 [===>..........................] - ETA: 2:18  [ loss=0.0226 ][Training] 14/84 [====>.........................] - ETA: 2:16  [ loss=0.0241 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0251 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0225 ][Training] 17/84 [=====>........................] - ETA: 2:09  [ loss=0.0241 ][Training] 18/84 [=====>........................] - ETA: 2:07  [ loss=0.0229 ][Training] 19/84 [=====>........................] - ETA: 2:05  [ loss=0.0216 ][Training] 20/84 [======>.......................] - ETA: 2:03  [ loss=0.0213 ][Training] 21/84 [======>.......................] - ETA: 2:00  [ loss=0.0250 ][Training] 22/84 [======>.......................] - ETA: 1:58  [ loss=0.0253 ][Training] 23/84 [=======>......................] - ETA: 1:56  [ loss=0.0215 ][Training] 24/84 [=======>......................] - ETA: 1:54  [ loss=0.0224 ][Training] 25/84 [=======>......................] - ETA: 1:52  [ loss=0.0237 ][Training] 26/84 [========>.....................] - ETA: 1:50  [ loss=0.0222 ][Training] 27/84 [========>.....................] - ETA: 1:48  [ loss=0.0220 ][Training] 28/84 [=========>....................] - ETA: 1:46  [ loss=0.0237 ] 
03/21/2023 11:58:42 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:58:42 - INFO - __main__ -     Num examples = 723
03/21/2023 11:58:42 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 29s  [ loss=0.0570 ][Evaluating] 1/18 [>.............................] - ETA: 29s[Evaluating] 2/18 [==>...........................] - ETA: 26s  [ loss=0.0772 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0509 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0457 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.1067 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.1169 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.1368 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 16s  [ loss=0.0631 ][Evaluating] 8/18 [============>.................] - ETA: 16s[Evaluating] 9/18 [==============>...............] - ETA: 14s  [ loss=0.0717 ][Evaluating] 9/18 [==============>...............] - ETA: 14s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0754 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0727 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0968 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0582 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0815 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0743 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0547 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0547 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.1025 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:59:13 - INFO - __main__ -   

03/21/2023 11:59:13 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:59:13 - INFO - __main__ -    acc: 0.8638 - recall: 0.8566 - f1: 0.8602 - loss: 0.0776 
03/21/2023 11:59:13 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:59:13 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:59:13 - INFO - __main__ -    acc: 0.7941 - recall: 0.7895 - f1: 0.7918 
03/21/2023 11:59:13 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:59:13 - INFO - __main__ -    acc: 0.7021 - recall: 0.6600 - f1: 0.6804 
03/21/2023 11:59:13 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:59:13 - INFO - __main__ -    acc: 0.8464 - recall: 0.8396 - f1: 0.8430 
03/21/2023 11:59:13 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:59:13 - INFO - __main__ -    acc: 0.9296 - recall: 0.9326 - f1: 0.9311 
03/21/2023 11:59:15 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-700
03/21/2023 11:59:18 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-700
03/21/2023 11:59:18 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:59:36 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 29/84 [=========>....................] - ETA: 3:25  [ loss=0.0273 ][Training] 30/84 [=========>....................] - ETA: 3:18  [ loss=0.0235 ][Training] 31/84 [==========>...................] - ETA: 3:12  [ loss=0.0229 ][Training] 32/84 [==========>...................] - ETA: 3:05  [ loss=0.0219 ][Training] 33/84 [==========>...................] - ETA: 2:59  [ loss=0.0224 ][Training] 34/84 [===========>..................] - ETA: 2:53  [ loss=0.0257 ][Training] 35/84 [===========>..................] - ETA: 2:48  [ loss=0.0245 ][Training] 36/84 [===========>..................] - ETA: 2:42  [ loss=0.0288 ][Training] 37/84 [============>.................] - ETA: 2:37  [ loss=0.0263 ][Training] 38/84 [============>.................] - ETA: 2:32  [ loss=0.0287 ][Training] 39/84 [============>.................] - ETA: 2:27  [ loss=0.0216 ][Training] 40/84 [=============>................] - ETA: 2:22  [ loss=0.0241 ][Training] 41/84 [=============>................] - ETA: 2:18  [ loss=0.0233 ][Training] 42/84 [==============>...............] - ETA: 2:13  [ loss=0.0264 ][Training] 43/84 [==============>...............] - ETA: 2:09  [ loss=0.0233 ][Training] 44/84 [==============>...............] - ETA: 2:05  [ loss=0.0219 ][Training] 45/84 [===============>..............] - ETA: 2:00  [ loss=0.0243 ][Training] 46/84 [===============>..............] - ETA: 1:56  [ loss=0.0231 ][Training] 47/84 [===============>..............] - ETA: 1:52  [ loss=0.0251 ][Training] 48/84 [================>.............] - ETA: 1:48  [ loss=0.0240 ][Training] 49/84 [================>.............] - ETA: 1:44  [ loss=0.0245 ][Training] 50/84 [================>.............] - ETA: 1:41  [ loss=0.0261 ][Training] 51/84 [=================>............] - ETA: 1:37  [ loss=0.0240 ][Training] 52/84 [=================>............] - ETA: 1:33  [ loss=0.0238 ][Training] 53/84 [=================>............] - ETA: 1:30  [ loss=0.0253 ][Training] 54/84 [==================>...........] - ETA: 1:26  [ loss=0.0212 ][Training] 55/84 [==================>...........] - ETA: 1:23  [ loss=0.0222 ][Training] 56/84 [===================>..........] - ETA: 1:20  [ loss=0.0259 ][Training] 57/84 [===================>..........] - ETA: 1:16  [ loss=0.0268 ][Training] 58/84 [===================>..........] - ETA: 1:13  [ loss=0.0250 ][Training] 59/84 [====================>.........] - ETA: 1:10  [ loss=0.0255 ][Training] 60/84 [====================>.........] - ETA: 1:07  [ loss=0.0236 ][Training] 61/84 [====================>.........] - ETA: 1:04  [ loss=0.0235 ][Training] 62/84 [=====================>........] - ETA: 1:01  [ loss=0.0235 ][Training] 63/84 [=====================>........] - ETA: 58s  [ loss=0.0209 ][Training] 64/84 [=====================>........] - ETA: 54s  [ loss=0.0236 ][Training] 65/84 [======================>.......] - ETA: 51s  [ loss=0.0229 ][Training] 66/84 [======================>.......] - ETA: 49s  [ loss=0.0216 ][Training] 67/84 [======================>.......] - ETA: 46s  [ loss=0.0226 ][Training] 68/84 [=======================>......] - ETA: 43s  [ loss=0.0232 ][Training] 69/84 [=======================>......] - ETA: 40s  [ loss=0.0249 ][Training] 70/84 [========================>.....] - ETA: 37s  [ loss=0.0253 ][Training] 71/84 [========================>.....] - ETA: 34s  [ loss=0.0235 ][Training] 72/84 [========================>.....] - ETA: 31s  [ loss=0.0250 ][Training] 73/84 [=========================>....] - ETA: 29s  [ loss=0.0260 ][Training] 74/84 [=========================>....] - ETA: 26s  [ loss=0.0254 ][Training] 75/84 [=========================>....] - ETA: 23s  [ loss=0.0233 ][Training] 76/84 [==========================>...] - ETA: 20s  [ loss=0.0200 ][Training] 77/84 [==========================>...] - ETA: 18s  [ loss=0.0205 ][Training] 78/84 [==========================>...] - ETA: 15s  [ loss=0.0241 ][Training] 79/84 [===========================>..] - ETA: 12s  [ loss=0.0248 ][Training] 80/84 [===========================>..] - ETA: 10s  [ loss=0.0222 ][Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0233 ][Training] 82/84 [============================>.] - ETA: 5s  [ loss=0.0257 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0202 ][Training] 84/84 [==============================] 2.6s/step  [ loss=0.0256 ]
03/21/2023 12:01:24 - INFO - __main__ -   


Epoch: 9/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:32  [ loss=0.0265 ][Training] 2/84 [..............................] - ETA: 3:37  [ loss=0.0241 ][Training] 3/84 [>.............................] - ETA: 3:16  [ loss=0.0271 ][Training] 4/84 [>.............................] - ETA: 3:04  [ loss=0.0217 ][Training] 5/84 [>.............................] - ETA: 2:57  [ loss=0.0259 ][Training] 6/84 [=>............................] - ETA: 2:50  [ loss=0.0202 ][Training] 7/84 [=>............................] - ETA: 2:45  [ loss=0.0257 ][Training] 8/84 [=>............................] - ETA: 2:42  [ loss=0.0238 ][Training] 9/84 [==>...........................] - ETA: 2:39  [ loss=0.0217 ][Training] 10/84 [==>...........................] - ETA: 2:36  [ loss=0.0205 ][Training] 11/84 [==>...........................] - ETA: 2:33  [ loss=0.0234 ][Training] 12/84 [===>..........................] - ETA: 2:30  [ loss=0.0251 ][Training] 13/84 [===>..........................] - ETA: 2:28  [ loss=0.0248 ][Training] 14/84 [====>.........................] - ETA: 2:26  [ loss=0.0257 ][Training] 15/84 [====>.........................] - ETA: 2:23  [ loss=0.0214 ][Training] 16/84 [====>.........................] - ETA: 2:21  [ loss=0.0216 ][Training] 17/84 [=====>........................] - ETA: 2:18  [ loss=0.0222 ][Training] 18/84 [=====>........................] - ETA: 2:16  [ loss=0.0216 ][Training] 19/84 [=====>........................] - ETA: 2:13  [ loss=0.0251 ][Training] 20/84 [======>.......................] - ETA: 2:11  [ loss=0.0220 ][Training] 21/84 [======>.......................] - ETA: 2:09  [ loss=0.0232 ][Training] 22/84 [======>.......................] - ETA: 2:07  [ loss=0.0257 ][Training] 23/84 [=======>......................] - ETA: 2:04  [ loss=0.0218 ][Training] 24/84 [=======>......................] - ETA: 2:02  [ loss=0.0246 ][Training] 25/84 [=======>......................] - ETA: 2:00  [ loss=0.0254 ][Training] 26/84 [========>.....................] - ETA: 1:58  [ loss=0.0230 ][Training] 27/84 [========>.....................] - ETA: 1:55  [ loss=0.0224 ][Training] 28/84 [=========>....................] - ETA: 1:53  [ loss=0.0239 ][Training] 29/84 [=========>....................] - ETA: 1:51  [ loss=0.0257 ][Training] 30/84 [=========>....................] - ETA: 1:49  [ loss=0.0251 ][Training] 31/84 [==========>...................] - ETA: 1:47  [ loss=0.0225 ][Training] 32/84 [==========>...................] - ETA: 1:45  [ loss=0.0235 ][Training] 33/84 [==========>...................] - ETA: 1:42  [ loss=0.0207 ][Training] 34/84 [===========>..................] - ETA: 1:40  [ loss=0.0245 ][Training] 35/84 [===========>..................] - ETA: 1:38  [ loss=0.0232 ][Training] 36/84 [===========>..................] - ETA: 1:36  [ loss=0.0246 ][Training] 37/84 [============>.................] - ETA: 1:34  [ loss=0.0241 ][Training] 38/84 [============>.................] - ETA: 1:32  [ loss=0.0244 ][Training] 39/84 [============>.................] - ETA: 1:30  [ loss=0.0238 ][Training] 40/84 [=============>................] - ETA: 1:28  [ loss=0.0233 ][Training] 41/84 [=============>................] - ETA: 1:26  [ loss=0.0231 ][Training] 42/84 [==============>...............] - ETA: 1:24  [ loss=0.0223 ][Training] 43/84 [==============>...............] - ETA: 1:21  [ loss=0.0225 ][Training] 44/84 [==============>...............] - ETA: 1:19  [ loss=0.0245 ] 
03/21/2023 12:02:52 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 12:02:52 - INFO - __main__ -     Num examples = 723
03/21/2023 12:02:52 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 30s  [ loss=0.0564 ][Evaluating] 1/18 [>.............................] - ETA: 30s[Evaluating] 2/18 [==>...........................] - ETA: 27s  [ loss=0.0786 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0496 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0446 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.1131 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.1160 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 19s  [ loss=0.1382 ][Evaluating] 7/18 [==========>...................] - ETA: 19s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0637 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0674 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 14s  [ loss=0.0737 ][Evaluating] 10/18 [===============>..............] - ETA: 14s[Evaluating] 11/18 [=================>............] - ETA: 12s  [ loss=0.0727 ][Evaluating] 11/18 [=================>............] - ETA: 12s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0954 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0614 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 7s  [ loss=0.0856 ][Evaluating] 14/18 [======================>.......] - ETA: 7s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0733 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0547 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0548 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.8s/step  [ loss=0.1014 ]
[Evaluating] 18/18 [==============================] 1.8s/step
03/21/2023 12:03:24 - INFO - __main__ -   

03/21/2023 12:03:24 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 12:03:24 - INFO - __main__ -    acc: 0.8593 - recall: 0.8619 - f1: 0.8606 - loss: 0.0778 
03/21/2023 12:03:24 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 12:03:24 - INFO - __main__ -   ******* LOC results ********
03/21/2023 12:03:24 - INFO - __main__ -    acc: 0.7953 - recall: 0.7953 - f1: 0.7953 
03/21/2023 12:03:24 - INFO - __main__ -   ******* MISC results ********
03/21/2023 12:03:24 - INFO - __main__ -    acc: 0.7174 - recall: 0.6600 - f1: 0.6875 
03/21/2023 12:03:24 - INFO - __main__ -   ******* ORG results ********
03/21/2023 12:03:24 - INFO - __main__ -    acc: 0.8065 - recall: 0.8583 - f1: 0.8316 
03/21/2023 12:03:24 - INFO - __main__ -   ******* PER results ********
03/21/2023 12:03:24 - INFO - __main__ -    acc: 0.9431 - recall: 0.9310 - f1: 0.9370 
03/21/2023 12:03:26 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-800
03/21/2023 12:03:29 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-800
03/21/2023 12:03:29 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 12:03:47 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 45/84 [===============>..............] - ETA: 2:05  [ loss=0.0248 ][Training] 46/84 [===============>..............] - ETA: 2:00  [ loss=0.0251 ][Training] 47/84 [===============>..............] - ETA: 1:56  [ loss=0.0230 ][Training] 48/84 [================>.............] - ETA: 1:52  [ loss=0.0227 ][Training] 49/84 [================>.............] - ETA: 1:48  [ loss=0.0218 ][Training] 50/84 [================>.............] - ETA: 1:44  [ loss=0.0256 ][Training] 51/84 [=================>............] - ETA: 1:40  [ loss=0.0257 ][Training] 52/84 [=================>............] - ETA: 1:36  [ loss=0.0262 ][Training] 53/84 [=================>............] - ETA: 1:33  [ loss=0.0242 ][Training] 54/84 [==================>...........] - ETA: 1:29  [ loss=0.0229 ][Training] 55/84 [==================>...........] - ETA: 1:25  [ loss=0.0246 ][Training] 56/84 [===================>..........] - ETA: 1:22  [ loss=0.0233 ][Training] 57/84 [===================>..........] - ETA: 1:19  [ loss=0.0233 ][Training] 58/84 [===================>..........] - ETA: 1:15  [ loss=0.0240 ][Training] 59/84 [====================>.........] - ETA: 1:12  [ loss=0.0242 ][Training] 60/84 [====================>.........] - ETA: 1:09  [ loss=0.0222 ][Training] 61/84 [====================>.........] - ETA: 1:05  [ loss=0.0217 ][Training] 62/84 [=====================>........] - ETA: 1:02  [ loss=0.0255 ][Training] 63/84 [=====================>........] - ETA: 59s  [ loss=0.0198 ][Training] 64/84 [=====================>........] - ETA: 56s  [ loss=0.0221 ][Training] 65/84 [======================>.......] - ETA: 53s  [ loss=0.0249 ][Training] 66/84 [======================>.......] - ETA: 50s  [ loss=0.0267 ][Training] 67/84 [======================>.......] - ETA: 47s  [ loss=0.0226 ][Training] 68/84 [=======================>......] - ETA: 44s  [ loss=0.0239 ][Training] 69/84 [=======================>......] - ETA: 41s  [ loss=0.0215 ][Training] 70/84 [========================>.....] - ETA: 38s  [ loss=0.0227 ][Training] 71/84 [========================>.....] - ETA: 35s  [ loss=0.0214 ][Training] 72/84 [========================>.....] - ETA: 32s  [ loss=0.0257 ][Training] 73/84 [=========================>....] - ETA: 29s  [ loss=0.0274 ][Training] 74/84 [=========================>....] - ETA: 26s  [ loss=0.0216 ][Training] 75/84 [=========================>....] - ETA: 24s  [ loss=0.0250 ][Training] 76/84 [==========================>...] - ETA: 21s  [ loss=0.0225 ][Training] 77/84 [==========================>...] - ETA: 18s  [ loss=0.0192 ][Training] 78/84 [==========================>...] - ETA: 15s  [ loss=0.0226 ][Training] 79/84 [===========================>..] - ETA: 13s  [ loss=0.0251 ][Training] 80/84 [===========================>..] - ETA: 10s  [ loss=0.0202 ][Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0261 ][Training] 82/84 [============================>.] - ETA: 5s  [ loss=0.0238 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0234 ][Training] 84/84 [==============================] 2.6s/step  [ loss=0.0244 ]
03/21/2023 12:05:02 - INFO - __main__ -   

03/21/2023 12:05:02 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 12:05:06 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 12:05:08 - INFO - __main__ -   Test on best eval model
03/21/2023 12:05:09 - INFO - __main__ -   ***** Running evaluation test best *****
03/21/2023 12:05:09 - INFO - __main__ -     Num examples = 723
03/21/2023 12:05:09 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 28s  [ loss=0.0811 ][Evaluating] 1/18 [>.............................] - ETA: 28s[Evaluating] 2/18 [==>...........................] - ETA: 26s  [ loss=0.0571 ][Evaluating] 2/18 [==>...........................] - ETA: 26s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0819 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 23s  [ loss=0.0482 ][Evaluating] 4/18 [=====>........................] - ETA: 23s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.1163 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.0659 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.0738 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0460 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0729 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0558 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 12s  [ loss=0.0703 ][Evaluating] 11/18 [=================>............] - ETA: 12s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0882 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0909 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.1255 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0566 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0903 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0944 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.8s/step  [ loss=0.0683 ]
[Evaluating] 18/18 [==============================] 1.8s/step
03/21/2023 12:05:41 - INFO - __main__ -   

03/21/2023 12:05:41 - INFO - __main__ -   ***** Eval results test best *****
03/21/2023 12:05:41 - INFO - __main__ -    acc: 0.8669 - recall: 0.8694 - f1: 0.8681 - loss: 0.0768 
03/21/2023 12:05:41 - INFO - __main__ -   ***** Entity results test best *****
03/21/2023 12:05:41 - INFO - __main__ -   ******* LOC results ********
03/21/2023 12:05:41 - INFO - __main__ -    acc: 0.8531 - recall: 0.8483 - f1: 0.8507 
03/21/2023 12:05:41 - INFO - __main__ -   ******* MISC results ********
03/21/2023 12:05:41 - INFO - __main__ -    acc: 0.7429 - recall: 0.6667 - f1: 0.7027 
03/21/2023 12:05:41 - INFO - __main__ -   ******* ORG results ********
03/21/2023 12:05:41 - INFO - __main__ -    acc: 0.8213 - recall: 0.8608 - f1: 0.8405 
03/21/2023 12:05:41 - INFO - __main__ -   ******* PER results ********
03/21/2023 12:05:41 - INFO - __main__ -    acc: 0.9291 - recall: 0.9321 - f1: 0.9306 
03/21/2023 12:05:41 - INFO - __main__ -   Test on best eval loss model
03/21/2023 12:05:42 - INFO - __main__ -   ***** Running evaluation test best_loss *****
03/21/2023 12:05:42 - INFO - __main__ -     Num examples = 723
03/21/2023 12:05:42 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 29s  [ loss=0.0617 ][Evaluating] 1/18 [>.............................] - ETA: 30s[Evaluating] 2/18 [==>...........................] - ETA: 27s  [ loss=0.0555 ][Evaluating] 2/18 [==>...........................] - ETA: 28s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0569 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 23s  [ loss=0.0436 ][Evaluating] 4/18 [=====>........................] - ETA: 23s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.0659 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.0594 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.0605 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0468 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0657 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 14s  [ loss=0.0447 ][Evaluating] 10/18 [===============>..............] - ETA: 14s[Evaluating] 11/18 [=================>............] - ETA: 12s  [ loss=0.0507 ][Evaluating] 11/18 [=================>............] - ETA: 12s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0671 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0654 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0869 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0503 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0598 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0752 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.8s/step  [ loss=0.0723 ]
[Evaluating] 18/18 [==============================] 1.8s/step
03/21/2023 12:06:14 - INFO - __main__ -   

03/21/2023 12:06:14 - INFO - __main__ -   ***** Eval results test best_loss *****
03/21/2023 12:06:14 - INFO - __main__ -    acc: 0.8460 - recall: 0.8561 - f1: 0.8510 - loss: 0.0605 
03/21/2023 12:06:14 - INFO - __main__ -   ***** Entity results test best_loss *****
03/21/2023 12:06:14 - INFO - __main__ -   ******* LOC results ********
03/21/2023 12:06:14 - INFO - __main__ -    acc: 0.8786 - recall: 0.8539 - f1: 0.8661 
03/21/2023 12:06:14 - INFO - __main__ -   ******* MISC results ********
03/21/2023 12:06:14 - INFO - __main__ -    acc: 0.5965 - recall: 0.6538 - f1: 0.6239 
03/21/2023 12:06:14 - INFO - __main__ -   ******* ORG results ********
03/21/2023 12:06:14 - INFO - __main__ -    acc: 0.8111 - recall: 0.8481 - f1: 0.8292 
03/21/2023 12:06:14 - INFO - __main__ -   ******* PER results ********
03/21/2023 12:06:14 - INFO - __main__ -    acc: 0.9308 - recall: 0.9128 - f1: 0.9217 
03/21/2023 12:06:14 - INFO - __main__ -   Test on last eval model
03/21/2023 12:06:15 - INFO - __main__ -   ***** Running evaluation test last *****
03/21/2023 12:06:15 - INFO - __main__ -     Num examples = 723
03/21/2023 12:06:15 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 27s  [ loss=0.0817 ][Evaluating] 1/18 [>.............................] - ETA: 27s[Evaluating] 2/18 [==>...........................] - ETA: 27s  [ loss=0.0567 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 26s  [ loss=0.0817 ][Evaluating] 3/18 [====>.........................] - ETA: 26s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0485 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.1168 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 21s  [ loss=0.0662 ][Evaluating] 6/18 [=========>....................] - ETA: 21s[Evaluating] 7/18 [==========>...................] - ETA: 19s  [ loss=0.0742 ][Evaluating] 7/18 [==========>...................] - ETA: 19s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0460 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0729 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0558 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 12s  [ loss=0.0713 ][Evaluating] 11/18 [=================>............] - ETA: 12s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0886 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0906 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.1250 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0562 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0899 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0954 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0678 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 12:06:46 - INFO - __main__ -   

03/21/2023 12:06:46 - INFO - __main__ -   ***** Eval results test last *****
03/21/2023 12:06:46 - INFO - __main__ -    acc: 0.8701 - recall: 0.8694 - f1: 0.8698 - loss: 0.0770 
03/21/2023 12:06:46 - INFO - __main__ -   ***** Entity results test last *****
03/21/2023 12:06:46 - INFO - __main__ -   ******* LOC results ********
03/21/2023 12:06:46 - INFO - __main__ -    acc: 0.8629 - recall: 0.8483 - f1: 0.8555 
03/21/2023 12:06:46 - INFO - __main__ -   ******* MISC results ********
03/21/2023 12:06:46 - INFO - __main__ -    acc: 0.7536 - recall: 0.6667 - f1: 0.7075 
03/21/2023 12:06:46 - INFO - __main__ -   ******* ORG results ********
03/21/2023 12:06:46 - INFO - __main__ -    acc: 0.8188 - recall: 0.8582 - f1: 0.8381 
03/21/2023 12:06:46 - INFO - __main__ -   ******* PER results ********
03/21/2023 12:06:46 - INFO - __main__ -    acc: 0.9323 - recall: 0.9338 - f1: 0.9330 
