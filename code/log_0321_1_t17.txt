nohup: ignoring input
03/21/2023 11:30:45 - INFO - __main__ -   label_list: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[START]', '[END]'], length: 12
The number of samples: 3373
The number of images: 3373
The number of samples: 723
The number of images: 723
The number of samples: 723
The number of images: 723
03/21/2023 11:30:45 - INFO - root -   Constructing vocabulary for image caption
03/21/2023 11:30:45 - INFO - __main__ -    The size of vocabulary = 3524
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/21/2023 11:31:04 - INFO - __main__ -   Args: Namespace(adam_epsilon=1e-08, alpha=0.0001, bert_type='uncased', beta=0.0001, cls_init=0, crf_dropout=0.5, crf_learning_rate=0.0001, crop_size=224, cross_dropout=0.2, data_dir='../data/twitter2017', device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, drop_last=True, eval_all_checkpoints=False, evaluate_during_training=True, gradient_accumulation_steps=1, hidden_size=768, id2label={0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC', 9: 'X', 10: '[START]', 11: '[END]'}, image_dir='../data/twitter2017_images', image_dropout=0.0, label2id={'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'X': 9, '[START]': 10, '[END]': 11}, learning_rate=0.0001, load_image_checkpoint=False, load_text_checkpoint=False, local_rank=-1, logging_steps=100, markup='bio', max_grad_norm=1.0, max_seq_length=64, n_gpu=1, num_layers=6, num_train_epochs=10, num_workers=8, output_dir='../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/', per_gpu_eval_batch_size=40, per_gpu_train_batch_size=40, predict_checkpoints=0, replace_end=3, replace_start=1, resnet_pretrained_dir='../models/resnet152-b121ed2d.pth', save_steps=100, seed=42, sigma=1.0, skip_connection=True, task='twitter17', test_batch_size=1, text_dropout=0.0, theta=0.1, ti_crop_size=32, train_batch_size=40, use_quantile=True, use_xlmr=False, warmup_proportion=0.1, weight_decay=0.01)
03/21/2023 11:31:04 - INFO - __main__ -   Summary dir: ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/summary_1679369464
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
03/21/2023 11:31:04 - INFO - __main__ -   ***** Running training *****
03/21/2023 11:31:04 - INFO - __main__ -     Num examples = 3373
03/21/2023 11:31:04 - INFO - __main__ -     Num Epochs = 10
03/21/2023 11:31:04 - INFO - __main__ -     Gradient Accumulation steps = 1
03/21/2023 11:31:04 - INFO - __main__ -     Total optimization steps = 840

Epoch: 0/10
/home/ubuntu/multimodal-fusion/MuSE/code/models.py:1235: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[Training] 1/84 [..............................] - ETA: 5:27  [ loss=0.9246 ][Training] 2/84 [..............................] - ETA: 4:26  [ loss=0.8644 ][Training] 3/84 [>.............................] - ETA: 3:45  [ loss=0.8212 ][Training] 4/84 [>.............................] - ETA: 3:32  [ loss=0.8370 ][Training] 5/84 [>.............................] - ETA: 3:19  [ loss=0.8017 ][Training] 6/84 [=>............................] - ETA: 3:08  [ loss=0.6879 ][Training] 7/84 [=>............................] - ETA: 2:59  [ loss=0.5904 ][Training] 8/84 [=>............................] - ETA: 2:52  [ loss=0.4626 ][Training] 9/84 [==>...........................] - ETA: 2:46  [ loss=0.4870 ][Training] 10/84 [==>...........................] - ETA: 2:41  [ loss=0.3826 ][Training] 11/84 [==>...........................] - ETA: 2:37  [ loss=0.3814 ][Training] 12/84 [===>..........................] - ETA: 2:33  [ loss=0.5147 ][Training] 13/84 [===>..........................] - ETA: 2:29  [ loss=0.4040 ][Training] 14/84 [====>.........................] - ETA: 2:26  [ loss=0.4291 ][Training] 15/84 [====>.........................] - ETA: 2:23  [ loss=0.4392 ][Training] 16/84 [====>.........................] - ETA: 2:20  [ loss=0.4336 ][Training] 17/84 [=====>........................] - ETA: 2:17  [ loss=0.3931 ][Training] 18/84 [=====>........................] - ETA: 2:14  [ loss=0.4037 ][Training] 19/84 [=====>........................] - ETA: 2:11  [ loss=0.3687 ][Training] 20/84 [======>.......................] - ETA: 2:08  [ loss=0.3900 ][Training] 21/84 [======>.......................] - ETA: 2:06  [ loss=0.4002 ][Training] 22/84 [======>.......................] - ETA: 2:03  [ loss=0.3715 ][Training] 23/84 [=======>......................] - ETA: 2:01  [ loss=0.3724 ][Training] 24/84 [=======>......................] - ETA: 1:59  [ loss=0.3338 ][Training] 25/84 [=======>......................] - ETA: 1:56  [ loss=0.4201 ][Training] 26/84 [========>.....................] - ETA: 1:54  [ loss=0.3883 ][Training] 27/84 [========>.....................] - ETA: 1:52  [ loss=0.3782 ][Training] 28/84 [=========>....................] - ETA: 1:50  [ loss=0.3754 ][Training] 29/84 [=========>....................] - ETA: 1:48  [ loss=0.3406 ][Training] 30/84 [=========>....................] - ETA: 1:46  [ loss=0.3406 ][Training] 31/84 [==========>...................] - ETA: 1:43  [ loss=0.3474 ][Training] 32/84 [==========>...................] - ETA: 1:41  [ loss=0.3849 ][Training] 33/84 [==========>...................] - ETA: 1:39  [ loss=0.3429 ][Training] 34/84 [===========>..................] - ETA: 1:37  [ loss=0.3952 ][Training] 35/84 [===========>..................] - ETA: 1:35  [ loss=0.2807 ][Training] 36/84 [===========>..................] - ETA: 1:33  [ loss=0.2823 ][Training] 37/84 [============>.................] - ETA: 1:30  [ loss=0.3253 ][Training] 38/84 [============>.................] - ETA: 1:28  [ loss=0.2298 ][Training] 39/84 [============>.................] - ETA: 1:26  [ loss=0.2384 ][Training] 40/84 [=============>................] - ETA: 1:24  [ loss=0.2380 ][Training] 41/84 [=============>................] - ETA: 1:22  [ loss=0.2464 ][Training] 42/84 [==============>...............] - ETA: 1:20  [ loss=0.2120 ][Training] 43/84 [==============>...............] - ETA: 1:18  [ loss=0.1975 ][Training] 44/84 [==============>...............] - ETA: 1:16  [ loss=0.1632 ][Training] 45/84 [===============>..............] - ETA: 1:14  [ loss=0.2104 ][Training] 46/84 [===============>..............] - ETA: 1:12  [ loss=0.1782 ][Training] 47/84 [===============>..............] - ETA: 1:10  [ loss=0.1912 ][Training] 48/84 [================>.............] - ETA: 1:08  [ loss=0.1947 ][Training] 49/84 [================>.............] - ETA: 1:06  [ loss=0.1659 ][Training] 50/84 [================>.............] - ETA: 1:04  [ loss=0.1824 ][Training] 51/84 [=================>............] - ETA: 1:03  [ loss=0.1576 ][Training] 52/84 [=================>............] - ETA: 1:01  [ loss=0.1531 ][Training] 53/84 [=================>............] - ETA: 59s  [ loss=0.1387 ][Training] 54/84 [==================>...........] - ETA: 57s  [ loss=0.1062 ][Training] 55/84 [==================>...........] - ETA: 55s  [ loss=0.1282 ][Training] 56/84 [===================>..........] - ETA: 53s  [ loss=0.1353 ][Training] 57/84 [===================>..........] - ETA: 51s  [ loss=0.1207 ][Training] 58/84 [===================>..........] - ETA: 49s  [ loss=0.1348 ][Training] 59/84 [====================>.........] - ETA: 47s  [ loss=0.1131 ][Training] 60/84 [====================>.........] - ETA: 45s  [ loss=0.1263 ][Training] 61/84 [====================>.........] - ETA: 43s  [ loss=0.1087 ][Training] 62/84 [=====================>........] - ETA: 41s  [ loss=0.1571 ][Training] 63/84 [=====================>........] - ETA: 39s  [ loss=0.1403 ][Training] 64/84 [=====================>........] - ETA: 38s  [ loss=0.1224 ][Training] 65/84 [======================>.......] - ETA: 36s  [ loss=0.1383 ][Training] 66/84 [======================>.......] - ETA: 34s  [ loss=0.1154 ][Training] 67/84 [======================>.......] - ETA: 32s  [ loss=0.1036 ][Training] 68/84 [=======================>......] - ETA: 30s  [ loss=0.1125 ][Training] 69/84 [=======================>......] - ETA: 28s  [ loss=0.0848 ][Training] 70/84 [========================>.....] - ETA: 26s  [ loss=0.1426 ][Training] 71/84 [========================>.....] - ETA: 24s  [ loss=0.0823 ][Training] 72/84 [========================>.....] - ETA: 22s  [ loss=0.0921 ][Training] 73/84 [=========================>....] - ETA: 20s  [ loss=0.1095 ][Training] 74/84 [=========================>....] - ETA: 18s  [ loss=0.1156 ][Training] 75/84 [=========================>....] - ETA: 16s  [ loss=0.1012 ][Training] 76/84 [==========================>...] - ETA: 15s  [ loss=0.1204 ][Training] 77/84 [==========================>...] - ETA: 13s  [ loss=0.0938 ][Training] 78/84 [==========================>...] - ETA: 11s  [ loss=0.0944 ][Training] 79/84 [===========================>..] - ETA: 9s  [ loss=0.1094 ][Training] 80/84 [===========================>..] - ETA: 7s  [ loss=0.0835 ][Training] 81/84 [===========================>..] - ETA: 5s  [ loss=0.0905 ][Training] 82/84 [============================>.] - ETA: 3s  [ loss=0.0723 ][Training] 83/84 [============================>.] - ETA: 1s  [ loss=0.1021 ][Training] 84/84 [==============================] 1.9s/step  [ loss=0.0979 ]
03/21/2023 11:33:42 - INFO - __main__ -   


Epoch: 1/10
[Training] 1/84 [..............................] - ETA: 4:17  [ loss=0.0971 ][Training] 2/84 [..............................] - ETA: 3:30  [ loss=0.0893 ][Training] 3/84 [>.............................] - ETA: 3:08  [ loss=0.0738 ][Training] 4/84 [>.............................] - ETA: 2:56  [ loss=0.1003 ][Training] 5/84 [>.............................] - ETA: 2:51  [ loss=0.0950 ][Training] 6/84 [=>............................] - ETA: 2:45  [ loss=0.0890 ][Training] 7/84 [=>............................] - ETA: 2:40  [ loss=0.0848 ][Training] 8/84 [=>............................] - ETA: 2:35  [ loss=0.0685 ][Training] 9/84 [==>...........................] - ETA: 2:31  [ loss=0.0875 ][Training] 10/84 [==>...........................] - ETA: 2:28  [ loss=0.0648 ][Training] 11/84 [==>...........................] - ETA: 2:25  [ loss=0.0828 ][Training] 12/84 [===>..........................] - ETA: 2:22  [ loss=0.0683 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0748 ][Training] 14/84 [====>.........................] - ETA: 2:17  [ loss=0.0863 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0590 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0776 ] 
03/21/2023 11:34:14 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:34:14 - INFO - __main__ -     Num examples = 723
03/21/2023 11:34:14 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 25s  [ loss=0.0719 ][Evaluating] 1/18 [>.............................] - ETA: 25s[Evaluating] 2/18 [==>...........................] - ETA: 23s  [ loss=0.0884 ][Evaluating] 2/18 [==>...........................] - ETA: 23s[Evaluating] 3/18 [====>.........................] - ETA: 22s  [ loss=0.0789 ][Evaluating] 3/18 [====>.........................] - ETA: 22s[Evaluating] 4/18 [=====>........................] - ETA: 21s  [ loss=0.0687 ][Evaluating] 4/18 [=====>........................] - ETA: 21s[Evaluating] 5/18 [=======>......................] - ETA: 20s  [ loss=0.0889 ][Evaluating] 5/18 [=======>......................] - ETA: 20s[Evaluating] 6/18 [=========>....................] - ETA: 19s  [ loss=0.0964 ][Evaluating] 6/18 [=========>....................] - ETA: 19s[Evaluating] 7/18 [==========>...................] - ETA: 17s  [ loss=0.1024 ][Evaluating] 7/18 [==========>...................] - ETA: 17s[Evaluating] 8/18 [============>.................] - ETA: 15s  [ loss=0.0609 ][Evaluating] 8/18 [============>.................] - ETA: 15s[Evaluating] 9/18 [==============>...............] - ETA: 14s  [ loss=0.0783 ][Evaluating] 9/18 [==============>...............] - ETA: 14s[Evaluating] 10/18 [===============>..............] - ETA: 12s  [ loss=0.0814 ][Evaluating] 10/18 [===============>..............] - ETA: 12s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0840 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 9s  [ loss=0.0776 ][Evaluating] 12/18 [===================>..........] - ETA: 9s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0746 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0942 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 4s  [ loss=0.0964 ][Evaluating] 15/18 [========================>.....] - ETA: 4s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0752 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0630 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0798 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:34:44 - INFO - __main__ -   

03/21/2023 11:34:44 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.7610 - recall: 0.7754 - f1: 0.7681 - loss: 0.0812 
03/21/2023 11:34:44 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:34:44 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.7348 - recall: 0.5673 - f1: 0.6403 
03/21/2023 11:34:44 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.6406 - recall: 0.2733 - f1: 0.3832 
03/21/2023 11:34:44 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.7139 - recall: 0.7807 - f1: 0.7458 
03/21/2023 11:34:44 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:34:44 - INFO - __main__ -    acc: 0.8022 - recall: 0.9502 - f1: 0.8699 
03/21/2023 11:34:46 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-100
03/21/2023 11:34:49 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-100
03/21/2023 11:34:49 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:34:54 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:34:54 - INFO - __main__ -   Saving best eval loss model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:34:59 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 17/84 [=====>........................] - ETA: 5:07  [ loss=0.0554 ][Training] 18/84 [=====>........................] - ETA: 4:53  [ loss=0.0746 ][Training] 19/84 [=====>........................] - ETA: 4:39  [ loss=0.0652 ][Training] 20/84 [======>.......................] - ETA: 4:27  [ loss=0.0817 ][Training] 21/84 [======>.......................] - ETA: 4:16  [ loss=0.0903 ][Training] 22/84 [======>.......................] - ETA: 4:06  [ loss=0.0788 ][Training] 23/84 [=======>......................] - ETA: 3:56  [ loss=0.0796 ][Training] 24/84 [=======>......................] - ETA: 3:47  [ loss=0.0845 ][Training] 25/84 [=======>......................] - ETA: 3:39  [ loss=0.0851 ][Training] 26/84 [========>.....................] - ETA: 3:31  [ loss=0.0556 ][Training] 27/84 [========>.....................] - ETA: 3:24  [ loss=0.0626 ][Training] 28/84 [=========>....................] - ETA: 3:17  [ loss=0.0681 ][Training] 29/84 [=========>....................] - ETA: 3:10  [ loss=0.1084 ][Training] 30/84 [=========>....................] - ETA: 3:04  [ loss=0.0602 ][Training] 31/84 [==========>...................] - ETA: 2:58  [ loss=0.0775 ][Training] 32/84 [==========>...................] - ETA: 2:52  [ loss=0.0651 ][Training] 33/84 [==========>...................] - ETA: 2:47  [ loss=0.0779 ][Training] 34/84 [===========>..................] - ETA: 2:41  [ loss=0.0908 ][Training] 35/84 [===========>..................] - ETA: 2:36  [ loss=0.0507 ][Training] 36/84 [===========>..................] - ETA: 2:31  [ loss=0.0659 ][Training] 37/84 [============>.................] - ETA: 2:26  [ loss=0.0783 ][Training] 38/84 [============>.................] - ETA: 2:22  [ loss=0.0569 ][Training] 39/84 [============>.................] - ETA: 2:17  [ loss=0.0587 ][Training] 40/84 [=============>................] - ETA: 2:13  [ loss=0.0690 ][Training] 41/84 [=============>................] - ETA: 2:09  [ loss=0.0468 ][Training] 42/84 [==============>...............] - ETA: 2:05  [ loss=0.0585 ][Training] 43/84 [==============>...............] - ETA: 2:01  [ loss=0.0512 ][Training] 44/84 [==============>...............] - ETA: 1:57  [ loss=0.0859 ][Training] 45/84 [===============>..............] - ETA: 1:54  [ loss=0.0762 ][Training] 46/84 [===============>..............] - ETA: 1:50  [ loss=0.0736 ][Training] 47/84 [===============>..............] - ETA: 1:46  [ loss=0.0760 ][Training] 48/84 [================>.............] - ETA: 1:43  [ loss=0.0557 ][Training] 49/84 [================>.............] - ETA: 1:39  [ loss=0.0737 ][Training] 50/84 [================>.............] - ETA: 1:36  [ loss=0.0872 ][Training] 51/84 [=================>............] - ETA: 1:32  [ loss=0.0656 ][Training] 52/84 [=================>............] - ETA: 1:29  [ loss=0.0577 ][Training] 53/84 [=================>............] - ETA: 1:25  [ loss=0.0831 ][Training] 54/84 [==================>...........] - ETA: 1:22  [ loss=0.0558 ][Training] 55/84 [==================>...........] - ETA: 1:19  [ loss=0.0666 ][Training] 56/84 [===================>..........] - ETA: 1:16  [ loss=0.0537 ][Training] 57/84 [===================>..........] - ETA: 1:12  [ loss=0.0525 ][Training] 58/84 [===================>..........] - ETA: 1:09  [ loss=0.0758 ][Training] 59/84 [====================>.........] - ETA: 1:06  [ loss=0.0890 ][Training] 60/84 [====================>.........] - ETA: 1:03  [ loss=0.0435 ][Training] 61/84 [====================>.........] - ETA: 1:00  [ loss=0.0702 ][Training] 62/84 [=====================>........] - ETA: 57s  [ loss=0.0562 ][Training] 63/84 [=====================>........] - ETA: 54s  [ loss=0.0603 ][Training] 64/84 [=====================>........] - ETA: 52s  [ loss=0.0686 ][Training] 65/84 [======================>.......] - ETA: 49s  [ loss=0.0572 ][Training] 66/84 [======================>.......] - ETA: 46s  [ loss=0.0655 ][Training] 67/84 [======================>.......] - ETA: 43s  [ loss=0.0649 ][Training] 68/84 [=======================>......] - ETA: 40s  [ loss=0.0576 ][Training] 69/84 [=======================>......] - ETA: 38s  [ loss=0.0545 ][Training] 70/84 [========================>.....] - ETA: 35s  [ loss=0.0595 ][Training] 71/84 [========================>.....] - ETA: 32s  [ loss=0.0659 ][Training] 72/84 [========================>.....] - ETA: 30s  [ loss=0.0631 ][Training] 73/84 [=========================>....] - ETA: 27s  [ loss=0.0525 ][Training] 74/84 [=========================>....] - ETA: 25s  [ loss=0.0519 ][Training] 75/84 [=========================>....] - ETA: 22s  [ loss=0.0868 ][Training] 76/84 [==========================>...] - ETA: 19s  [ loss=0.0579 ][Training] 77/84 [==========================>...] - ETA: 17s  [ loss=0.0837 ][Training] 78/84 [==========================>...] - ETA: 14s  [ loss=0.0507 ][Training] 79/84 [===========================>..] - ETA: 12s  [ loss=0.0561 ][Training] 80/84 [===========================>..] - ETA: 9s  [ loss=0.0562 ][Training] 81/84 [===========================>..] - ETA: 7s  [ loss=0.0704 ][Training] 82/84 [============================>.] - ETA: 4s  [ loss=0.0578 ][Training] 83/84 [============================>.] - ETA: 2s  [ loss=0.0663 ][Training] 84/84 [==============================] 2.4s/step  [ loss=0.0469 ]
03/21/2023 11:37:07 - INFO - __main__ -   


Epoch: 2/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/84 [..............................] - ETA: 4:17  [ loss=0.0432 ][Training] 2/84 [..............................] - ETA: 3:25  [ loss=0.0381 ][Training] 3/84 [>.............................] - ETA: 3:04  [ loss=0.0431 ][Training] 4/84 [>.............................] - ETA: 2:53  [ loss=0.0531 ][Training] 5/84 [>.............................] - ETA: 2:46  [ loss=0.0484 ][Training] 6/84 [=>............................] - ETA: 2:41  [ loss=0.0619 ][Training] 7/84 [=>............................] - ETA: 2:37  [ loss=0.0392 ][Training] 8/84 [=>............................] - ETA: 2:33  [ loss=0.0400 ][Training] 9/84 [==>...........................] - ETA: 2:30  [ loss=0.0361 ][Training] 10/84 [==>...........................] - ETA: 2:27  [ loss=0.0520 ][Training] 11/84 [==>...........................] - ETA: 2:24  [ loss=0.0463 ][Training] 12/84 [===>..........................] - ETA: 2:21  [ loss=0.0376 ][Training] 13/84 [===>..........................] - ETA: 2:19  [ loss=0.0339 ][Training] 14/84 [====>.........................] - ETA: 2:16  [ loss=0.0401 ][Training] 15/84 [====>.........................] - ETA: 2:14  [ loss=0.0372 ][Training] 16/84 [====>.........................] - ETA: 2:12  [ loss=0.0653 ][Training] 17/84 [=====>........................] - ETA: 2:09  [ loss=0.0480 ][Training] 18/84 [=====>........................] - ETA: 2:07  [ loss=0.0453 ][Training] 19/84 [=====>........................] - ETA: 2:05  [ loss=0.0450 ][Training] 20/84 [======>.......................] - ETA: 2:03  [ loss=0.0512 ][Training] 21/84 [======>.......................] - ETA: 2:00  [ loss=0.0417 ][Training] 22/84 [======>.......................] - ETA: 1:58  [ loss=0.0459 ][Training] 23/84 [=======>......................] - ETA: 1:56  [ loss=0.0437 ][Training] 24/84 [=======>......................] - ETA: 1:54  [ loss=0.0439 ][Training] 25/84 [=======>......................] - ETA: 1:52  [ loss=0.0522 ][Training] 26/84 [========>.....................] - ETA: 1:50  [ loss=0.0429 ][Training] 27/84 [========>.....................] - ETA: 1:48  [ loss=0.0421 ][Training] 28/84 [=========>....................] - ETA: 1:46  [ loss=0.0342 ][Training] 29/84 [=========>....................] - ETA: 1:44  [ loss=0.0527 ][Training] 30/84 [=========>....................] - ETA: 1:43  [ loss=0.0478 ][Training] 31/84 [==========>...................] - ETA: 1:41  [ loss=0.0537 ][Training] 32/84 [==========>...................] - ETA: 1:39  [ loss=0.0432 ] 
03/21/2023 11:38:08 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 11:38:08 - INFO - __main__ -     Num examples = 723
03/21/2023 11:38:08 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 30s  [ loss=0.0468 ][Evaluating] 1/18 [>.............................] - ETA: 30s[Evaluating] 2/18 [==>...........................] - ETA: 27s  [ loss=0.0645 ][Evaluating] 2/18 [==>...........................] - ETA: 27s[Evaluating] 3/18 [====>.........................] - ETA: 25s  [ loss=0.0509 ][Evaluating] 3/18 [====>.........................] - ETA: 25s[Evaluating] 4/18 [=====>........................] - ETA: 24s  [ loss=0.0438 ][Evaluating] 4/18 [=====>........................] - ETA: 24s[Evaluating] 5/18 [=======>......................] - ETA: 22s  [ loss=0.0805 ][Evaluating] 5/18 [=======>......................] - ETA: 22s[Evaluating] 6/18 [=========>....................] - ETA: 20s  [ loss=0.0880 ][Evaluating] 6/18 [=========>....................] - ETA: 20s[Evaluating] 7/18 [==========>...................] - ETA: 18s  [ loss=0.0890 ][Evaluating] 7/18 [==========>...................] - ETA: 18s[Evaluating] 8/18 [============>.................] - ETA: 17s  [ loss=0.0564 ][Evaluating] 8/18 [============>.................] - ETA: 17s[Evaluating] 9/18 [==============>...............] - ETA: 15s  [ loss=0.0720 ][Evaluating] 9/18 [==============>...............] - ETA: 15s[Evaluating] 10/18 [===============>..............] - ETA: 13s  [ loss=0.0593 ][Evaluating] 10/18 [===============>..............] - ETA: 13s[Evaluating] 11/18 [=================>............] - ETA: 11s  [ loss=0.0613 ][Evaluating] 11/18 [=================>............] - ETA: 11s[Evaluating] 12/18 [===================>..........] - ETA: 10s  [ loss=0.0589 ][Evaluating] 12/18 [===================>..........] - ETA: 10s[Evaluating] 13/18 [====================>.........] - ETA: 8s  [ loss=0.0559 ][Evaluating] 13/18 [====================>.........] - ETA: 8s[Evaluating] 14/18 [======================>.......] - ETA: 6s  [ loss=0.0637 ][Evaluating] 14/18 [======================>.......] - ETA: 6s[Evaluating] 15/18 [========================>.....] - ETA: 5s  [ loss=0.0684 ][Evaluating] 15/18 [========================>.....] - ETA: 5s[Evaluating] 16/18 [=========================>....] - ETA: 3s  [ loss=0.0490 ][Evaluating] 16/18 [=========================>....] - ETA: 3s[Evaluating] 17/18 [===========================>..] - ETA: 1s  [ loss=0.0459 ][Evaluating] 17/18 [===========================>..] - ETA: 1s[Evaluating] 18/18 [==============================] 1.7s/step  [ loss=0.0731 ]
[Evaluating] 18/18 [==============================] 1.7s/step
03/21/2023 11:38:40 - INFO - __main__ -   

03/21/2023 11:38:40 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.8352 - recall: 0.8422 - f1: 0.8387 - loss: 0.0626 
03/21/2023 11:38:40 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 11:38:40 - INFO - __main__ -   ******* LOC results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.8092 - recall: 0.7193 - f1: 0.7616 
03/21/2023 11:38:40 - INFO - __main__ -   ******* MISC results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.5824 - recall: 0.7067 - f1: 0.6386 
03/21/2023 11:38:40 - INFO - __main__ -   ******* ORG results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.8041 - recall: 0.8342 - f1: 0.8189 
03/21/2023 11:38:40 - INFO - __main__ -   ******* PER results ********
03/21/2023 11:38:40 - INFO - __main__ -    acc: 0.9374 - recall: 0.9133 - f1: 0.9252 
03/21/2023 11:38:41 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-200
03/21/2023 11:38:44 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-200
03/21/2023 11:38:44 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:39:03 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:39:03 - INFO - __main__ -   Saving best eval loss model checkpoint to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 11:39:36 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter17_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
[Training] 33/84 [==========>...................] - ETA: 3:52  [ loss=0.0448 ][Training] 34/84 [===========>..................] - ETA: 3:44  [ loss=0.0343 ]