nohup: ignoring input
03/21/2023 10:35:53 - INFO - __main__ -   label_list: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[START]', '[END]'], length: 12
The number of samples: 4000
The number of images: 4000
The number of samples: 1000
The number of images: 1000
The number of samples: 3257
The number of images: 3257
03/21/2023 10:35:53 - INFO - root -   Constructing vocabulary for image caption
03/21/2023 10:35:54 - INFO - __main__ -    The size of vocabulary = 4736
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/21/2023 10:36:14 - INFO - __main__ -   Args: Namespace(adam_epsilon=1e-08, alpha=0.0001, bert_type='uncased', beta=0.0001, cls_init=0, crf_dropout=0.5, crf_learning_rate=0.0001, crop_size=224, cross_dropout=0.2, data_dir='/home/ubuntu/multimodal-fusion/baselines/UMT/data/twitter2015', device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, drop_last=True, eval_all_checkpoints=False, evaluate_during_training=True, gradient_accumulation_steps=1, hidden_size=768, id2label={0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC', 9: 'X', 10: '[START]', 11: '[END]'}, image_dir='/home/ubuntu/multimodal-fusion/datasets/IJCAI2019_data/twitter2015_images', image_dropout=0.0, label2id={'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'X': 9, '[START]': 10, '[END]': 11}, learning_rate=0.0001, load_image_checkpoint=False, load_text_checkpoint=False, local_rank=-1, logging_steps=100, markup='bio', max_grad_norm=1.0, max_seq_length=64, n_gpu=1, num_layers=6, num_train_epochs=10, num_workers=8, output_dir='../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/', per_gpu_eval_batch_size=40, per_gpu_train_batch_size=40, predict_checkpoints=0, replace_end=3, replace_start=1, resnet_pretrained_dir='../models/resnet152-b121ed2d.pth', save_steps=100, seed=42, sigma=1.0, skip_connection=True, task='twitter15', test_batch_size=1, text_dropout=0.0, theta=0.1, ti_crop_size=32, train_batch_size=40, use_quantile=True, use_xlmr=False, warmup_proportion=0.1, weight_decay=0.01)
03/21/2023 10:36:14 - INFO - __main__ -   Summary dir: ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/summary_1679366174
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
03/21/2023 10:36:14 - INFO - __main__ -   ***** Running training *****
03/21/2023 10:36:14 - INFO - __main__ -     Num examples = 4000
03/21/2023 10:36:14 - INFO - __main__ -     Num Epochs = 10
03/21/2023 10:36:14 - INFO - __main__ -     Gradient Accumulation steps = 1
03/21/2023 10:36:14 - INFO - __main__ -     Total optimization steps = 1000

Epoch: 0/10
/home/ubuntu/multimodal-fusion/MuSE/code/models.py:1235: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
[Training] 1/100 [..............................] - ETA: 6:43  [ loss=1.8292 ][Training] 2/100 [..............................] - ETA: 5:33  [ loss=1.7752 ][Training] 3/100 [..............................] - ETA: 4:43  [ loss=1.7758 ][Training] 4/100 [>.............................] - ETA: 4:18  [ loss=1.7922 ][Training] 5/100 [>.............................] - ETA: 4:00  [ loss=1.6113 ][Training] 6/100 [>.............................] - ETA: 3:47  [ loss=1.5062 ][Training] 7/100 [=>............................] - ETA: 3:37  [ loss=1.3924 ][Training] 8/100 [=>............................] - ETA: 3:30  [ loss=1.1156 ][Training] 9/100 [=>............................] - ETA: 3:23  [ loss=1.0457 ][Training] 10/100 [==>...........................] - ETA: 3:18  [ loss=0.9607 ][Training] 11/100 [==>...........................] - ETA: 3:14  [ loss=0.8800 ][Training] 12/100 [==>...........................] - ETA: 3:09  [ loss=0.8139 ][Training] 13/100 [==>...........................] - ETA: 3:05  [ loss=0.7513 ][Training] 14/100 [===>..........................] - ETA: 3:01  [ loss=0.6784 ][Training] 15/100 [===>..........................] - ETA: 2:58  [ loss=0.6822 ][Training] 16/100 [===>..........................] - ETA: 2:54  [ loss=0.6721 ][Training] 17/100 [====>.........................] - ETA: 2:51  [ loss=0.6899 ][Training] 18/100 [====>.........................] - ETA: 2:48  [ loss=0.6810 ][Training] 19/100 [====>.........................] - ETA: 2:45  [ loss=0.6694 ][Training] 20/100 [=====>........................] - ETA: 2:43  [ loss=0.6278 ][Training] 21/100 [=====>........................] - ETA: 2:42  [ loss=0.6117 ][Training] 22/100 [=====>........................] - ETA: 2:39  [ loss=0.6076 ][Training] 23/100 [=====>........................] - ETA: 2:37  [ loss=0.5643 ][Training] 24/100 [======>.......................] - ETA: 2:34  [ loss=0.4950 ][Training] 25/100 [======>.......................] - ETA: 2:31  [ loss=0.4330 ][Training] 26/100 [======>.......................] - ETA: 2:28  [ loss=0.3678 ][Training] 27/100 [=======>......................] - ETA: 2:26  [ loss=0.4095 ][Training] 28/100 [=======>......................] - ETA: 2:23  [ loss=0.3448 ][Training] 29/100 [=======>......................] - ETA: 2:21  [ loss=0.3540 ][Training] 30/100 [========>.....................] - ETA: 2:19  [ loss=0.3521 ][Training] 31/100 [========>.....................] - ETA: 2:16  [ loss=0.2790 ][Training] 32/100 [========>.....................] - ETA: 2:14  [ loss=0.3210 ][Training] 33/100 [========>.....................] - ETA: 2:12  [ loss=0.2486 ][Training] 34/100 [=========>....................] - ETA: 2:10  [ loss=0.2824 ][Training] 35/100 [=========>....................] - ETA: 2:07  [ loss=0.2606 ][Training] 36/100 [=========>....................] - ETA: 2:05  [ loss=0.2527 ][Training] 37/100 [==========>...................] - ETA: 2:03  [ loss=0.2700 ][Training] 38/100 [==========>...................] - ETA: 2:01  [ loss=0.2280 ][Training] 39/100 [==========>...................] - ETA: 1:59  [ loss=0.2591 ][Training] 40/100 [===========>..................] - ETA: 1:57  [ loss=0.2336 ][Training] 41/100 [===========>..................] - ETA: 1:54  [ loss=0.2651 ][Training] 42/100 [===========>..................] - ETA: 1:52  [ loss=0.2155 ][Training] 43/100 [===========>..................] - ETA: 1:50  [ loss=0.2267 ][Training] 44/100 [============>.................] - ETA: 1:48  [ loss=0.2080 ][Training] 45/100 [============>.................] - ETA: 1:46  [ loss=0.1724 ][Training] 46/100 [============>.................] - ETA: 1:44  [ loss=0.1660 ][Training] 47/100 [=============>................] - ETA: 1:42  [ loss=0.1948 ][Training] 48/100 [=============>................] - ETA: 1:40  [ loss=0.2020 ][Training] 49/100 [=============>................] - ETA: 1:38  [ loss=0.1871 ][Training] 50/100 [==============>...............] - ETA: 1:36  [ loss=0.1833 ][Training] 51/100 [==============>...............] - ETA: 1:34  [ loss=0.1763 ][Training] 52/100 [==============>...............] - ETA: 1:32  [ loss=0.1782 ][Training] 53/100 [==============>...............] - ETA: 1:30  [ loss=0.1644 ][Training] 54/100 [===============>..............] - ETA: 1:28  [ loss=0.1762 ][Training] 55/100 [===============>..............] - ETA: 1:26  [ loss=0.1629 ][Training] 56/100 [===============>..............] - ETA: 1:24  [ loss=0.1420 ][Training] 57/100 [================>.............] - ETA: 1:22  [ loss=0.1517 ][Training] 58/100 [================>.............] - ETA: 1:20  [ loss=0.1560 ][Training] 59/100 [================>.............] - ETA: 1:18  [ loss=0.1553 ][Training] 60/100 [=================>............] - ETA: 1:16  [ loss=0.1518 ][Training] 61/100 [=================>............] - ETA: 1:14  [ loss=0.1260 ][Training] 62/100 [=================>............] - ETA: 1:12  [ loss=0.1566 ][Training] 63/100 [=================>............] - ETA: 1:10  [ loss=0.1415 ][Training] 64/100 [==================>...........] - ETA: 1:08  [ loss=0.1320 ][Training] 65/100 [==================>...........] - ETA: 1:06  [ loss=0.1472 ][Training] 66/100 [==================>...........] - ETA: 1:04  [ loss=0.1145 ][Training] 67/100 [===================>..........] - ETA: 1:02  [ loss=0.1139 ][Training] 68/100 [===================>..........] - ETA: 1:00  [ loss=0.1454 ][Training] 69/100 [===================>..........] - ETA: 59s  [ loss=0.1352 ][Training] 70/100 [====================>.........] - ETA: 57s  [ loss=0.1288 ][Training] 71/100 [====================>.........] - ETA: 55s  [ loss=0.1445 ][Training] 72/100 [====================>.........] - ETA: 53s  [ loss=0.1282 ][Training] 73/100 [====================>.........] - ETA: 51s  [ loss=0.1528 ][Training] 74/100 [=====================>........] - ETA: 49s  [ loss=0.1004 ][Training] 75/100 [=====================>........] - ETA: 47s  [ loss=0.1219 ][Training] 76/100 [=====================>........] - ETA: 45s  [ loss=0.1006 ][Training] 77/100 [======================>.......] - ETA: 43s  [ loss=0.1307 ][Training] 78/100 [======================>.......] - ETA: 41s  [ loss=0.1099 ][Training] 79/100 [======================>.......] - ETA: 39s  [ loss=0.1763 ][Training] 80/100 [=======================>......] - ETA: 37s  [ loss=0.1266 ][Training] 81/100 [=======================>......] - ETA: 36s  [ loss=0.1517 ][Training] 82/100 [=======================>......] - ETA: 34s  [ loss=0.1111 ][Training] 83/100 [=======================>......] - ETA: 32s  [ loss=0.1426 ][Training] 84/100 [========================>.....] - ETA: 30s  [ loss=0.1096 ][Training] 85/100 [========================>.....] - ETA: 28s  [ loss=0.1040 ][Training] 86/100 [========================>.....] - ETA: 26s  [ loss=0.0953 ][Training] 87/100 [=========================>....] - ETA: 24s  [ loss=0.1120 ][Training] 88/100 [=========================>....] - ETA: 22s  [ loss=0.1590 ][Training] 89/100 [=========================>....] - ETA: 20s  [ loss=0.1315 ][Training] 90/100 [==========================>...] - ETA: 18s  [ loss=0.1214 ][Training] 91/100 [==========================>...] - ETA: 17s  [ loss=0.1181 ][Training] 92/100 [==========================>...] - ETA: 15s  [ loss=0.1067 ][Training] 93/100 [==========================>...] - ETA: 13s  [ loss=0.1228 ][Training] 94/100 [===========================>..] - ETA: 11s  [ loss=0.0967 ][Training] 95/100 [===========================>..] - ETA: 9s  [ loss=0.1170 ][Training] 96/100 [===========================>..] - ETA: 7s  [ loss=0.1045 ][Training] 97/100 [============================>.] - ETA: 5s  [ loss=0.1058 ][Training] 98/100 [============================>.] - ETA: 3s  [ loss=0.1013 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=0.1040 ][Training] 100/100 [==============================] 1.9s/step  [ loss=0.1420 ]
 
03/21/2023 10:39:23 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 10:39:23 - INFO - __main__ -     Num examples = 1000
03/21/2023 10:39:23 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 33s  [ loss=0.1154 ][Evaluating] 1/25 [>.............................] - ETA: 33s[Evaluating] 2/25 [=>............................] - ETA: 31s  [ loss=0.1112 ][Evaluating] 2/25 [=>............................] - ETA: 31s[Evaluating] 3/25 [==>...........................] - ETA: 29s  [ loss=0.1056 ][Evaluating] 3/25 [==>...........................] - ETA: 29s[Evaluating] 4/25 [===>..........................] - ETA: 27s  [ loss=0.0931 ][Evaluating] 4/25 [===>..........................] - ETA: 27s[Evaluating] 5/25 [=====>........................] - ETA: 26s  [ loss=0.1449 ][Evaluating] 5/25 [=====>........................] - ETA: 26s[Evaluating] 6/25 [======>.......................] - ETA: 25s  [ loss=0.1063 ][Evaluating] 6/25 [======>.......................] - ETA: 25s[Evaluating] 7/25 [=======>......................] - ETA: 23s  [ loss=0.1447 ][Evaluating] 7/25 [=======>......................] - ETA: 23s[Evaluating] 8/25 [========>.....................] - ETA: 22s  [ loss=0.1016 ][Evaluating] 8/25 [========>.....................] - ETA: 22s[Evaluating] 9/25 [=========>....................] - ETA: 21s  [ loss=0.0954 ][Evaluating] 9/25 [=========>....................] - ETA: 21s[Evaluating] 10/25 [===========>..................] - ETA: 19s  [ loss=0.1019 ][Evaluating] 10/25 [===========>..................] - ETA: 19s[Evaluating] 11/25 [============>.................] - ETA: 18s  [ loss=0.1068 ][Evaluating] 11/25 [============>.................] - ETA: 18s[Evaluating] 12/25 [=============>................] - ETA: 17s  [ loss=0.0884 ][Evaluating] 12/25 [=============>................] - ETA: 17s[Evaluating] 13/25 [==============>...............] - ETA: 15s  [ loss=0.0972 ][Evaluating] 13/25 [==============>...............] - ETA: 15s[Evaluating] 14/25 [===============>..............] - ETA: 14s  [ loss=0.0933 ][Evaluating] 14/25 [===============>..............] - ETA: 14s[Evaluating] 15/25 [=================>............] - ETA: 13s  [ loss=0.1209 ][Evaluating] 15/25 [=================>............] - ETA: 13s[Evaluating] 16/25 [==================>...........] - ETA: 11s  [ loss=0.1166 ][Evaluating] 16/25 [==================>...........] - ETA: 11s[Evaluating] 17/25 [===================>..........] - ETA: 10s  [ loss=0.0976 ][Evaluating] 17/25 [===================>..........] - ETA: 10s[Evaluating] 18/25 [====================>.........] - ETA: 9s  [ loss=0.0962 ][Evaluating] 18/25 [====================>.........] - ETA: 9s[Evaluating] 19/25 [=====================>........] - ETA: 7s  [ loss=0.1317 ][Evaluating] 19/25 [=====================>........] - ETA: 7s[Evaluating] 20/25 [=======================>......] - ETA: 6s  [ loss=0.0997 ][Evaluating] 20/25 [=======================>......] - ETA: 6s[Evaluating] 21/25 [========================>.....] - ETA: 5s  [ loss=0.1206 ][Evaluating] 21/25 [========================>.....] - ETA: 5s[Evaluating] 22/25 [=========================>....] - ETA: 3s  [ loss=0.0819 ][Evaluating] 22/25 [=========================>....] - ETA: 3s[Evaluating] 23/25 [==========================>...] - ETA: 2s  [ loss=0.0922 ][Evaluating] 23/25 [==========================>...] - ETA: 2s[Evaluating] 24/25 [===========================>..] - ETA: 1s  [ loss=0.0986 ][Evaluating] 24/25 [===========================>..] - ETA: 1s[Evaluating] 25/25 [==============================] 1.3s/step  [ loss=0.1137 ]
[Evaluating] 25/25 [==============================] 1.3s/step
03/21/2023 10:39:56 - INFO - __main__ -   

03/21/2023 10:39:56 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 10:39:56 - INFO - __main__ -    acc: 0.5870 - recall: 0.6567 - f1: 0.6199 - loss: 0.1070 
03/21/2023 10:39:56 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 10:39:56 - INFO - __main__ -   ******* LOC results ********
03/21/2023 10:39:56 - INFO - __main__ -    acc: 0.5513 - recall: 0.8640 - f1: 0.6731 
03/21/2023 10:39:56 - INFO - __main__ -   ******* MISC results ********
03/21/2023 10:39:56 - INFO - __main__ -    acc: 0.0000 - recall: 0.0000 - f1: 0.0000 
03/21/2023 10:39:56 - INFO - __main__ -   ******* ORG results ********
03/21/2023 10:39:56 - INFO - __main__ -    acc: 0.3663 - recall: 0.2996 - f1: 0.3296 
03/21/2023 10:39:56 - INFO - __main__ -   ******* PER results ********
03/21/2023 10:39:56 - INFO - __main__ -    acc: 0.6918 - recall: 0.8822 - f1: 0.7755 
03/21/2023 10:39:58 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-100
03/21/2023 10:40:01 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-100
03/21/2023 10:40:01 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:40:05 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:40:05 - INFO - __main__ -   Saving best eval loss model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:40:10 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:40:10 - INFO - __main__ -   


Epoch: 1/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/100 [..............................] - ETA: 6:54  [ loss=0.0991 ][Training] 2/100 [..............................] - ETA: 4:59  [ loss=0.1204 ][Training] 3/100 [..............................] - ETA: 4:19  [ loss=0.0979 ][Training] 4/100 [>.............................] - ETA: 3:56  [ loss=0.1110 ][Training] 5/100 [>.............................] - ETA: 3:42  [ loss=0.0926 ][Training] 6/100 [>.............................] - ETA: 3:32  [ loss=0.0972 ][Training] 7/100 [=>............................] - ETA: 3:23  [ loss=0.1063 ][Training] 8/100 [=>............................] - ETA: 3:17  [ loss=0.0909 ][Training] 9/100 [=>............................] - ETA: 3:12  [ loss=0.0946 ][Training] 10/100 [==>...........................] - ETA: 3:07  [ loss=0.0875 ][Training] 11/100 [==>...........................] - ETA: 3:04  [ loss=0.1011 ][Training] 12/100 [==>...........................] - ETA: 3:00  [ loss=0.0706 ][Training] 13/100 [==>...........................] - ETA: 2:57  [ loss=0.0917 ][Training] 14/100 [===>..........................] - ETA: 2:53  [ loss=0.0987 ][Training] 15/100 [===>..........................] - ETA: 2:50  [ loss=0.1142 ][Training] 16/100 [===>..........................] - ETA: 2:47  [ loss=0.0906 ][Training] 17/100 [====>.........................] - ETA: 2:44  [ loss=0.1061 ][Training] 18/100 [====>.........................] - ETA: 2:41  [ loss=0.1080 ][Training] 19/100 [====>.........................] - ETA: 2:38  [ loss=0.0944 ][Training] 20/100 [=====>........................] - ETA: 2:35  [ loss=0.1174 ][Training] 21/100 [=====>........................] - ETA: 2:33  [ loss=0.0958 ][Training] 22/100 [=====>........................] - ETA: 2:30  [ loss=0.0718 ][Training] 23/100 [=====>........................] - ETA: 2:29  [ loss=0.1008 ][Training] 24/100 [======>.......................] - ETA: 2:27  [ loss=0.0993 ][Training] 25/100 [======>.......................] - ETA: 2:24  [ loss=0.0740 ][Training] 26/100 [======>.......................] - ETA: 2:22  [ loss=0.0777 ][Training] 27/100 [=======>......................] - ETA: 2:20  [ loss=0.0840 ][Training] 28/100 [=======>......................] - ETA: 2:18  [ loss=0.1020 ][Training] 29/100 [=======>......................] - ETA: 2:16  [ loss=0.1160 ][Training] 30/100 [========>.....................] - ETA: 2:13  [ loss=0.0820 ][Training] 31/100 [========>.....................] - ETA: 2:11  [ loss=0.1257 ][Training] 32/100 [========>.....................] - ETA: 2:09  [ loss=0.1142 ][Training] 33/100 [========>.....................] - ETA: 2:07  [ loss=0.0898 ][Training] 34/100 [=========>....................] - ETA: 2:05  [ loss=0.0906 ][Training] 35/100 [=========>....................] - ETA: 2:03  [ loss=0.0793 ][Training] 36/100 [=========>....................] - ETA: 2:01  [ loss=0.0768 ][Training] 37/100 [==========>...................] - ETA: 1:59  [ loss=0.1033 ][Training] 38/100 [==========>...................] - ETA: 1:57  [ loss=0.0890 ][Training] 39/100 [==========>...................] - ETA: 1:55  [ loss=0.0919 ][Training] 40/100 [===========>..................] - ETA: 1:53  [ loss=0.0879 ][Training] 41/100 [===========>..................] - ETA: 1:51  [ loss=0.0777 ][Training] 42/100 [===========>..................] - ETA: 1:49  [ loss=0.0858 ][Training] 43/100 [===========>..................] - ETA: 1:47  [ loss=0.0772 ][Training] 44/100 [============>.................] - ETA: 1:45  [ loss=0.0873 ][Training] 45/100 [============>.................] - ETA: 1:43  [ loss=0.0928 ][Training] 46/100 [============>.................] - ETA: 1:41  [ loss=0.0798 ][Training] 47/100 [=============>................] - ETA: 1:39  [ loss=0.1110 ][Training] 48/100 [=============>................] - ETA: 1:37  [ loss=0.0806 ][Training] 49/100 [=============>................] - ETA: 1:35  [ loss=0.1092 ][Training] 50/100 [==============>...............] - ETA: 1:34  [ loss=0.0845 ][Training] 51/100 [==============>...............] - ETA: 1:32  [ loss=0.0966 ][Training] 52/100 [==============>...............] - ETA: 1:30  [ loss=0.0690 ][Training] 53/100 [==============>...............] - ETA: 1:28  [ loss=0.0720 ][Training] 54/100 [===============>..............] - ETA: 1:26  [ loss=0.0714 ][Training] 55/100 [===============>..............] - ETA: 1:24  [ loss=0.1058 ][Training] 56/100 [===============>..............] - ETA: 1:22  [ loss=0.0984 ][Training] 57/100 [================>.............] - ETA: 1:20  [ loss=0.0754 ][Training] 58/100 [================>.............] - ETA: 1:18  [ loss=0.0998 ][Training] 59/100 [================>.............] - ETA: 1:16  [ loss=0.1044 ][Training] 60/100 [=================>............] - ETA: 1:14  [ loss=0.0876 ][Training] 61/100 [=================>............] - ETA: 1:12  [ loss=0.0779 ][Training] 62/100 [=================>............] - ETA: 1:10  [ loss=0.0674 ][Training] 63/100 [=================>............] - ETA: 1:08  [ loss=0.0827 ][Training] 64/100 [==================>...........] - ETA: 1:07  [ loss=0.1071 ][Training] 65/100 [==================>...........] - ETA: 1:05  [ loss=0.0786 ][Training] 66/100 [==================>...........] - ETA: 1:03  [ loss=0.0955 ][Training] 67/100 [===================>..........] - ETA: 1:01  [ loss=0.0921 ][Training] 68/100 [===================>..........] - ETA: 59s  [ loss=0.0700 ][Training] 69/100 [===================>..........] - ETA: 57s  [ loss=0.0835 ][Training] 70/100 [====================>.........] - ETA: 55s  [ loss=0.0854 ][Training] 71/100 [====================>.........] - ETA: 53s  [ loss=0.0768 ][Training] 72/100 [====================>.........] - ETA: 51s  [ loss=0.0910 ][Training] 73/100 [====================>.........] - ETA: 50s  [ loss=0.0956 ][Training] 74/100 [=====================>........] - ETA: 48s  [ loss=0.0711 ][Training] 75/100 [=====================>........] - ETA: 46s  [ loss=0.0765 ][Training] 76/100 [=====================>........] - ETA: 44s  [ loss=0.0744 ][Training] 77/100 [======================>.......] - ETA: 42s  [ loss=0.0924 ][Training] 78/100 [======================>.......] - ETA: 40s  [ loss=0.0944 ][Training] 79/100 [======================>.......] - ETA: 38s  [ loss=0.0734 ][Training] 80/100 [=======================>......] - ETA: 37s  [ loss=0.0870 ][Training] 81/100 [=======================>......] - ETA: 35s  [ loss=0.0722 ][Training] 82/100 [=======================>......] - ETA: 33s  [ loss=0.0804 ][Training] 83/100 [=======================>......] - ETA: 31s  [ loss=0.0724 ][Training] 84/100 [========================>.....] - ETA: 29s  [ loss=0.0644 ][Training] 85/100 [========================>.....] - ETA: 27s  [ loss=0.0748 ][Training] 86/100 [========================>.....] - ETA: 25s  [ loss=0.0706 ][Training] 87/100 [=========================>....] - ETA: 24s  [ loss=0.0726 ][Training] 88/100 [=========================>....] - ETA: 22s  [ loss=0.0862 ][Training] 89/100 [=========================>....] - ETA: 20s  [ loss=0.0859 ][Training] 90/100 [==========================>...] - ETA: 18s  [ loss=0.0904 ][Training] 91/100 [==========================>...] - ETA: 16s  [ loss=0.0809 ][Training] 92/100 [==========================>...] - ETA: 14s  [ loss=0.0750 ][Training] 93/100 [==========================>...] - ETA: 12s  [ loss=0.0858 ][Training] 94/100 [===========================>..] - ETA: 11s  [ loss=0.1003 ][Training] 95/100 [===========================>..] - ETA: 9s  [ loss=0.0933 ][Training] 96/100 [===========================>..] - ETA: 7s  [ loss=0.0985 ][Training] 97/100 [============================>.] - ETA: 5s  [ loss=0.0853 ][Training] 98/100 [============================>.] - ETA: 3s  [ loss=0.0747 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=0.0679 ][Training] 100/100 [==============================] 1.8s/step  [ loss=0.0882 ]
 
03/21/2023 10:43:15 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 10:43:15 - INFO - __main__ -     Num examples = 1000
03/21/2023 10:43:15 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 34s  [ loss=0.0914 ][Evaluating] 1/25 [>.............................] - ETA: 34s[Evaluating] 2/25 [=>............................] - ETA: 30s  [ loss=0.0741 ][Evaluating] 2/25 [=>............................] - ETA: 31s[Evaluating] 3/25 [==>...........................] - ETA: 30s  [ loss=0.0819 ][Evaluating] 3/25 [==>...........................] - ETA: 30s[Evaluating] 4/25 [===>..........................] - ETA: 28s  [ loss=0.0782 ][Evaluating] 4/25 [===>..........................] - ETA: 28s[Evaluating] 5/25 [=====>........................] - ETA: 27s  [ loss=0.1008 ][Evaluating] 5/25 [=====>........................] - ETA: 27s[Evaluating] 6/25 [======>.......................] - ETA: 25s  [ loss=0.0781 ][Evaluating] 6/25 [======>.......................] - ETA: 25s[Evaluating] 7/25 [=======>......................] - ETA: 24s  [ loss=0.1142 ][Evaluating] 7/25 [=======>......................] - ETA: 24s[Evaluating] 8/25 [========>.....................] - ETA: 22s  [ loss=0.0763 ][Evaluating] 8/25 [========>.....................] - ETA: 22s[Evaluating] 9/25 [=========>....................] - ETA: 21s  [ loss=0.0721 ][Evaluating] 9/25 [=========>....................] - ETA: 21s[Evaluating] 10/25 [===========>..................] - ETA: 20s  [ loss=0.0777 ][Evaluating] 10/25 [===========>..................] - ETA: 20s[Evaluating] 11/25 [============>.................] - ETA: 18s  [ loss=0.0800 ][Evaluating] 11/25 [============>.................] - ETA: 18s[Evaluating] 12/25 [=============>................] - ETA: 17s  [ loss=0.0643 ][Evaluating] 12/25 [=============>................] - ETA: 17s[Evaluating] 13/25 [==============>...............] - ETA: 15s  [ loss=0.0909 ][Evaluating] 13/25 [==============>...............] - ETA: 15s[Evaluating] 14/25 [===============>..............] - ETA: 14s  [ loss=0.0718 ][Evaluating] 14/25 [===============>..............] - ETA: 14s[Evaluating] 15/25 [=================>............] - ETA: 13s  [ loss=0.1108 ][Evaluating] 15/25 [=================>............] - ETA: 13s[Evaluating] 16/25 [==================>...........] - ETA: 11s  [ loss=0.0866 ][Evaluating] 16/25 [==================>...........] - ETA: 11s[Evaluating] 17/25 [===================>..........] - ETA: 10s  [ loss=0.0847 ][Evaluating] 17/25 [===================>..........] - ETA: 10s[Evaluating] 18/25 [====================>.........] - ETA: 9s  [ loss=0.0796 ][Evaluating] 18/25 [====================>.........] - ETA: 9s[Evaluating] 19/25 [=====================>........] - ETA: 7s  [ loss=0.0823 ][Evaluating] 19/25 [=====================>........] - ETA: 7s[Evaluating] 20/25 [=======================>......] - ETA: 6s  [ loss=0.0713 ][Evaluating] 20/25 [=======================>......] - ETA: 6s[Evaluating] 21/25 [========================>.....] - ETA: 5s  [ loss=0.0944 ][Evaluating] 21/25 [========================>.....] - ETA: 5s[Evaluating] 22/25 [=========================>....] - ETA: 3s  [ loss=0.0698 ][Evaluating] 22/25 [=========================>....] - ETA: 3s[Evaluating] 23/25 [==========================>...] - ETA: 2s  [ loss=0.0819 ][Evaluating] 23/25 [==========================>...] - ETA: 2s[Evaluating] 24/25 [===========================>..] - ETA: 1s  [ loss=0.0867 ][Evaluating] 24/25 [===========================>..] - ETA: 1s[Evaluating] 25/25 [==============================] 1.3s/step  [ loss=0.0899 ]
[Evaluating] 25/25 [==============================] 1.3s/step
03/21/2023 10:43:48 - INFO - __main__ -   

03/21/2023 10:43:48 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 10:43:48 - INFO - __main__ -    acc: 0.7358 - recall: 0.7158 - f1: 0.7257 - loss: 0.0836 
03/21/2023 10:43:48 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 10:43:48 - INFO - __main__ -   ******* LOC results ********
03/21/2023 10:43:48 - INFO - __main__ -    acc: 0.7383 - recall: 0.8487 - f1: 0.7897 
03/21/2023 10:43:48 - INFO - __main__ -   ******* MISC results ********
03/21/2023 10:43:48 - INFO - __main__ -    acc: 0.4715 - recall: 0.2636 - f1: 0.3382 
03/21/2023 10:43:48 - INFO - __main__ -   ******* ORG results ********
03/21/2023 10:43:48 - INFO - __main__ -    acc: 0.6462 - recall: 0.5101 - f1: 0.5701 
03/21/2023 10:43:48 - INFO - __main__ -   ******* PER results ********
03/21/2023 10:43:48 - INFO - __main__ -    acc: 0.8193 - recall: 0.8623 - f1: 0.8402 
03/21/2023 10:43:49 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-200
03/21/2023 10:43:53 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-200
03/21/2023 10:43:53 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:44:12 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:44:12 - INFO - __main__ -   Saving best eval loss model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:44:40 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:44:40 - INFO - __main__ -   


Epoch: 2/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/100 [..............................] - ETA: 6:39  [ loss=0.0829 ][Training] 2/100 [..............................] - ETA: 4:53  [ loss=0.0772 ][Training] 3/100 [..............................] - ETA: 4:16  [ loss=0.0655 ][Training] 4/100 [>.............................] - ETA: 3:57  [ loss=0.0610 ][Training] 5/100 [>.............................] - ETA: 3:43  [ loss=0.0612 ][Training] 6/100 [>.............................] - ETA: 3:33  [ loss=0.0599 ][Training] 7/100 [=>............................] - ETA: 3:26  [ loss=0.0564 ][Training] 8/100 [=>............................] - ETA: 3:19  [ loss=0.0610 ][Training] 9/100 [=>............................] - ETA: 3:14  [ loss=0.0643 ][Training] 10/100 [==>...........................] - ETA: 3:09  [ loss=0.0560 ][Training] 11/100 [==>...........................] - ETA: 3:04  [ loss=0.0469 ][Training] 12/100 [==>...........................] - ETA: 3:00  [ loss=0.0752 ][Training] 13/100 [==>...........................] - ETA: 2:57  [ loss=0.0640 ][Training] 14/100 [===>..........................] - ETA: 2:54  [ loss=0.0467 ][Training] 15/100 [===>..........................] - ETA: 2:51  [ loss=0.0517 ][Training] 16/100 [===>..........................] - ETA: 2:48  [ loss=0.0797 ][Training] 17/100 [====>.........................] - ETA: 2:45  [ loss=0.0532 ][Training] 18/100 [====>.........................] - ETA: 2:43  [ loss=0.0566 ][Training] 19/100 [====>.........................] - ETA: 2:40  [ loss=0.0538 ][Training] 20/100 [=====>........................] - ETA: 2:37  [ loss=0.0542 ][Training] 21/100 [=====>........................] - ETA: 2:35  [ loss=0.0592 ][Training] 22/100 [=====>........................] - ETA: 2:32  [ loss=0.0561 ][Training] 23/100 [=====>........................] - ETA: 2:30  [ loss=0.0511 ][Training] 24/100 [======>.......................] - ETA: 2:27  [ loss=0.0492 ][Training] 25/100 [======>.......................] - ETA: 2:25  [ loss=0.0498 ][Training] 26/100 [======>.......................] - ETA: 2:23  [ loss=0.0638 ][Training] 27/100 [=======>......................] - ETA: 2:21  [ loss=0.0576 ][Training] 28/100 [=======>......................] - ETA: 2:18  [ loss=0.0599 ][Training] 29/100 [=======>......................] - ETA: 2:16  [ loss=0.0490 ][Training] 30/100 [========>.....................] - ETA: 2:15  [ loss=0.0734 ][Training] 31/100 [========>.....................] - ETA: 2:12  [ loss=0.0526 ][Training] 32/100 [========>.....................] - ETA: 2:10  [ loss=0.0751 ][Training] 33/100 [========>.....................] - ETA: 2:08  [ loss=0.0831 ][Training] 34/100 [=========>....................] - ETA: 2:06  [ loss=0.0591 ][Training] 35/100 [=========>....................] - ETA: 2:04  [ loss=0.0605 ][Training] 36/100 [=========>....................] - ETA: 2:02  [ loss=0.0556 ][Training] 37/100 [==========>...................] - ETA: 1:59  [ loss=0.0632 ][Training] 38/100 [==========>...................] - ETA: 1:57  [ loss=0.0663 ][Training] 39/100 [==========>...................] - ETA: 1:55  [ loss=0.0524 ][Training] 40/100 [===========>..................] - ETA: 1:53  [ loss=0.0498 ][Training] 41/100 [===========>..................] - ETA: 1:51  [ loss=0.0554 ][Training] 42/100 [===========>..................] - ETA: 1:49  [ loss=0.0552 ][Training] 43/100 [===========>..................] - ETA: 1:47  [ loss=0.0581 ][Training] 44/100 [============>.................] - ETA: 1:45  [ loss=0.0626 ][Training] 45/100 [============>.................] - ETA: 1:44  [ loss=0.0638 ][Training] 46/100 [============>.................] - ETA: 1:42  [ loss=0.0711 ][Training] 47/100 [=============>................] - ETA: 1:40  [ loss=0.0679 ][Training] 48/100 [=============>................] - ETA: 1:38  [ loss=0.0605 ][Training] 49/100 [=============>................] - ETA: 1:36  [ loss=0.0738 ][Training] 50/100 [==============>...............] - ETA: 1:34  [ loss=0.0570 ][Training] 51/100 [==============>...............] - ETA: 1:32  [ loss=0.0696 ][Training] 52/100 [==============>...............] - ETA: 1:30  [ loss=0.0793 ][Training] 53/100 [==============>...............] - ETA: 1:28  [ loss=0.0569 ][Training] 54/100 [===============>..............] - ETA: 1:26  [ loss=0.0661 ][Training] 55/100 [===============>..............] - ETA: 1:25  [ loss=0.0571 ][Training] 56/100 [===============>..............] - ETA: 1:23  [ loss=0.0675 ][Training] 57/100 [================>.............] - ETA: 1:21  [ loss=0.0626 ][Training] 58/100 [================>.............] - ETA: 1:19  [ loss=0.0531 ][Training] 59/100 [================>.............] - ETA: 1:17  [ loss=0.0619 ][Training] 60/100 [=================>............] - ETA: 1:15  [ loss=0.0640 ][Training] 61/100 [=================>............] - ETA: 1:13  [ loss=0.0533 ][Training] 62/100 [=================>............] - ETA: 1:11  [ loss=0.0819 ][Training] 63/100 [=================>............] - ETA: 1:09  [ loss=0.0406 ][Training] 64/100 [==================>...........] - ETA: 1:07  [ loss=0.0652 ][Training] 65/100 [==================>...........] - ETA: 1:05  [ loss=0.0577 ][Training] 66/100 [==================>...........] - ETA: 1:03  [ loss=0.0619 ][Training] 67/100 [===================>..........] - ETA: 1:02  [ loss=0.0645 ][Training] 68/100 [===================>..........] - ETA: 1:00  [ loss=0.0486 ][Training] 69/100 [===================>..........] - ETA: 58s  [ loss=0.0672 ][Training] 70/100 [====================>.........] - ETA: 56s  [ loss=0.0587 ][Training] 71/100 [====================>.........] - ETA: 54s  [ loss=0.0598 ][Training] 72/100 [====================>.........] - ETA: 52s  [ loss=0.0494 ][Training] 73/100 [====================>.........] - ETA: 50s  [ loss=0.0582 ][Training] 74/100 [=====================>........] - ETA: 48s  [ loss=0.0530 ][Training] 75/100 [=====================>........] - ETA: 46s  [ loss=0.0689 ][Training] 76/100 [=====================>........] - ETA: 45s  [ loss=0.0588 ][Training] 77/100 [======================>.......] - ETA: 43s  [ loss=0.0645 ][Training] 78/100 [======================>.......] - ETA: 41s  [ loss=0.0648 ][Training] 79/100 [======================>.......] - ETA: 39s  [ loss=0.0653 ][Training] 80/100 [=======================>......] - ETA: 37s  [ loss=0.0620 ][Training] 81/100 [=======================>......] - ETA: 35s  [ loss=0.0653 ][Training] 82/100 [=======================>......] - ETA: 33s  [ loss=0.0630 ][Training] 83/100 [=======================>......] - ETA: 31s  [ loss=0.0587 ][Training] 84/100 [========================>.....] - ETA: 29s  [ loss=0.0628 ][Training] 85/100 [========================>.....] - ETA: 28s  [ loss=0.0631 ][Training] 86/100 [========================>.....] - ETA: 26s  [ loss=0.0475 ][Training] 87/100 [=========================>....] - ETA: 24s  [ loss=0.0608 ][Training] 88/100 [=========================>....] - ETA: 22s  [ loss=0.0748 ][Training] 89/100 [=========================>....] - ETA: 20s  [ loss=0.0623 ][Training] 90/100 [==========================>...] - ETA: 18s  [ loss=0.0618 ][Training] 91/100 [==========================>...] - ETA: 16s  [ loss=0.0483 ][Training] 92/100 [==========================>...] - ETA: 14s  [ loss=0.0580 ][Training] 93/100 [==========================>...] - ETA: 13s  [ loss=0.0502 ][Training] 94/100 [===========================>..] - ETA: 11s  [ loss=0.0583 ][Training] 95/100 [===========================>..] - ETA: 9s  [ loss=0.0665 ][Training] 96/100 [===========================>..] - ETA: 7s  [ loss=0.0487 ][Training] 97/100 [============================>.] - ETA: 5s  [ loss=0.0504 ][Training] 98/100 [============================>.] - ETA: 3s  [ loss=0.0568 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=0.0749 ][Training] 100/100 [==============================] 1.9s/step  [ loss=0.0453 ]
 
03/21/2023 10:47:47 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 10:47:47 - INFO - __main__ -     Num examples = 1000
03/21/2023 10:47:47 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 38s  [ loss=0.1001 ][Evaluating] 1/25 [>.............................] - ETA: 38s[Evaluating] 2/25 [=>............................] - ETA: 33s  [ loss=0.0772 ][Evaluating] 2/25 [=>............................] - ETA: 33s[Evaluating] 3/25 [==>...........................] - ETA: 30s  [ loss=0.0876 ][Evaluating] 3/25 [==>...........................] - ETA: 30s[Evaluating] 4/25 [===>..........................] - ETA: 28s  [ loss=0.0738 ][Evaluating] 4/25 [===>..........................] - ETA: 28s[Evaluating] 5/25 [=====>........................] - ETA: 27s  [ loss=0.1051 ][Evaluating] 5/25 [=====>........................] - ETA: 27s[Evaluating] 6/25 [======>.......................] - ETA: 25s  [ loss=0.0856 ][Evaluating] 6/25 [======>.......................] - ETA: 25s[Evaluating] 7/25 [=======>......................] - ETA: 24s  [ loss=0.1110 ][Evaluating] 7/25 [=======>......................] - ETA: 24s[Evaluating] 8/25 [========>.....................] - ETA: 22s  [ loss=0.0805 ][Evaluating] 8/25 [========>.....................] - ETA: 23s[Evaluating] 9/25 [=========>....................] - ETA: 21s  [ loss=0.0766 ][Evaluating] 9/25 [=========>....................] - ETA: 21s[Evaluating] 10/25 [===========>..................] - ETA: 20s  [ loss=0.0816 ][Evaluating] 10/25 [===========>..................] - ETA: 20s[Evaluating] 11/25 [============>.................] - ETA: 18s  [ loss=0.0802 ][Evaluating] 11/25 [============>.................] - ETA: 18s[Evaluating] 12/25 [=============>................] - ETA: 17s  [ loss=0.0743 ][Evaluating] 12/25 [=============>................] - ETA: 17s[Evaluating] 13/25 [==============>...............] - ETA: 16s  [ loss=0.0876 ][Evaluating] 13/25 [==============>...............] - ETA: 16s[Evaluating] 14/25 [===============>..............] - ETA: 14s  [ loss=0.0878 ][Evaluating] 14/25 [===============>..............] - ETA: 14s[Evaluating] 15/25 [=================>............] - ETA: 13s  [ loss=0.1235 ][Evaluating] 15/25 [=================>............] - ETA: 13s[Evaluating] 16/25 [==================>...........] - ETA: 12s  [ loss=0.1072 ][Evaluating] 16/25 [==================>...........] - ETA: 12s[Evaluating] 17/25 [===================>..........] - ETA: 10s  [ loss=0.0873 ][Evaluating] 17/25 [===================>..........] - ETA: 10s[Evaluating] 18/25 [====================>.........] - ETA: 9s  [ loss=0.0933 ][Evaluating] 18/25 [====================>.........] - ETA: 9s[Evaluating] 19/25 [=====================>........] - ETA: 8s  [ loss=0.0851 ][Evaluating] 19/25 [=====================>........] - ETA: 8s[Evaluating] 20/25 [=======================>......] - ETA: 6s  [ loss=0.0724 ][Evaluating] 20/25 [=======================>......] - ETA: 6s[Evaluating] 21/25 [========================>.....] - ETA: 5s  [ loss=0.0987 ][Evaluating] 21/25 [========================>.....] - ETA: 5s[Evaluating] 22/25 [=========================>....] - ETA: 4s  [ loss=0.0771 ][Evaluating] 22/25 [=========================>....] - ETA: 4s[Evaluating] 23/25 [==========================>...] - ETA: 2s  [ loss=0.0820 ][Evaluating] 23/25 [==========================>...] - ETA: 2s[Evaluating] 24/25 [===========================>..] - ETA: 1s  [ loss=0.1021 ][Evaluating] 24/25 [===========================>..] - ETA: 1s[Evaluating] 25/25 [==============================] 1.3s/step  [ loss=0.0907 ]
[Evaluating] 25/25 [==============================] 1.3s/step
03/21/2023 10:48:21 - INFO - __main__ -   

03/21/2023 10:48:21 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 10:48:21 - INFO - __main__ -    acc: 0.7137 - recall: 0.7411 - f1: 0.7272 - loss: 0.0891 
03/21/2023 10:48:21 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 10:48:21 - INFO - __main__ -   ******* LOC results ********
03/21/2023 10:48:21 - INFO - __main__ -    acc: 0.7927 - recall: 0.8276 - f1: 0.8097 
03/21/2023 10:48:21 - INFO - __main__ -   ******* MISC results ********
03/21/2023 10:48:21 - INFO - __main__ -    acc: 0.3761 - recall: 0.3727 - f1: 0.3744 
03/21/2023 10:48:21 - INFO - __main__ -   ******* ORG results ********
03/21/2023 10:48:21 - INFO - __main__ -    acc: 0.6055 - recall: 0.6275 - f1: 0.6163 
03/21/2023 10:48:21 - INFO - __main__ -   ******* PER results ********
03/21/2023 10:48:21 - INFO - __main__ -    acc: 0.8141 - recall: 0.8569 - f1: 0.8350 
03/21/2023 10:48:22 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-300
03/21/2023 10:48:25 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-300
03/21/2023 10:48:25 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:48:44 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:48:44 - INFO - __main__ -   


Epoch: 3/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/100 [..............................] - ETA: 6:11  [ loss=0.0464 ][Training] 2/100 [..............................] - ETA: 4:34  [ loss=0.0550 ][Training] 3/100 [..............................] - ETA: 4:00  [ loss=0.0405 ][Training] 4/100 [>.............................] - ETA: 3:42  [ loss=0.0478 ][Training] 5/100 [>.............................] - ETA: 3:31  [ loss=0.0386 ][Training] 6/100 [>.............................] - ETA: 3:23  [ loss=0.0495 ][Training] 7/100 [=>............................] - ETA: 3:17  [ loss=0.0409 ][Training] 8/100 [=>............................] - ETA: 3:12  [ loss=0.0506 ][Training] 9/100 [=>............................] - ETA: 3:08  [ loss=0.0449 ][Training] 10/100 [==>...........................] - ETA: 3:05  [ loss=0.0516 ][Training] 11/100 [==>...........................] - ETA: 3:01  [ loss=0.0466 ][Training] 12/100 [==>...........................] - ETA: 2:59  [ loss=0.0495 ][Training] 13/100 [==>...........................] - ETA: 2:56  [ loss=0.0426 ][Training] 14/100 [===>..........................] - ETA: 2:53  [ loss=0.0513 ][Training] 15/100 [===>..........................] - ETA: 2:50  [ loss=0.0439 ][Training] 16/100 [===>..........................] - ETA: 2:48  [ loss=0.0503 ][Training] 17/100 [====>.........................] - ETA: 2:45  [ loss=0.0440 ][Training] 18/100 [====>.........................] - ETA: 2:43  [ loss=0.0454 ][Training] 19/100 [====>.........................] - ETA: 2:40  [ loss=0.0473 ][Training] 20/100 [=====>........................] - ETA: 2:38  [ loss=0.0497 ][Training] 21/100 [=====>........................] - ETA: 2:35  [ loss=0.0469 ][Training] 22/100 [=====>........................] - ETA: 2:33  [ loss=0.0471 ][Training] 23/100 [=====>........................] - ETA: 2:30  [ loss=0.0396 ][Training] 24/100 [======>.......................] - ETA: 2:28  [ loss=0.0436 ][Training] 25/100 [======>.......................] - ETA: 2:26  [ loss=0.0372 ][Training] 26/100 [======>.......................] - ETA: 2:23  [ loss=0.0410 ][Training] 27/100 [=======>......................] - ETA: 2:21  [ loss=0.0388 ][Training] 28/100 [=======>......................] - ETA: 2:19  [ loss=0.0418 ][Training] 29/100 [=======>......................] - ETA: 2:16  [ loss=0.0464 ][Training] 30/100 [========>.....................] - ETA: 2:14  [ loss=0.0430 ][Training] 31/100 [========>.....................] - ETA: 2:12  [ loss=0.0382 ][Training] 32/100 [========>.....................] - ETA: 2:10  [ loss=0.0367 ][Training] 33/100 [========>.....................] - ETA: 2:08  [ loss=0.0421 ][Training] 34/100 [=========>....................] - ETA: 2:06  [ loss=0.0580 ][Training] 35/100 [=========>....................] - ETA: 2:04  [ loss=0.0422 ][Training] 36/100 [=========>....................] - ETA: 2:02  [ loss=0.0444 ][Training] 37/100 [==========>...................] - ETA: 2:00  [ loss=0.0436 ][Training] 38/100 [==========>...................] - ETA: 1:58  [ loss=0.0505 ][Training] 39/100 [==========>...................] - ETA: 1:56  [ loss=0.0455 ][Training] 40/100 [===========>..................] - ETA: 1:54  [ loss=0.0440 ][Training] 41/100 [===========>..................] - ETA: 1:52  [ loss=0.0339 ][Training] 42/100 [===========>..................] - ETA: 1:50  [ loss=0.0541 ][Training] 43/100 [===========>..................] - ETA: 1:48  [ loss=0.0464 ][Training] 44/100 [============>.................] - ETA: 1:46  [ loss=0.0409 ][Training] 45/100 [============>.................] - ETA: 1:44  [ loss=0.0401 ][Training] 46/100 [============>.................] - ETA: 1:42  [ loss=0.0483 ][Training] 47/100 [=============>................] - ETA: 1:40  [ loss=0.0507 ][Training] 48/100 [=============>................] - ETA: 1:38  [ loss=0.0517 ][Training] 49/100 [=============>................] - ETA: 1:36  [ loss=0.0420 ][Training] 50/100 [==============>...............] - ETA: 1:34  [ loss=0.0402 ][Training] 51/100 [==============>...............] - ETA: 1:32  [ loss=0.0433 ][Training] 52/100 [==============>...............] - ETA: 1:30  [ loss=0.0553 ][Training] 53/100 [==============>...............] - ETA: 1:28  [ loss=0.0431 ][Training] 54/100 [===============>..............] - ETA: 1:26  [ loss=0.0410 ][Training] 55/100 [===============>..............] - ETA: 1:25  [ loss=0.0411 ][Training] 56/100 [===============>..............] - ETA: 1:23  [ loss=0.0464 ][Training] 57/100 [================>.............] - ETA: 1:21  [ loss=0.0494 ][Training] 58/100 [================>.............] - ETA: 1:19  [ loss=0.0504 ][Training] 59/100 [================>.............] - ETA: 1:17  [ loss=0.0414 ][Training] 60/100 [=================>............] - ETA: 1:15  [ loss=0.0474 ][Training] 61/100 [=================>............] - ETA: 1:13  [ loss=0.0400 ][Training] 62/100 [=================>............] - ETA: 1:11  [ loss=0.0375 ][Training] 63/100 [=================>............] - ETA: 1:09  [ loss=0.0417 ][Training] 64/100 [==================>...........] - ETA: 1:07  [ loss=0.0462 ][Training] 65/100 [==================>...........] - ETA: 1:05  [ loss=0.0461 ][Training] 66/100 [==================>...........] - ETA: 1:04  [ loss=0.0433 ][Training] 67/100 [===================>..........] - ETA: 1:02  [ loss=0.0466 ][Training] 68/100 [===================>..........] - ETA: 1:00  [ loss=0.0455 ][Training] 69/100 [===================>..........] - ETA: 58s  [ loss=0.0434 ][Training] 70/100 [====================>.........] - ETA: 56s  [ loss=0.0514 ][Training] 71/100 [====================>.........] - ETA: 54s  [ loss=0.0472 ][Training] 72/100 [====================>.........] - ETA: 52s  [ loss=0.0553 ][Training] 73/100 [====================>.........] - ETA: 50s  [ loss=0.0420 ][Training] 74/100 [=====================>........] - ETA: 48s  [ loss=0.0539 ][Training] 75/100 [=====================>........] - ETA: 47s  [ loss=0.0428 ][Training] 76/100 [=====================>........] - ETA: 45s  [ loss=0.0403 ][Training] 77/100 [======================>.......] - ETA: 43s  [ loss=0.0504 ][Training] 78/100 [======================>.......] - ETA: 41s  [ loss=0.0550 ][Training] 79/100 [======================>.......] - ETA: 39s  [ loss=0.0357 ][Training] 80/100 [=======================>......] - ETA: 37s  [ loss=0.0420 ][Training] 81/100 [=======================>......] - ETA: 35s  [ loss=0.0427 ][Training] 82/100 [=======================>......] - ETA: 33s  [ loss=0.0441 ][Training] 83/100 [=======================>......] - ETA: 32s  [ loss=0.0435 ][Training] 84/100 [========================>.....] - ETA: 30s  [ loss=0.0468 ][Training] 85/100 [========================>.....] - ETA: 28s  [ loss=0.0458 ][Training] 86/100 [========================>.....] - ETA: 26s  [ loss=0.0467 ][Training] 87/100 [=========================>....] - ETA: 24s  [ loss=0.0454 ][Training] 88/100 [=========================>....] - ETA: 22s  [ loss=0.0489 ][Training] 89/100 [=========================>....] - ETA: 20s  [ loss=0.0499 ][Training] 90/100 [==========================>...] - ETA: 18s  [ loss=0.0359 ][Training] 91/100 [==========================>...] - ETA: 16s  [ loss=0.0509 ][Training] 92/100 [==========================>...] - ETA: 15s  [ loss=0.0453 ][Training] 93/100 [==========================>...] - ETA: 13s  [ loss=0.0529 ][Training] 94/100 [===========================>..] - ETA: 11s  [ loss=0.0424 ][Training] 95/100 [===========================>..] - ETA: 9s  [ loss=0.0405 ][Training] 96/100 [===========================>..] - ETA: 7s  [ loss=0.0480 ][Training] 97/100 [============================>.] - ETA: 5s  [ loss=0.0422 ][Training] 98/100 [============================>.] - ETA: 3s  [ loss=0.0477 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=0.0483 ][Training] 100/100 [==============================] 1.9s/step  [ loss=0.0373 ]
 
03/21/2023 10:51:52 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 10:51:52 - INFO - __main__ -     Num examples = 1000
03/21/2023 10:51:52 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 34s  [ loss=0.1227 ][Evaluating] 1/25 [>.............................] - ETA: 35s[Evaluating] 2/25 [=>............................] - ETA: 31s  [ loss=0.0802 ][Evaluating] 2/25 [=>............................] - ETA: 31s[Evaluating] 3/25 [==>...........................] - ETA: 29s  [ loss=0.0906 ][Evaluating] 3/25 [==>...........................] - ETA: 29s[Evaluating] 4/25 [===>..........................] - ETA: 28s  [ loss=0.0757 ][Evaluating] 4/25 [===>..........................] - ETA: 28s[Evaluating] 5/25 [=====>........................] - ETA: 27s  [ loss=0.1316 ][Evaluating] 5/25 [=====>........................] - ETA: 27s[Evaluating] 6/25 [======>.......................] - ETA: 25s  [ loss=0.0878 ][Evaluating] 6/25 [======>.......................] - ETA: 26s[Evaluating] 7/25 [=======>......................] - ETA: 24s  [ loss=0.1382 ][Evaluating] 7/25 [=======>......................] - ETA: 24s[Evaluating] 8/25 [========>.....................] - ETA: 23s  [ loss=0.0903 ][Evaluating] 8/25 [========>.....................] - ETA: 23s[Evaluating] 9/25 [=========>....................] - ETA: 21s  [ loss=0.0938 ][Evaluating] 9/25 [=========>....................] - ETA: 21s[Evaluating] 10/25 [===========>..................] - ETA: 20s  [ loss=0.0953 ][Evaluating] 10/25 [===========>..................] - ETA: 20s[Evaluating] 11/25 [============>.................] - ETA: 19s  [ loss=0.0923 ][Evaluating] 11/25 [============>.................] - ETA: 19s[Evaluating] 12/25 [=============>................] - ETA: 17s  [ loss=0.0784 ][Evaluating] 12/25 [=============>................] - ETA: 17s[Evaluating] 13/25 [==============>...............] - ETA: 16s  [ loss=0.0937 ][Evaluating] 13/25 [==============>...............] - ETA: 16s[Evaluating] 14/25 [===============>..............] - ETA: 15s  [ loss=0.0879 ][Evaluating] 14/25 [===============>..............] - ETA: 15s[Evaluating] 15/25 [=================>............] - ETA: 13s  [ loss=0.1363 ][Evaluating] 15/25 [=================>............] - ETA: 13s[Evaluating] 16/25 [==================>...........] - ETA: 12s  [ loss=0.1039 ][Evaluating] 16/25 [==================>...........] - ETA: 12s[Evaluating] 17/25 [===================>..........] - ETA: 10s  [ loss=0.1008 ][Evaluating] 17/25 [===================>..........] - ETA: 10s[Evaluating] 18/25 [====================>.........] - ETA: 9s  [ loss=0.0844 ][Evaluating] 18/25 [====================>.........] - ETA: 9s[Evaluating] 19/25 [=====================>........] - ETA: 8s  [ loss=0.1014 ][Evaluating] 19/25 [=====================>........] - ETA: 8s[Evaluating] 20/25 [=======================>......] - ETA: 6s  [ loss=0.0704 ][Evaluating] 20/25 [=======================>......] - ETA: 6s[Evaluating] 21/25 [========================>.....] - ETA: 5s  [ loss=0.1033 ][Evaluating] 21/25 [========================>.....] - ETA: 5s[Evaluating] 22/25 [=========================>....] - ETA: 4s  [ loss=0.0874 ][Evaluating] 22/25 [=========================>....] - ETA: 4s[Evaluating] 23/25 [==========================>...] - ETA: 2s  [ loss=0.0962 ][Evaluating] 23/25 [==========================>...] - ETA: 2s[Evaluating] 24/25 [===========================>..] - ETA: 1s  [ loss=0.1055 ][Evaluating] 24/25 [===========================>..] - ETA: 1s[Evaluating] 25/25 [==============================] 1.3s/step  [ loss=0.0900 ]
[Evaluating] 25/25 [==============================] 1.3s/step
03/21/2023 10:52:26 - INFO - __main__ -   

03/21/2023 10:52:26 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 10:52:26 - INFO - __main__ -    acc: 0.6978 - recall: 0.7476 - f1: 0.7218 - loss: 0.0975 
03/21/2023 10:52:26 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 10:52:26 - INFO - __main__ -   ******* LOC results ********
03/21/2023 10:52:26 - INFO - __main__ -    acc: 0.7895 - recall: 0.8333 - f1: 0.8108 
03/21/2023 10:52:26 - INFO - __main__ -   ******* MISC results ********
03/21/2023 10:52:26 - INFO - __main__ -    acc: 0.3523 - recall: 0.3091 - f1: 0.3293 
03/21/2023 10:52:26 - INFO - __main__ -   ******* ORG results ********
03/21/2023 10:52:26 - INFO - __main__ -    acc: 0.5169 - recall: 0.6802 - f1: 0.5874 
03/21/2023 10:52:26 - INFO - __main__ -   ******* PER results ********
03/21/2023 10:52:26 - INFO - __main__ -    acc: 0.8265 - recall: 0.8714 - f1: 0.8483 
03/21/2023 10:52:28 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-400
03/21/2023 10:52:30 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-400
03/21/2023 10:52:31 - INFO - __main__ -   


Epoch: 4/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/100 [..............................] - ETA: 5:18  [ loss=0.0432 ][Training] 2/100 [..............................] - ETA: 4:04  [ loss=0.0345 ][Training] 3/100 [..............................] - ETA: 3:39  [ loss=0.0395 ][Training] 4/100 [>.............................] - ETA: 3:26  [ loss=0.0457 ][Training] 5/100 [>.............................] - ETA: 3:17  [ loss=0.0389 ][Training] 6/100 [>.............................] - ETA: 3:11  [ loss=0.0402 ][Training] 7/100 [=>............................] - ETA: 3:06  [ loss=0.0306 ][Training] 8/100 [=>............................] - ETA: 3:02  [ loss=0.0371 ][Training] 9/100 [=>............................] - ETA: 2:58  [ loss=0.0376 ][Training] 10/100 [==>...........................] - ETA: 2:55  [ loss=0.0386 ][Training] 11/100 [==>...........................] - ETA: 2:52  [ loss=0.0410 ][Training] 12/100 [==>...........................] - ETA: 2:50  [ loss=0.0331 ][Training] 13/100 [==>...........................] - ETA: 2:47  [ loss=0.0331 ][Training] 14/100 [===>..........................] - ETA: 2:44  [ loss=0.0439 ][Training] 15/100 [===>..........................] - ETA: 2:44  [ loss=0.0355 ][Training] 16/100 [===>..........................] - ETA: 2:41  [ loss=0.0327 ][Training] 17/100 [====>.........................] - ETA: 2:39  [ loss=0.0422 ][Training] 18/100 [====>.........................] - ETA: 2:37  [ loss=0.0330 ][Training] 19/100 [====>.........................] - ETA: 2:34  [ loss=0.0383 ][Training] 20/100 [=====>........................] - ETA: 2:32  [ loss=0.0372 ][Training] 21/100 [=====>........................] - ETA: 2:30  [ loss=0.0335 ][Training] 22/100 [=====>........................] - ETA: 2:28  [ loss=0.0335 ][Training] 23/100 [=====>........................] - ETA: 2:26  [ loss=0.0363 ][Training] 24/100 [======>.......................] - ETA: 2:23  [ loss=0.0360 ][Training] 25/100 [======>.......................] - ETA: 2:21  [ loss=0.0498 ][Training] 26/100 [======>.......................] - ETA: 2:19  [ loss=0.0421 ][Training] 27/100 [=======>......................] - ETA: 2:17  [ loss=0.0357 ][Training] 28/100 [=======>......................] - ETA: 2:15  [ loss=0.0306 ][Training] 29/100 [=======>......................] - ETA: 2:13  [ loss=0.0419 ][Training] 30/100 [========>.....................] - ETA: 2:12  [ loss=0.0332 ][Training] 31/100 [========>.....................] - ETA: 2:10  [ loss=0.0351 ][Training] 32/100 [========>.....................] - ETA: 2:08  [ loss=0.0363 ][Training] 33/100 [========>.....................] - ETA: 2:06  [ loss=0.0361 ][Training] 34/100 [=========>....................] - ETA: 2:04  [ loss=0.0301 ][Training] 35/100 [=========>....................] - ETA: 2:02  [ loss=0.0363 ][Training] 36/100 [=========>....................] - ETA: 2:00  [ loss=0.0318 ][Training] 37/100 [==========>...................] - ETA: 1:58  [ loss=0.0341 ][Training] 38/100 [==========>...................] - ETA: 1:56  [ loss=0.0442 ][Training] 39/100 [==========>...................] - ETA: 1:54  [ loss=0.0361 ][Training] 40/100 [===========>..................] - ETA: 1:52  [ loss=0.0296 ][Training] 41/100 [===========>..................] - ETA: 1:50  [ loss=0.0357 ][Training] 42/100 [===========>..................] - ETA: 1:48  [ loss=0.0393 ][Training] 43/100 [===========>..................] - ETA: 1:46  [ loss=0.0365 ][Training] 44/100 [============>.................] - ETA: 1:44  [ loss=0.0375 ][Training] 45/100 [============>.................] - ETA: 1:42  [ loss=0.0389 ][Training] 46/100 [============>.................] - ETA: 1:40  [ loss=0.0320 ][Training] 47/100 [=============>................] - ETA: 1:38  [ loss=0.0337 ][Training] 48/100 [=============>................] - ETA: 1:36  [ loss=0.0380 ][Training] 49/100 [=============>................] - ETA: 1:34  [ loss=0.0324 ][Training] 50/100 [==============>...............] - ETA: 1:32  [ loss=0.0318 ][Training] 51/100 [==============>...............] - ETA: 1:30  [ loss=0.0399 ][Training] 52/100 [==============>...............] - ETA: 1:28  [ loss=0.0386 ][Training] 53/100 [==============>...............] - ETA: 1:27  [ loss=0.0374 ][Training] 54/100 [===============>..............] - ETA: 1:25  [ loss=0.0331 ][Training] 55/100 [===============>..............] - ETA: 1:23  [ loss=0.0353 ][Training] 56/100 [===============>..............] - ETA: 1:21  [ loss=0.0420 ][Training] 57/100 [================>.............] - ETA: 1:19  [ loss=0.0351 ][Training] 58/100 [================>.............] - ETA: 1:17  [ loss=0.0398 ][Training] 59/100 [================>.............] - ETA: 1:15  [ loss=0.0355 ][Training] 60/100 [=================>............] - ETA: 1:14  [ loss=0.0418 ][Training] 61/100 [=================>............] - ETA: 1:12  [ loss=0.0363 ][Training] 62/100 [=================>............] - ETA: 1:10  [ loss=0.0376 ][Training] 63/100 [=================>............] - ETA: 1:08  [ loss=0.0342 ][Training] 64/100 [==================>...........] - ETA: 1:06  [ loss=0.0337 ][Training] 65/100 [==================>...........] - ETA: 1:04  [ loss=0.0386 ][Training] 66/100 [==================>...........] - ETA: 1:02  [ loss=0.0404 ][Training] 67/100 [===================>..........] - ETA: 1:01  [ loss=0.0331 ][Training] 68/100 [===================>..........] - ETA: 59s  [ loss=0.0321 ][Training] 69/100 [===================>..........] - ETA: 57s  [ loss=0.0392 ][Training] 70/100 [====================>.........] - ETA: 55s  [ loss=0.0371 ][Training] 71/100 [====================>.........] - ETA: 53s  [ loss=0.0359 ][Training] 72/100 [====================>.........] - ETA: 51s  [ loss=0.0341 ][Training] 73/100 [====================>.........] - ETA: 49s  [ loss=0.0320 ][Training] 74/100 [=====================>........] - ETA: 48s  [ loss=0.0386 ][Training] 75/100 [=====================>........] - ETA: 46s  [ loss=0.0364 ][Training] 76/100 [=====================>........] - ETA: 44s  [ loss=0.0326 ][Training] 77/100 [======================>.......] - ETA: 42s  [ loss=0.0302 ][Training] 78/100 [======================>.......] - ETA: 40s  [ loss=0.0420 ][Training] 79/100 [======================>.......] - ETA: 38s  [ loss=0.0329 ][Training] 80/100 [=======================>......] - ETA: 36s  [ loss=0.0324 ][Training] 81/100 [=======================>......] - ETA: 35s  [ loss=0.0337 ][Training] 82/100 [=======================>......] - ETA: 33s  [ loss=0.0317 ][Training] 83/100 [=======================>......] - ETA: 31s  [ loss=0.0340 ][Training] 84/100 [========================>.....] - ETA: 29s  [ loss=0.0359 ][Training] 85/100 [========================>.....] - ETA: 27s  [ loss=0.0323 ][Training] 86/100 [========================>.....] - ETA: 25s  [ loss=0.0360 ][Training] 87/100 [=========================>....] - ETA: 24s  [ loss=0.0367 ][Training] 88/100 [=========================>....] - ETA: 22s  [ loss=0.0342 ][Training] 89/100 [=========================>....] - ETA: 20s  [ loss=0.0328 ][Training] 90/100 [==========================>...] - ETA: 18s  [ loss=0.0390 ][Training] 91/100 [==========================>...] - ETA: 16s  [ loss=0.0358 ][Training] 92/100 [==========================>...] - ETA: 14s  [ loss=0.0355 ][Training] 93/100 [==========================>...] - ETA: 12s  [ loss=0.0296 ][Training] 94/100 [===========================>..] - ETA: 11s  [ loss=0.0288 ][Training] 95/100 [===========================>..] - ETA: 9s  [ loss=0.0325 ][Training] 96/100 [===========================>..] - ETA: 7s  [ loss=0.0347 ][Training] 97/100 [============================>.] - ETA: 5s  [ loss=0.0441 ][Training] 98/100 [============================>.] - ETA: 3s  [ loss=0.0384 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=0.0362 ][Training] 100/100 [==============================] 1.9s/step  [ loss=0.0421 ]
 
03/21/2023 10:55:36 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 10:55:36 - INFO - __main__ -     Num examples = 1000
03/21/2023 10:55:36 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 35s  [ loss=0.1383 ][Evaluating] 1/25 [>.............................] - ETA: 35s[Evaluating] 2/25 [=>............................] - ETA: 32s  [ loss=0.0818 ][Evaluating] 2/25 [=>............................] - ETA: 32s[Evaluating] 3/25 [==>...........................] - ETA: 30s  [ loss=0.1015 ][Evaluating] 3/25 [==>...........................] - ETA: 30s[Evaluating] 4/25 [===>..........................] - ETA: 28s  [ loss=0.0857 ][Evaluating] 4/25 [===>..........................] - ETA: 28s[Evaluating] 5/25 [=====>........................] - ETA: 27s  [ loss=0.1441 ][Evaluating] 5/25 [=====>........................] - ETA: 27s[Evaluating] 6/25 [======>.......................] - ETA: 25s  [ loss=0.0948 ][Evaluating] 6/25 [======>.......................] - ETA: 25s[Evaluating] 7/25 [=======>......................] - ETA: 24s  [ loss=0.1389 ][Evaluating] 7/25 [=======>......................] - ETA: 24s[Evaluating] 8/25 [========>.....................] - ETA: 22s  [ loss=0.1027 ][Evaluating] 8/25 [========>.....................] - ETA: 23s[Evaluating] 9/25 [=========>....................] - ETA: 21s  [ loss=0.0899 ][Evaluating] 9/25 [=========>....................] - ETA: 21s[Evaluating] 10/25 [===========>..................] - ETA: 20s  [ loss=0.0973 ][Evaluating] 10/25 [===========>..................] - ETA: 20s[Evaluating] 11/25 [============>.................] - ETA: 18s  [ loss=0.1001 ][Evaluating] 11/25 [============>.................] - ETA: 18s[Evaluating] 12/25 [=============>................] - ETA: 17s  [ loss=0.0810 ][Evaluating] 12/25 [=============>................] - ETA: 17s[Evaluating] 13/25 [==============>...............] - ETA: 16s  [ loss=0.0979 ][Evaluating] 13/25 [==============>...............] - ETA: 16s[Evaluating] 14/25 [===============>..............] - ETA: 14s  [ loss=0.0863 ][Evaluating] 14/25 [===============>..............] - ETA: 14s[Evaluating] 15/25 [=================>............] - ETA: 13s  [ loss=0.1453 ][Evaluating] 15/25 [=================>............] - ETA: 13s[Evaluating] 16/25 [==================>...........] - ETA: 12s  [ loss=0.1150 ][Evaluating] 16/25 [==================>...........] - ETA: 12s[Evaluating] 17/25 [===================>..........] - ETA: 10s  [ loss=0.1136 ][Evaluating] 17/25 [===================>..........] - ETA: 10s[Evaluating] 18/25 [====================>.........] - ETA: 9s  [ loss=0.0933 ][Evaluating] 18/25 [====================>.........] - ETA: 9s[Evaluating] 19/25 [=====================>........] - ETA: 8s  [ loss=0.0989 ][Evaluating] 19/25 [=====================>........] - ETA: 8s[Evaluating] 20/25 [=======================>......] - ETA: 6s  [ loss=0.0753 ][Evaluating] 20/25 [=======================>......] - ETA: 6s[Evaluating] 21/25 [========================>.....] - ETA: 5s  [ loss=0.1238 ][Evaluating] 21/25 [========================>.....] - ETA: 5s[Evaluating] 22/25 [=========================>....] - ETA: 4s  [ loss=0.0901 ][Evaluating] 22/25 [=========================>....] - ETA: 4s[Evaluating] 23/25 [==========================>...] - ETA: 2s  [ loss=0.1033 ][Evaluating] 23/25 [==========================>...] - ETA: 2s[Evaluating] 24/25 [===========================>..] - ETA: 1s  [ loss=0.1182 ][Evaluating] 24/25 [===========================>..] - ETA: 1s[Evaluating] 25/25 [==============================] 1.3s/step  [ loss=0.0970 ]
[Evaluating] 25/25 [==============================] 1.3s/step
03/21/2023 10:56:09 - INFO - __main__ -   

03/21/2023 10:56:09 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 10:56:09 - INFO - __main__ -    acc: 0.7262 - recall: 0.7541 - f1: 0.7399 - loss: 0.1046 
03/21/2023 10:56:09 - INFO - __main__ -   ***** Entity results  *****
03/21/2023 10:56:09 - INFO - __main__ -   ******* LOC results ********
03/21/2023 10:56:09 - INFO - __main__ -    acc: 0.7880 - recall: 0.8544 - f1: 0.8199 
03/21/2023 10:56:09 - INFO - __main__ -   ******* MISC results ********
03/21/2023 10:56:09 - INFO - __main__ -    acc: 0.4167 - recall: 0.4318 - f1: 0.4241 
03/21/2023 10:56:09 - INFO - __main__ -   ******* ORG results ********
03/21/2023 10:56:09 - INFO - __main__ -    acc: 0.6667 - recall: 0.5344 - f1: 0.5933 
03/21/2023 10:56:09 - INFO - __main__ -   ******* PER results ********
03/21/2023 10:56:09 - INFO - __main__ -    acc: 0.8043 - recall: 0.8859 - f1: 0.8431 
03/21/2023 10:56:10 - INFO - __main__ -   Saving model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-500
03/21/2023 10:56:13 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/checkpoint-500
03/21/2023 10:56:13 - INFO - __main__ -   Saving best eval result model checkpoint to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:56:31 - INFO - __main__ -   Saving optimizer and scheduler states to ../outputs/twitter15_output/alpha0.0001_beta0.0001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr1e-4_clr1e-4_uncased_cd0.2_last/
03/21/2023 10:56:32 - INFO - __main__ -   


Epoch: 5/10
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[Training] 1/100 [..............................] - ETA: 5:36  [ loss=0.0362 ][Training] 2/100 [..............................] - ETA: 4:13  [ loss=0.0290 ][Training] 3/100 [..............................] - ETA: 3:44  [ loss=0.0279 ][Training] 4/100 [>.............................] - ETA: 3:30  [ loss=0.0327 ]