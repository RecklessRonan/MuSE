nohup: ignoring input
03/22/2023 11:25:14 - INFO - __main__ -   label_list: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[START]', '[END]'], length: 12
The number of samples: 3373
The number of images: 3373
The number of samples: 723
The number of images: 723
The number of samples: 723
The number of images: 723
03/22/2023 11:25:14 - INFO - root -   Constructing vocabulary for image caption
03/22/2023 11:25:14 - INFO - __main__ -    The size of vocabulary = 3524
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/22/2023 11:25:30 - INFO - __main__ -   Finish loading model [204M] parameters
03/22/2023 11:25:35 - INFO - __main__ -   Args: Namespace(adam_epsilon=1e-08, alpha=0.001, bert_type='uncased', beta=0.001, cls_init=0, crf_dropout=0.5, crf_learning_rate=5e-05, crop_size=224, cross_dropout=0.2, data_dir='../data/twitter2017', device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, drop_last=True, eval_all_checkpoints=False, evaluate_during_training=True, gradient_accumulation_steps=1, hidden_size=768, id2label={0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC', 9: 'X', 10: '[START]', 11: '[END]'}, image_dir='../data/twitter2017_images', image_dropout=0.0, label2id={'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'X': 9, '[START]': 10, '[END]': 11}, learning_rate=5e-05, load_image_checkpoint=False, load_text_checkpoint=False, local_rank=-1, logging_steps=100, markup='bio', max_grad_norm=1.0, max_seq_length=64, n_gpu=1, num_layers=6, num_train_epochs=10, num_workers=8, output_dir='../outputs_ct/twitter17_output/alpha0.001_beta0.001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr5e-5_clr5e-5_uncased_cd0.2_last/', per_gpu_eval_batch_size=40, per_gpu_train_batch_size=40, predict_checkpoints=0, replace_end=3, replace_start=1, resnet_pretrained_dir='../models/resnet152-b121ed2d.pth', save_steps=100, seed=42, sigma=1.0, skip_connection=True, task='twitter17', test_batch_size=1, text_dropout=0.0, theta=0.1, ti_crop_size=32, train_batch_size=40, use_quantile=True, use_xlmr=False, warmup_proportion=0.1, weight_decay=0.01)
03/22/2023 11:25:35 - INFO - __main__ -   Summary dir: ../outputs_ct/twitter17_output/alpha0.001_beta0.001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr5e-5_clr5e-5_uncased_cd0.2_last/summary_1679455535
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
03/22/2023 11:25:35 - INFO - __main__ -   ***** Running training *****
03/22/2023 11:25:35 - INFO - __main__ -     Num examples = 3373
03/22/2023 11:25:35 - INFO - __main__ -     Num Epochs = 10
03/22/2023 11:25:35 - INFO - __main__ -     Gradient Accumulation steps = 1
03/22/2023 11:25:35 - INFO - __main__ -     Total optimization steps = 840

Epoch: 0/10
/home/ubuntu/multimodal-fusion/MuSE/code/models.py:1235: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
[Training] 1/84 [..............................] - ETA: 3:25  [ loss=55.4694 ][Training] 2/84 [..............................] - ETA: 2:43  [ loss=54.8095 ][Training] 3/84 [>.............................] - ETA: 2:25  [ loss=58.2520 ][Training] 4/84 [>.............................] - ETA: 2:17  [ loss=54.1784 ][Training] 5/84 [>.............................] - ETA: 2:11  [ loss=52.3001 ][Training] 6/84 [=>............................] - ETA: 2:06  [ loss=52.1543 ][Training] 7/84 [=>............................] - ETA: 2:02  [ loss=52.2874 ][Training] 8/84 [=>............................] - ETA: 1:59  [ loss=42.8072 ][Training] 9/84 [==>...........................] - ETA: 1:56  [ loss=40.4786 ][Training] 10/84 [==>...........................] - ETA: 1:54  [ loss=39.0365 ][Training] 11/84 [==>...........................] - ETA: 1:51  [ loss=33.1405 ][Training] 12/84 [===>..........................] - ETA: 1:49  [ loss=31.2198 ][Training] 13/84 [===>..........................] - ETA: 1:47  [ loss=24.3617 ][Training] 14/84 [====>.........................] - ETA: 1:46  [ loss=26.2463 ][Training] 15/84 [====>.........................] - ETA: 1:44  [ loss=23.6465 ][Training] 16/84 [====>.........................] - ETA: 1:42  [ loss=23.5195 ][Training] 17/84 [=====>........................] - ETA: 1:40  [ loss=23.3033 ][Training] 18/84 [=====>........................] - ETA: 1:38  [ loss=23.8986 ][Training] 19/84 [=====>........................] - ETA: 1:36  [ loss=22.0959 ][Training] 20/84 [======>.......................] - ETA: 1:34  [ loss=24.1102 ][Training] 21/84 [======>.......................] - ETA: 1:33  [ loss=22.0725 ][Training] 22/84 [======>.......................] - ETA: 1:31  [ loss=22.6885 ][Training] 23/84 [=======>......................] - ETA: 1:29  [ loss=21.5439 ][Training] 24/84 [=======>......................] - ETA: 1:27  [ loss=21.1248 ][Training] 25/84 [=======>......................] - ETA: 1:26  [ loss=22.3479 ][Training] 26/84 [========>.....................] - ETA: 1:24  [ loss=20.6431 ][Training] 27/84 [========>.....................] - ETA: 1:22  [ loss=21.2318 ][Training] 28/84 [=========>....................] - ETA: 1:21  [ loss=21.4088 ][Training] 29/84 [=========>....................] - ETA: 1:19  [ loss=21.7893 ][Training] 30/84 [=========>....................] - ETA: 1:17  [ loss=22.0312 ][Training] 31/84 [==========>...................] - ETA: 1:16  [ loss=23.9088 ][Training] 32/84 [==========>...................] - ETA: 1:14  [ loss=19.3211 ][Training] 33/84 [==========>...................] - ETA: 1:13  [ loss=20.7403 ][Training] 34/84 [===========>..................] - ETA: 1:11  [ loss=20.8370 ][Training] 35/84 [===========>..................] - ETA: 1:10  [ loss=20.8988 ][Training] 36/84 [===========>..................] - ETA: 1:08  [ loss=21.0914 ][Training] 37/84 [============>.................] - ETA: 1:07  [ loss=21.5827 ][Training] 38/84 [============>.................] - ETA: 1:05  [ loss=17.9047 ][Training] 39/84 [============>.................] - ETA: 1:04  [ loss=18.1722 ][Training] 40/84 [=============>................] - ETA: 1:03  [ loss=13.8974 ][Training] 41/84 [=============>................] - ETA: 1:01  [ loss=13.8144 ][Training] 42/84 [==============>...............] - ETA: 1:00  [ loss=15.4305 ][Training] 43/84 [==============>...............] - ETA: 58s  [ loss=15.5022 ][Training] 44/84 [==============>...............] - ETA: 57s  [ loss=14.3928 ][Training] 45/84 [===============>..............] - ETA: 55s  [ loss=11.5785 ][Training] 46/84 [===============>..............] - ETA: 54s  [ loss=11.9677 ][Training] 47/84 [===============>..............] - ETA: 53s  [ loss=13.0939 ][Training] 48/84 [================>.............] - ETA: 51s  [ loss=11.6815 ][Training] 49/84 [================>.............] - ETA: 50s  [ loss=12.9446 ][Training] 50/84 [================>.............] - ETA: 48s  [ loss=11.1843 ][Training] 51/84 [=================>............] - ETA: 47s  [ loss=9.0096 ][Training] 52/84 [=================>............] - ETA: 45s  [ loss=10.8951 ][Training] 53/84 [=================>............] - ETA: 44s  [ loss=10.9511 ][Training] 54/84 [==================>...........] - ETA: 42s  [ loss=9.8541 ][Training] 55/84 [==================>...........] - ETA: 41s  [ loss=10.2621 ][Training] 56/84 [===================>..........] - ETA: 39s  [ loss=9.0609 ][Training] 57/84 [===================>..........] - ETA: 38s  [ loss=8.7259 ][Training] 58/84 [===================>..........] - ETA: 37s  [ loss=11.9596 ][Training] 59/84 [====================>.........] - ETA: 35s  [ loss=8.0738 ][Training] 60/84 [====================>.........] - ETA: 34s  [ loss=8.0860 ][Training] 61/84 [====================>.........] - ETA: 32s  [ loss=6.4657 ][Training] 62/84 [=====================>........] - ETA: 31s  [ loss=8.4646 ][Training] 63/84 [=====================>........] - ETA: 29s  [ loss=8.2505 ][Training] 64/84 [=====================>........] - ETA: 28s  [ loss=8.0694 ][Training] 65/84 [======================>.......] - ETA: 26s  [ loss=7.5688 ][Training] 66/84 [======================>.......] - ETA: 25s  [ loss=5.2477 ][Training] 67/84 [======================>.......] - ETA: 24s  [ loss=5.3621 ][Training] 68/84 [=======================>......] - ETA: 22s  [ loss=6.4607 ][Training] 69/84 [=======================>......] - ETA: 21s  [ loss=5.6477 ][Training] 70/84 [========================>.....] - ETA: 19s  [ loss=7.7695 ][Training] 71/84 [========================>.....] - ETA: 18s  [ loss=4.7196 ][Training] 72/84 [========================>.....] - ETA: 16s  [ loss=6.7593 ][Training] 73/84 [=========================>....] - ETA: 15s  [ loss=5.5750 ][Training] 74/84 [=========================>....] - ETA: 14s  [ loss=5.4890 ][Training] 75/84 [=========================>....] - ETA: 12s  [ loss=5.0335 ][Training] 76/84 [==========================>...] - ETA: 11s  [ loss=6.9165 ][Training] 77/84 [==========================>...] - ETA: 9s  [ loss=4.1438 ][Training] 78/84 [==========================>...] - ETA: 8s  [ loss=5.5182 ][Training] 79/84 [===========================>..] - ETA: 7s  [ loss=4.7532 ][Training] 80/84 [===========================>..] - ETA: 5s  [ loss=4.5425 ][Training] 81/84 [===========================>..] - ETA: 4s  [ loss=5.3828 ][Training] 82/84 [============================>.] - ETA: 2s  [ loss=6.6010 ][Training] 83/84 [============================>.] - ETA: 1s  [ loss=5.1254 ][Training] 84/84 [==============================] 1.4s/step  [ loss=3.7950 ]
03/22/2023 11:27:34 - INFO - __main__ -   


Epoch: 1/10
[Training] 1/84 [..............................] - ETA: 3:25  [ loss=3.5703 ][Training] 2/84 [..............................] - ETA: 2:37  [ loss=5.2707 ][Training] 3/84 [>.............................] - ETA: 2:20  [ loss=4.0136 ][Training] 4/84 [>.............................] - ETA: 2:11  [ loss=4.3034 ][Training] 5/84 [>.............................] - ETA: 2:05  [ loss=4.5896 ][Training] 6/84 [=>............................] - ETA: 2:00  [ loss=5.2777 ][Training] 7/84 [=>............................] - ETA: 1:57  [ loss=3.7770 ][Training] 8/84 [=>............................] - ETA: 1:53  [ loss=3.9986 ][Training] 9/84 [==>...........................] - ETA: 1:51  [ loss=3.3737 ][Training] 10/84 [==>...........................] - ETA: 1:48  [ loss=3.8818 ][Training] 11/84 [==>...........................] - ETA: 1:46  [ loss=3.0135 ][Training] 12/84 [===>..........................] - ETA: 1:44  [ loss=3.4608 ][Training] 13/84 [===>..........................] - ETA: 1:42  [ loss=2.5724 ][Training] 14/84 [====>.........................] - ETA: 1:40  [ loss=3.6120 ][Training] 15/84 [====>.........................] - ETA: 1:38  [ loss=3.6476 ][Training] 16/84 [====>.........................] - ETA: 1:36  [ loss=2.5423 ] 
03/22/2023 11:27:57 - INFO - __main__ -   ***** Running evaluation  *****
03/22/2023 11:27:57 - INFO - __main__ -     Num examples = 723
03/22/2023 11:27:57 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/18 [>.............................] - ETA: 31s  [ loss=2.7498 ]batch: (tensor([[  101,  1045,  2293,  ...,     0,     0,     0],
        [  101,  2621,  2003,  ...,     0,     0,     0],
        [  101,  2288,  2000,  ...,     0,     0,     0],
        ...,
        [  101, 11081,  2991,  ...,     0,     0,     0],
        [  101,  2728,  5624,  ...,     0,     0,     0],
        [  101,  2131,  2485,  ...,     0,     0,     0]], device='cuda:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), tensor([[1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0],
        ...,
        [1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), tensor([[[[-2.2430, -2.2430, -2.2430,  ..., -2.1880, -2.1880, -2.1880],
          [-2.2430, -2.2430, -2.2430,  ..., -2.1697, -2.1697, -2.1697],
          [-2.2430, -2.2430, -2.2430,  ..., -2.1514, -2.1514, -2.1514],
          ...,
          [-1.1252, -1.1252, -1.8398,  ..., -2.2430, -2.2430, -2.2430],
          [-0.8869, -1.0152, -1.7299,  ..., -2.2430, -2.2430, -2.2430],
          [-1.4184, -1.4917, -1.9498,  ..., -2.2430, -2.2430, -2.2430]],

         [[-2.4058, -2.4058, -2.4058,  ..., -2.3869, -2.3869, -2.3869],
          [-2.4058, -2.4058, -2.4058,  ..., -2.3869, -2.3869, -2.3869],
          [-2.4058, -2.4058, -2.4058,  ..., -2.3869, -2.3869, -2.3869],
          ...,
          [-1.2123, -1.2312, -2.0080,  ..., -2.4058, -2.4058, -2.4058],
          [-0.9281, -1.0986, -1.8943,  ..., -2.4058, -2.4058, -2.4058],
          [-1.4775, -1.5912, -2.1027,  ..., -2.4058, -2.4058, -2.4058]],

         [[-2.5652, -2.5652, -2.5652,  ..., -2.5273, -2.5273, -2.5273],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5273, -2.5273, -2.5273],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5463, -2.5463, -2.5463],
          ...,
          [-1.2580, -1.2959, -2.0916,  ..., -2.5652, -2.5652, -2.5652],
          [-1.0117, -1.2012, -1.9969,  ..., -2.5652, -2.5652, -2.5652],
          [-1.5801, -1.7127, -2.2242,  ..., -2.5652, -2.5652, -2.5652]]],


        [[[ 0.4508,  0.4691,  0.4508,  ...,  0.3958,  0.4141,  0.4325],
          [ 0.4508,  0.4691,  0.4508,  ...,  0.4141,  0.4141,  0.4141],
          [ 0.5058,  0.4874,  0.4691,  ...,  0.4141,  0.3775,  0.4691],
          ...,
          [-2.0414, -2.0048, -2.0048,  ..., -1.9864, -1.8032, -1.0519],
          [-2.0048, -2.0231, -2.0231,  ..., -1.9864, -1.8398, -1.0335],
          [-1.9864, -2.0414, -2.0414,  ..., -1.9498, -1.8032, -1.0152]],

         [[ 0.3980,  0.4359,  0.4170,  ...,  0.6822,  0.7390,  0.7201],
          [ 0.3601,  0.3791,  0.3791,  ...,  0.7011,  0.7011,  0.6633],
          [ 0.3601,  0.3412,  0.3601,  ...,  0.6822,  0.6633,  0.6633],
          ...,
          [-2.3679, -2.3490, -2.3490,  ..., -2.3300, -2.0269, -1.1365],
          [-2.3679, -2.3490, -2.3490,  ..., -2.3300, -2.0458, -1.1176],
          [-2.3300, -2.3679, -2.3679,  ..., -2.2921, -2.0080, -1.0986]],

         [[ 1.5647,  1.6405,  1.6216,  ...,  1.8300,  1.8489,  1.8489],
          [ 1.5458,  1.5837,  1.5837,  ...,  1.8489,  1.8300,  1.8110],
          [ 1.5647,  1.5837,  1.5837,  ...,  1.8300,  1.7921,  1.8300],
          ...,
          [-2.4326, -2.4326, -2.4326,  ..., -2.3758, -1.6369, -0.0266],
          [-2.4137, -2.4326, -2.4515,  ..., -2.3758, -1.6559,  0.0113],
          [-2.3947, -2.4515, -2.4705,  ..., -2.3379, -1.6180,  0.0302]]],


        [[[ 1.1105,  1.0922,  1.0922,  ...,  0.9639,  0.9456,  0.9272],
          [ 1.1288,  1.1105,  1.0922,  ...,  0.9639,  0.9272,  0.9089],
          [ 1.1105,  1.1288,  1.1105,  ...,  0.9456,  0.9456,  0.9272],
          ...,
          [ 0.5607,  0.4691,  0.4508,  ..., -1.7299, -1.7849, -1.7849],
          [ 0.5974,  0.5241,  0.4508,  ..., -1.7116, -1.7849, -1.8582],
          [ 0.5974,  0.5607,  0.4508,  ..., -1.7299, -1.7849, -1.8215]],

         [[ 1.0800,  1.0611,  1.0800,  ...,  0.6254,  0.6064,  0.6064],
          [ 1.0990,  1.0990,  1.0800,  ...,  0.6254,  0.6064,  0.5875],
          [ 1.0990,  1.0990,  1.0990,  ...,  0.6064,  0.6064,  0.5875],
          ...,
          [ 0.2465,  0.1896,  0.1896,  ..., -1.9132, -1.9132, -1.9511],
          [ 0.3033,  0.2465,  0.1896,  ..., -1.8943, -1.9322, -1.9701],
          [ 0.3223,  0.2844,  0.1896,  ..., -1.9132, -1.9511, -1.9701]],

         [[ 0.7691,  0.7880,  0.8259,  ..., -0.1403, -0.1403, -0.1024],
          [ 0.7880,  0.8070,  0.8259,  ..., -0.1213, -0.1213, -0.1213],
          [ 0.7880,  0.8259,  0.8638,  ..., -0.1024, -0.1024, -0.1213],
          ...,
          [ 0.1249, -0.0077,  0.0113,  ..., -2.0158, -2.0916, -2.1484],
          [ 0.1249,  0.0492,  0.0113,  ..., -1.9779, -2.0727, -2.1295],
          [ 0.0871,  0.1060, -0.0077,  ..., -1.9779, -2.0348, -2.0727]]],


        ...,


        [[[-1.1801, -1.1985, -1.2351,  ..., -1.8582, -1.8948, -1.8948],
          [-1.1801, -1.1985, -1.2351,  ..., -1.8582, -1.8765, -1.8948],
          [-1.2351, -1.2351, -1.2351,  ..., -1.8582, -1.8398, -1.8582],
          ...,
          [-0.5204, -0.4655, -0.5021,  ...,  0.2309,  0.4508,  0.6340],
          [-0.5388, -0.5204, -0.5571,  ...,  0.0476,  0.2492,  0.4325],
          [-0.5388, -0.5204, -0.5388,  ..., -0.1356,  0.0110,  0.1759]],

         [[-1.2691, -1.2881, -1.3259,  ..., -1.8753, -1.8753, -1.8753],
          [-1.2691, -1.2881, -1.3259,  ..., -1.8943, -1.8753, -1.8753],
          [-1.3449, -1.3449, -1.3449,  ..., -1.9132, -1.8943, -1.8943],
          ...,
          [-0.5492, -0.5113, -0.5303,  ...,  0.1328,  0.3791,  0.5685],
          [-0.5492, -0.5492, -0.5871,  ..., -0.0756,  0.1517,  0.3412],
          [-0.5682, -0.5492, -0.5682,  ..., -0.2461, -0.0945,  0.0760]],

         [[-1.4854, -1.4664, -1.4854,  ..., -1.9211, -1.9779, -1.9969],
          [-1.4475, -1.4664, -1.4664,  ..., -1.9400, -1.9590, -1.9969],
          [-1.4664, -1.4854, -1.4664,  ..., -1.9400, -1.9590, -1.9590],
          ...,
          [-0.6329, -0.5760, -0.6139,  ...,  0.1818,  0.3523,  0.5228],
          [-0.6329, -0.6329, -0.6707,  ..., -0.0266,  0.1249,  0.2954],
          [-0.6329, -0.6329, -0.6897,  ..., -0.1782, -0.0835,  0.0681]]],


        [[[-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          ...,
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430]],

         [[-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          ...,
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058]],

         [[-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          ...,
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652]]],


        [[[ 1.9168,  1.9351,  1.9351,  ...,  2.3383,  2.3566,  2.3749],
          [ 1.9351,  1.9351,  1.9535,  ...,  2.3566,  2.3566,  2.3749],
          [ 1.9535,  1.9535,  1.9718,  ...,  2.3566,  2.3566,  2.3749],
          ...,
          [-1.2351, -1.2351, -1.2168,  ...,  1.3121,  1.0189, -0.6304],
          [-0.8136, -0.8320, -0.8503,  ...,  1.2388,  0.7623, -1.1435],
          [-0.5937, -0.5937, -0.6121,  ...,  1.1838,  0.4325, -1.3634]],

         [[ 2.1220,  2.1409,  2.1409,  ...,  2.3872,  2.4062,  2.4251],
          [ 2.1409,  2.1409,  2.1599,  ...,  2.4062,  2.4062,  2.4251],
          [ 2.1599,  2.1599,  2.1788,  ...,  2.4062,  2.4062,  2.4251],
          ...,
          [-1.2881, -1.2881, -1.2691,  ...,  1.3263,  1.0800, -0.5871],
          [-0.7765, -0.7955, -0.8144,  ...,  1.2695,  0.7959, -1.1554],
          [-0.5113, -0.4924, -0.5113,  ...,  1.2127,  0.4359, -1.4207]],

         [[ 1.7731,  1.7921,  1.7921,  ...,  2.0573,  2.0763,  2.0952],
          [ 1.7921,  1.7921,  1.8110,  ...,  2.0763,  2.0763,  2.0952],
          [ 1.8110,  1.8110,  1.8300,  ...,  2.0763,  2.0763,  2.0952],
          ...,
          [-1.4285, -1.4285, -1.4285,  ...,  1.3185,  1.0153, -0.6897],
          [-0.8791, -0.8981, -0.9170,  ...,  1.2806,  0.7501, -1.2580],
          [-0.5760, -0.5571, -0.5760,  ...,  1.2427,  0.3902, -1.5043]]]],
       device='cuda:0'), tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 5, 0,  ..., 0, 0, 0],
        [0, 3, 4,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), tensor([[[[-2.2430e+00, -2.2430e+00, -2.2430e+00,  ..., -2.2247e+00,
           -2.1880e+00, -2.1330e+00],
          [-2.2430e+00, -2.1880e+00, -2.2063e+00,  ..., -2.1880e+00,
           -2.1330e+00, -2.0781e+00],
          [-2.2247e+00, -1.9315e+00, -1.7299e+00,  ..., -2.1330e+00,
           -2.0781e+00, -2.0048e+00],
          ...,
          [-8.8693e-01, -1.0519e+00, -1.0335e+00,  ..., -2.0781e+00,
           -1.9681e+00, -1.9681e+00],
          [-1.8032e+00, -2.0597e+00, -2.0048e+00,  ..., -2.1697e+00,
           -2.2063e+00, -2.1330e+00],
          [-1.8948e+00, -2.0597e+00, -2.1514e+00,  ..., -2.1697e+00,
           -2.2430e+00, -2.2247e+00]],

         [[-2.4058e+00, -2.4058e+00, -2.4058e+00,  ..., -2.4058e+00,
           -2.4058e+00, -2.3869e+00],
          [-2.4058e+00, -2.3490e+00, -2.3679e+00,  ..., -2.3869e+00,
           -2.3869e+00, -2.3869e+00],
          [-2.3869e+00, -2.1216e+00, -1.9132e+00,  ..., -2.3869e+00,
           -2.3869e+00, -2.4058e+00],
          ...,
          [-1.0228e+00, -1.2312e+00, -1.1554e+00,  ..., -2.3490e+00,
           -2.3490e+00, -2.3679e+00],
          [-1.9701e+00, -2.2542e+00, -2.1785e+00,  ..., -2.3869e+00,
           -2.3869e+00, -2.3869e+00],
          [-2.0648e+00, -2.2163e+00, -2.3490e+00,  ..., -2.3869e+00,
           -2.4058e+00, -2.4058e+00]],

         [[-2.5652e+00, -2.5652e+00, -2.5652e+00,  ..., -2.5463e+00,
           -2.5463e+00, -2.5652e+00],
          [-2.5463e+00, -2.4894e+00, -2.5084e+00,  ..., -2.5463e+00,
           -2.5652e+00, -2.5652e+00],
          [-2.5463e+00, -2.2242e+00, -1.9969e+00,  ..., -2.5652e+00,
           -2.5652e+00, -2.5652e+00],
          ...,
          [-1.0496e+00, -1.1633e+00, -1.0875e+00,  ..., -2.5084e+00,
           -2.5084e+00, -2.5084e+00],
          [-2.0537e+00, -2.3379e+00, -2.2621e+00,  ..., -2.5273e+00,
           -2.5463e+00, -2.5084e+00],
          [-2.1295e+00, -2.2810e+00, -2.4894e+00,  ..., -2.5273e+00,
           -2.5652e+00, -2.5463e+00]]],


        [[[ 4.1415e-01,  4.5080e-01,  4.6912e-01,  ...,  4.5080e-01,
            4.5080e-01,  4.1415e-01],
          [ 3.9582e-01,  4.3247e-01,  4.6912e-01,  ...,  4.5080e-01,
            4.1415e-01,  3.7750e-01],
          [ 5.2410e-01,  5.4242e-01,  5.6075e-01,  ...,  5.0577e-01,
            4.6912e-01,  3.9582e-01],
          ...,
          [-1.5650e+00, -1.6932e+00, -1.9864e+00,  ..., -1.7849e+00,
           -1.8582e+00, -1.8948e+00],
          [-1.1618e+00, -1.0885e+00, -1.4184e+00,  ..., -1.9864e+00,
           -1.9681e+00, -1.9681e+00],
          [-2.0231e+00, -1.6016e+00, -9.6023e-01,  ..., -2.0231e+00,
           -1.9864e+00, -1.9315e+00]],

         [[ 3.6014e-01,  3.7909e-01,  3.7909e-01,  ...,  6.4431e-01,
            6.4431e-01,  6.6326e-01],
          [ 4.7381e-01,  4.5486e-01,  3.9803e-01,  ...,  7.0115e-01,
            6.2537e-01,  5.8748e-01],
          [ 7.0115e-01,  6.6326e-01,  6.6326e-01,  ...,  7.9587e-01,
            7.2009e-01,  6.4431e-01],
          ...,
          [-2.3111e+00, -2.2732e+00, -2.3300e+00,  ..., -2.1216e+00,
           -2.1595e+00, -2.1595e+00],
          [-2.3111e+00, -2.2921e+00, -2.3300e+00,  ..., -2.1216e+00,
           -2.2353e+00, -2.2163e+00],
          [-2.3490e+00, -2.3490e+00, -2.3300e+00,  ..., -2.2353e+00,
           -2.2542e+00, -2.1785e+00]],

         [[ 1.5647e+00,  1.5647e+00,  1.6026e+00,  ...,  1.8300e+00,
            1.8300e+00,  1.8300e+00],
          [ 1.6974e+00,  1.6595e+00,  1.6216e+00,  ...,  1.8489e+00,
            1.8110e+00,  1.8110e+00],
          [ 1.8679e+00,  1.8489e+00,  1.8300e+00,  ...,  1.9247e+00,
            1.8679e+00,  1.8300e+00],
          ...,
          [-2.1674e+00, -2.1863e+00, -2.3189e+00,  ..., -2.0158e+00,
           -2.1674e+00, -2.1484e+00],
          [-1.9779e+00, -1.9211e+00, -2.0727e+00,  ..., -1.8453e+00,
           -2.2432e+00, -2.1484e+00],
          [-2.4515e+00, -2.2432e+00, -1.9022e+00,  ..., -2.2053e+00,
           -2.3568e+00, -2.1105e+00]]],


        [[[ 1.0922e+00,  1.1288e+00,  1.1288e+00,  ...,  1.0922e+00,
            1.0372e+00,  9.6390e-01],
          [ 1.1105e+00,  1.1288e+00,  1.1472e+00,  ...,  1.1105e+00,
            1.0555e+00,  9.6390e-01],
          [ 1.1105e+00,  1.1472e+00,  1.1655e+00,  ...,  1.1105e+00,
            1.0555e+00,  9.8222e-01],
          ...,
          [ 5.6075e-01,  5.7907e-01,  5.4242e-01,  ..., -1.8398e+00,
           -1.8582e+00, -1.8582e+00],
          [ 5.0577e-01,  5.4242e-01,  5.4242e-01,  ..., -1.8032e+00,
           -1.8032e+00, -1.8398e+00],
          [ 4.8745e-01,  5.0577e-01,  5.2410e-01,  ..., -1.7849e+00,
           -1.7665e+00, -1.7299e+00]],

         [[ 1.0990e+00,  1.0800e+00,  1.0422e+00,  ...,  7.3904e-01,
            7.0115e-01,  6.2537e-01],
          [ 1.1558e+00,  1.1369e+00,  1.0990e+00,  ...,  7.5798e-01,
            7.0115e-01,  6.2537e-01],
          [ 1.1558e+00,  1.1558e+00,  1.1179e+00,  ...,  7.5798e-01,
            7.0115e-01,  6.4431e-01],
          ...,
          [ 2.4647e-01,  2.4647e-01,  1.5175e-01,  ..., -2.0269e+00,
           -2.0458e+00, -2.0458e+00],
          [ 2.2753e-01,  2.0858e-01,  1.7069e-01,  ..., -1.9890e+00,
           -1.9890e+00, -2.0269e+00],
          [ 2.2753e-01,  1.8964e-01,  1.8964e-01,  ..., -1.9701e+00,
           -1.9322e+00, -1.9322e+00]],

         [[ 8.8273e-01,  8.2590e-01,  7.1223e-01,  ...,  1.1272e-02,
           -4.5562e-02, -1.2134e-01],
          [ 9.7746e-01,  9.0168e-01,  8.0695e-01,  ...,  3.0217e-02,
           -4.5562e-02, -1.2134e-01],
          [ 9.9640e-01,  9.3957e-01,  8.4484e-01,  ...,  1.1272e-02,
           -4.5562e-02, -1.0240e-01],
          ...,
          [ 1.1272e-02,  1.1272e-02, -2.5395e-01,  ..., -2.1295e+00,
           -2.1484e+00, -2.1484e+00],
          [ 1.1272e-02, -7.6726e-03, -1.2134e-01,  ..., -2.0916e+00,
           -2.0916e+00, -2.1295e+00],
          [ 1.1272e-02, -4.5562e-02, -8.3452e-02,  ..., -2.0727e+00,
           -2.0727e+00, -2.0537e+00]]],


        ...,


        [[[-1.3634e+00, -1.7116e+00, -1.7482e+00,  ..., -1.9315e+00,
           -1.9315e+00, -1.8398e+00],
          [-1.3267e+00, -1.7482e+00, -1.7116e+00,  ..., -1.8765e+00,
           -1.8215e+00, -1.6016e+00],
          [-1.0702e+00, -1.5833e+00, -1.5650e+00,  ..., -1.4733e+00,
           -1.2168e+00, -9.7856e-01],
          ...,
          [-8.3196e-01, -1.2168e+00, -1.7665e+00,  ...,  1.0995e-02,
           -4.2881e-01, -9.8955e-02],
          [-8.1363e-01, -9.2358e-01, -1.4000e+00,  ..., -3.3718e-01,
           -6.4871e-01,  2.9320e-02],
          [-4.8378e-01, -6.1206e-01, -8.1363e-01,  ..., -5.2043e-01,
           -6.8536e-01, -4.3980e-02]],

         [[-1.4586e+00, -1.7238e+00, -1.7806e+00,  ..., -1.9511e+00,
           -1.9701e+00, -1.8943e+00],
          [-1.5343e+00, -1.7996e+00, -1.7617e+00,  ..., -1.9322e+00,
           -1.8753e+00, -1.7048e+00],
          [-1.4586e+00, -1.7238e+00, -1.6101e+00,  ..., -1.4964e+00,
           -1.2691e+00, -1.1176e+00],
          ...,
          [-9.4705e-01, -1.3259e+00, -1.7996e+00,  ..., -1.8755e-02,
           -6.6288e-01, -2.0820e-01],
          [-8.1444e-01, -9.4705e-01, -1.3638e+00,  ..., -3.7871e-01,
           -7.3866e-01, -1.8755e-02],
          [-5.1132e-01, -6.2499e-01, -8.3338e-01,  ..., -5.4921e-01,
           -6.4393e-01, -9.4534e-02]],

         [[-1.5801e+00, -1.7506e+00, -1.8074e+00,  ..., -1.9400e+00,
           -2.0158e+00, -1.9590e+00],
          [-1.7127e+00, -1.8453e+00, -1.8074e+00,  ..., -1.9400e+00,
           -1.9779e+00, -1.8453e+00],
          [-1.7127e+00, -1.8264e+00, -1.6938e+00,  ..., -1.4664e+00,
           -1.3528e+00, -1.2770e+00],
          ...,
          [-1.0307e+00, -1.4096e+00, -1.8074e+00,  ...,  2.5755e-01,
           -5.7602e-01, -4.5562e-02],
          [-8.4124e-01, -9.7386e-01, -1.3717e+00,  ..., -1.5923e-01,
           -5.0024e-01,  1.6283e-01],
          [-5.9496e-01, -7.0863e-01, -8.7913e-01,  ..., -5.0024e-01,
           -5.1918e-01,  1.1272e-02]]],


        [[[-2.2430e+00, -2.2430e+00, -2.2430e+00,  ..., -2.2430e+00,
           -2.2430e+00, -2.2430e+00],
          [-2.2430e+00, -2.2430e+00, -2.2430e+00,  ..., -2.2430e+00,
           -2.2430e+00, -2.2430e+00],
          [-2.1514e+00, -2.1147e+00, -2.1514e+00,  ..., -2.2063e+00,
           -2.2247e+00, -2.2063e+00],
          ...,
          [ 1.6236e+00,  1.7152e+00,  1.3304e+00,  ...,  1.3927e-01,
            6.5970e-02,  1.0995e-02],
          [ 1.0005e+00,  1.0372e+00,  1.0372e+00,  ..., -1.5393e-01,
           -2.4556e-01, -2.4556e-01],
          [-1.5650e+00, -1.5466e+00, -1.5466e+00,  ..., -1.7299e+00,
           -1.7665e+00, -1.7849e+00]],

         [[-2.4058e+00, -2.4058e+00, -2.4058e+00,  ..., -2.4058e+00,
           -2.4058e+00, -2.4058e+00],
          [-2.4058e+00, -2.4058e+00, -2.4058e+00,  ..., -2.4058e+00,
           -2.4058e+00, -2.4058e+00],
          [-2.2921e+00, -2.2542e+00, -2.2921e+00,  ..., -2.3490e+00,
           -2.3490e+00, -2.3490e+00],
          ...,
          [ 4.3592e-01,  6.6326e-01, -1.8926e-01,  ..., -1.3828e+00,
           -1.4207e+00, -1.4396e+00],
          [-6.6288e-01, -5.6815e-01, -6.4393e-01,  ..., -1.5343e+00,
           -1.5722e+00, -1.5722e+00],
          [-2.0648e+00, -2.0648e+00, -2.0648e+00,  ..., -2.1974e+00,
           -2.1974e+00, -2.1974e+00]],

         [[-2.5652e+00, -2.5652e+00, -2.5652e+00,  ..., -2.5652e+00,
           -2.5652e+00, -2.5652e+00],
          [-2.5652e+00, -2.5652e+00, -2.5652e+00,  ..., -2.5652e+00,
           -2.5652e+00, -2.5652e+00],
          [-2.5084e+00, -2.4894e+00, -2.5084e+00,  ..., -2.5463e+00,
           -2.5463e+00, -2.5463e+00],
          ...,
          [-1.7818e-01,  8.7051e-02, -8.7913e-01,  ..., -1.9211e+00,
           -1.9400e+00, -1.9400e+00],
          [-1.3717e+00, -1.2580e+00, -1.3528e+00,  ..., -2.0158e+00,
           -2.0348e+00, -2.0348e+00],
          [-2.3379e+00, -2.3379e+00, -2.3379e+00,  ..., -2.4137e+00,
           -2.4326e+00, -2.4515e+00]]],


        [[[ 1.9901e+00,  2.0268e+00,  2.0634e+00,  ...,  2.3200e+00,
            2.3566e+00,  2.3749e+00],
          [ 2.0634e+00,  2.1001e+00,  2.1367e+00,  ...,  2.0084e+00,
            2.2833e+00,  2.3933e+00],
          [ 2.1367e+00,  2.1917e+00,  2.2100e+00,  ...,  9.4557e-01,
            1.7336e+00,  2.3383e+00],
          ...,
          [-1.4000e+00, -1.0885e+00, -1.1068e+00,  ..., -1.1068e+00,
            1.3927e-01,  1.6786e+00],
          [-1.4733e+00, -1.2168e+00, -1.0335e+00,  ..., -7.2201e-01,
            2.1257e-01,  1.6419e+00],
          [-1.2534e+00, -1.0885e+00, -8.6861e-01,  ..., -1.6199e+00,
            2.4922e-01,  1.3854e+00]],

         [[ 2.1788e+00,  2.2167e+00,  2.2546e+00,  ...,  2.4062e+00,
            2.4251e+00,  2.4251e+00],
          [ 2.1978e+00,  2.2357e+00,  2.2546e+00,  ...,  2.0652e+00,
            2.3304e+00,  2.4251e+00],
          [ 2.2357e+00,  2.2546e+00,  2.2736e+00,  ...,  9.4743e-01,
            1.7621e+00,  2.3872e+00],
          ...,
          [-1.5154e+00, -1.1933e+00, -1.1933e+00,  ..., -1.5533e+00,
            1.8947e-04,  1.6105e+00],
          [-1.5912e+00, -1.3259e+00, -1.1176e+00,  ..., -1.1554e+00,
            7.5969e-02,  1.5726e+00],
          [-1.3259e+00, -1.1554e+00, -9.4705e-01,  ..., -1.7806e+00,
            1.5175e-01,  1.3453e+00]],

         [[ 1.8300e+00,  1.8679e+00,  1.9057e+00,  ...,  2.0384e+00,
            2.0573e+00,  2.0763e+00],
          [ 1.8110e+00,  1.8489e+00,  1.8868e+00,  ...,  1.5837e+00,
            1.8868e+00,  2.0573e+00],
          [ 1.7921e+00,  1.8300e+00,  1.8300e+00,  ...,  3.9017e-01,
            1.2616e+00,  1.9626e+00],
          ...,
          [-1.7695e+00, -1.4664e+00, -1.4664e+00,  ..., -1.9779e+00,
           -1.4029e-01,  1.5458e+00],
          [-1.8643e+00, -1.5990e+00, -1.3717e+00,  ..., -1.5611e+00,
           -4.5562e-02,  1.4890e+00],
          [-1.5233e+00, -1.3528e+00, -1.1444e+00,  ..., -1.9590e+00,
            3.0217e-02,  1.2616e+00]]]], device='cuda:0'), tensor([[   1,   26,  117,  ...,    0,    0,    0],
        [   1,  465,   65,  ...,    0,    0,    0],
        [   1,  104,   33,  ...,    0,    0,    0],
        ...,
        [   1,    3,  627,  ...,    0,    0,    0],
        [   1,  654,  193,  ...,    0,    0,    0],
        [   1,  162, 1077,  ...,    0,    0,    0]], device='cuda:0'), tensor([[21],
        [21],
        [11],
        [16],
        [26],
        [24],
        [13],
        [15],
        [21],
        [31],
        [26],
        [13],
        [14],
        [20],
        [14],
        [23],
        [17],
        [13],
        [13],
        [10],
        [33],
        [17],
        [16],
        [15],
        [17],
        [13],
        [ 9],
        [14],
        [17],
        [14],
        [23],
        [11],
        [25],
        [22],
        [21],
        [26],
        [12],
        [15],
        [12],
        [16]], device='cuda:0'), tensor([27, 21, 10, 29, 27, 29, 13, 17, 22, 32, 26, 15, 15, 20, 15, 24, 18, 13,
        16, 10, 35, 18, 23, 21, 17, 13, 11, 18, 18, 15, 22, 11, 27, 24, 24, 28,
        12, 15, 15, 18], device='cuda:0'))
Traceback (most recent call last):
  File "main_ct.py", line 999, in <module>
    main()
  File "main_ct.py", line 802, in main
    args, model, dev_dataset)
  File "main_ct.py", line 255, in evaluate
    elif j == input_lens[i] - 1:
TypeError: unsupported operand type(s) for -: 'list' and 'int'
