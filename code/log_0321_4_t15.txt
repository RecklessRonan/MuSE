nohup: ignoring input
03/21/2023 17:13:38 - INFO - __main__ -   label_list: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[START]', '[END]'], length: 12
The number of samples: 4000
The number of images: 4000
The number of samples: 1000
The number of images: 1000
The number of samples: 3257
The number of images: 3257
03/21/2023 17:13:39 - INFO - root -   Constructing vocabulary for image caption
03/21/2023 17:13:40 - INFO - __main__ -    The size of vocabulary = 4736
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/21/2023 17:13:53 - INFO - __main__ -   Finish loading model [204M] parameters
03/21/2023 17:13:56 - INFO - __main__ -   Args: Namespace(adam_epsilon=1e-08, alpha=0.001, bert_type='uncased', beta=0.001, cls_init=0, crf_dropout=0.5, crf_learning_rate=5e-05, crop_size=224, cross_dropout=0.2, data_dir='../data/twitter2015', device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, drop_last=True, eval_all_checkpoints=False, evaluate_during_training=True, gradient_accumulation_steps=1, hidden_size=768, id2label={0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC', 9: 'X', 10: '[START]', 11: '[END]'}, image_dir='../data/twitter2015_images', image_dropout=0.0, label2id={'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'X': 9, '[START]': 10, '[END]': 11}, learning_rate=5e-05, load_image_checkpoint=False, load_text_checkpoint=False, local_rank=-1, logging_steps=100, markup='bio', max_grad_norm=1.0, max_seq_length=64, n_gpu=1, num_layers=6, num_train_epochs=10, num_workers=8, output_dir='../outputs_ct/twitter15_output/alpha0.001_beta0.001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr5e-5_clr5e-5_uncased_cd0.2_last/', per_gpu_eval_batch_size=40, per_gpu_train_batch_size=40, predict_checkpoints=0, replace_end=3, replace_start=1, resnet_pretrained_dir='../models/resnet152-b121ed2d.pth', save_steps=100, seed=42, sigma=1.0, skip_connection=True, task='twitter15', test_batch_size=1, text_dropout=0.0, theta=0.1, ti_crop_size=32, train_batch_size=40, use_quantile=True, use_xlmr=False, warmup_proportion=0.1, weight_decay=0.01)
03/21/2023 17:13:56 - INFO - __main__ -   Summary dir: ../outputs_ct/twitter15_output/alpha0.001_beta0.001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr5e-5_clr5e-5_uncased_cd0.2_last/summary_1679390036
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
03/21/2023 17:13:56 - INFO - __main__ -   ***** Running training *****
03/21/2023 17:13:56 - INFO - __main__ -     Num examples = 4000
03/21/2023 17:13:56 - INFO - __main__ -     Num Epochs = 10
03/21/2023 17:13:56 - INFO - __main__ -     Gradient Accumulation steps = 1
03/21/2023 17:13:56 - INFO - __main__ -     Total optimization steps = 1000

Epoch: 0/10
/home/ubuntu/multimodal-fusion/MuSE/code/models.py:1235: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
[Training] 1/100 [..............................] - ETA: 3:27  [ loss=115.3319 ][Training] 2/100 [..............................] - ETA: 2:55  [ loss=108.6240 ][Training] 3/100 [..............................] - ETA: 2:41  [ loss=113.0002 ][Training] 4/100 [>.............................] - ETA: 2:33  [ loss=109.9743 ][Training] 5/100 [>.............................] - ETA: 2:27  [ loss=109.1787 ][Training] 6/100 [>.............................] - ETA: 2:23  [ loss=105.4401 ][Training] 7/100 [=>............................] - ETA: 2:20  [ loss=101.9240 ][Training] 8/100 [=>............................] - ETA: 2:17  [ loss=93.2804 ][Training] 9/100 [=>............................] - ETA: 2:16  [ loss=87.1341 ][Training] 10/100 [==>...........................] - ETA: 2:14  [ loss=77.4940 ][Training] 11/100 [==>...........................] - ETA: 2:12  [ loss=72.0663 ][Training] 12/100 [==>...........................] - ETA: 2:10  [ loss=67.5262 ][Training] 13/100 [==>...........................] - ETA: 2:08  [ loss=62.2808 ][Training] 14/100 [===>..........................] - ETA: 2:06  [ loss=53.0197 ][Training] 15/100 [===>..........................] - ETA: 2:05  [ loss=49.3029 ][Training] 16/100 [===>..........................] - ETA: 2:03  [ loss=48.1618 ][Training] 17/100 [====>.........................] - ETA: 2:01  [ loss=47.7842 ][Training] 18/100 [====>.........................] - ETA: 2:00  [ loss=41.3268 ][Training] 19/100 [====>.........................] - ETA: 1:58  [ loss=43.5912 ][Training] 20/100 [=====>........................] - ETA: 1:56  [ loss=45.0553 ][Training] 21/100 [=====>........................] - ETA: 1:55  [ loss=41.4134 ][Training] 22/100 [=====>........................] - ETA: 1:53  [ loss=39.5343 ][Training] 23/100 [=====>........................] - ETA: 1:51  [ loss=40.4780 ][Training] 24/100 [======>.......................] - ETA: 1:50  [ loss=40.1937 ][Training] 25/100 [======>.......................] - ETA: 1:48  [ loss=38.5056 ][Training] 26/100 [======>.......................] - ETA: 1:47  [ loss=36.0937 ][Training] 27/100 [=======>......................] - ETA: 1:45  [ loss=39.3056 ][Training] 28/100 [=======>......................] - ETA: 1:44  [ loss=42.1209 ][Training] 29/100 [=======>......................] - ETA: 1:42  [ loss=35.6082 ][Training] 30/100 [========>.....................] - ETA: 1:41  [ loss=32.0182 ][Training] 31/100 [========>.....................] - ETA: 1:39  [ loss=31.1205 ][Training] 32/100 [========>.....................] - ETA: 1:38  [ loss=29.0107 ][Training] 33/100 [========>.....................] - ETA: 1:36  [ loss=27.0646 ][Training] 34/100 [=========>....................] - ETA: 1:35  [ loss=26.9135 ][Training] 35/100 [=========>....................] - ETA: 1:33  [ loss=22.8310 ][Training] 36/100 [=========>....................] - ETA: 1:32  [ loss=21.7011 ][Training] 37/100 [==========>...................] - ETA: 1:30  [ loss=22.2838 ][Training] 38/100 [==========>...................] - ETA: 1:29  [ loss=22.3299 ][Training] 39/100 [==========>...................] - ETA: 1:27  [ loss=16.9693 ][Training] 40/100 [===========>..................] - ETA: 1:26  [ loss=18.6426 ][Training] 41/100 [===========>..................] - ETA: 1:25  [ loss=15.0949 ][Training] 42/100 [===========>..................] - ETA: 1:23  [ loss=16.9459 ][Training] 43/100 [===========>..................] - ETA: 1:22  [ loss=15.1392 ][Training] 44/100 [============>.................] - ETA: 1:20  [ loss=14.9503 ][Training] 45/100 [============>.................] - ETA: 1:19  [ loss=13.5882 ][Training] 46/100 [============>.................] - ETA: 1:18  [ loss=14.1627 ][Training] 47/100 [=============>................] - ETA: 1:16  [ loss=14.1489 ][Training] 48/100 [=============>................] - ETA: 1:15  [ loss=12.7670 ][Training] 49/100 [=============>................] - ETA: 1:13  [ loss=12.9383 ][Training] 50/100 [==============>...............] - ETA: 1:12  [ loss=13.8064 ][Training] 51/100 [==============>...............] - ETA: 1:11  [ loss=13.1805 ][Training] 52/100 [==============>...............] - ETA: 1:09  [ loss=13.4042 ][Training] 53/100 [==============>...............] - ETA: 1:08  [ loss=12.3126 ][Training] 54/100 [===============>..............] - ETA: 1:06  [ loss=8.9863 ][Training] 55/100 [===============>..............] - ETA: 1:05  [ loss=12.8966 ][Training] 56/100 [===============>..............] - ETA: 1:04  [ loss=10.2317 ][Training] 57/100 [================>.............] - ETA: 1:02  [ loss=9.8446 ][Training] 58/100 [================>.............] - ETA: 1:01  [ loss=8.9895 ][Training] 59/100 [================>.............] - ETA: 59s  [ loss=10.5430 ][Training] 60/100 [=================>............] - ETA: 58s  [ loss=9.4842 ][Training] 61/100 [=================>............] - ETA: 56s  [ loss=9.6008 ][Training] 62/100 [=================>............] - ETA: 55s  [ loss=8.4172 ][Training] 63/100 [=================>............] - ETA: 53s  [ loss=10.1369 ][Training] 64/100 [==================>...........] - ETA: 52s  [ loss=8.4867 ][Training] 65/100 [==================>...........] - ETA: 51s  [ loss=8.7987 ][Training] 66/100 [==================>...........] - ETA: 49s  [ loss=10.6224 ][Training] 67/100 [===================>..........] - ETA: 48s  [ loss=9.2285 ][Training] 68/100 [===================>..........] - ETA: 46s  [ loss=8.6384 ][Training] 69/100 [===================>..........] - ETA: 45s  [ loss=6.8931 ][Training] 70/100 [====================>.........] - ETA: 43s  [ loss=7.1070 ][Training] 71/100 [====================>.........] - ETA: 42s  [ loss=9.0901 ][Training] 72/100 [====================>.........] - ETA: 40s  [ loss=7.3303 ][Training] 73/100 [====================>.........] - ETA: 39s  [ loss=7.5258 ][Training] 74/100 [=====================>........] - ETA: 37s  [ loss=6.1570 ][Training] 75/100 [=====================>........] - ETA: 36s  [ loss=7.0896 ][Training] 76/100 [=====================>........] - ETA: 35s  [ loss=6.2024 ][Training] 77/100 [======================>.......] - ETA: 33s  [ loss=7.3111 ][Training] 78/100 [======================>.......] - ETA: 32s  [ loss=7.5696 ][Training] 79/100 [======================>.......] - ETA: 30s  [ loss=7.2198 ][Training] 80/100 [=======================>......] - ETA: 29s  [ loss=6.9272 ][Training] 81/100 [=======================>......] - ETA: 27s  [ loss=6.9839 ][Training] 82/100 [=======================>......] - ETA: 26s  [ loss=5.3926 ][Training] 83/100 [=======================>......] - ETA: 24s  [ loss=6.5521 ][Training] 84/100 [========================>.....] - ETA: 23s  [ loss=5.2761 ][Training] 85/100 [========================>.....] - ETA: 21s  [ loss=6.3680 ][Training] 86/100 [========================>.....] - ETA: 20s  [ loss=7.2257 ][Training] 87/100 [=========================>....] - ETA: 19s  [ loss=6.3897 ][Training] 88/100 [=========================>....] - ETA: 17s  [ loss=5.4902 ][Training] 89/100 [=========================>....] - ETA: 16s  [ loss=6.1091 ][Training] 90/100 [==========================>...] - ETA: 14s  [ loss=5.9364 ][Training] 91/100 [==========================>...] - ETA: 13s  [ loss=6.7524 ][Training] 92/100 [==========================>...] - ETA: 11s  [ loss=5.8822 ][Training] 93/100 [==========================>...] - ETA: 10s  [ loss=4.6388 ][Training] 94/100 [===========================>..] - ETA: 8s  [ loss=4.4171 ][Training] 95/100 [===========================>..] - ETA: 7s  [ loss=6.8991 ][Training] 96/100 [===========================>..] - ETA: 5s  [ loss=6.2773 ][Training] 97/100 [============================>.] - ETA: 4s  [ loss=4.9154 ][Training] 98/100 [============================>.] - ETA: 2s  [ loss=6.0866 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=6.6893 ][Training] 100/100 [==============================] 1.5s/step  [ loss=6.2462 ]
 
03/21/2023 17:16:23 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 17:16:23 - INFO - __main__ -     Num examples = 1000
03/21/2023 17:16:23 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 39s  [ loss=6.0333 ]batch: (tensor([[  101, 19387,  1030,  ...,     0,     0,     0],
        [  101, 19387,  1030,  ...,     0,     0,     0],
        [  101,  2725,  1037,  ...,     0,     0,     0],
        ...,
        [  101,  5873, 12331,  ...,     0,     0,     0],
        [  101, 19387,  1030,  ...,     0,     0,     0],
        [  101, 19387,  1030,  ...,     0,     0,     0]], device='cuda:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), tensor([[1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0],
        ...,
        [1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0],
        [1, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), tensor([[[[ 1.9718,  1.9718,  1.9718,  ..., -2.0048, -2.0964, -2.2063],
          [ 1.9718,  1.9718,  1.9718,  ..., -1.9864, -2.0964, -2.2063],
          [ 1.9718,  1.9718,  1.9718,  ..., -1.9681, -2.0964, -2.2063],
          ...,
          [-1.1801, -1.1435, -1.1252,  ..., -2.0048, -1.5283, -0.4655],
          [-0.9786, -0.9969, -0.9969,  ..., -2.0048, -1.7482, -1.1985],
          [-0.8869, -0.8869, -0.9053,  ..., -1.7665, -1.8765, -1.7116]],

         [[ 1.9704,  1.9704,  1.9704,  ..., -2.0269, -2.1027, -2.2353],
          [ 1.9704,  1.9704,  1.9704,  ..., -2.0080, -2.1027, -2.2163],
          [ 1.9704,  1.9704,  1.9704,  ..., -2.0080, -2.1216, -2.2353],
          ...,
          [-1.2502, -1.2123, -1.1933,  ..., -2.2163, -2.1027, -2.1406],
          [-1.0418, -1.0607, -1.0607,  ..., -2.1974, -2.1027, -2.1595],
          [-0.9470, -0.9470, -0.9660,  ..., -2.2353, -2.1406, -2.1785]],

         [[ 1.6595,  1.6595,  1.6595,  ..., -1.6748, -1.8264, -1.9779],
          [ 1.6595,  1.6595,  1.6595,  ..., -1.6559, -1.8264, -1.9211],
          [ 1.6595,  1.6595,  1.6595,  ..., -1.6559, -1.8643, -1.9590],
          ...,
          [-1.2770, -1.2391, -1.2201,  ..., -2.0537, -2.0348, -2.2621],
          [-1.0686, -1.0875, -1.0875,  ..., -2.0916, -2.0348, -2.0727],
          [-0.9739, -0.9739, -0.9928,  ..., -2.0537, -2.0348, -2.0158]]],


        [[[-0.3555, -0.4105, -0.6670,  ...,  1.3121,  1.1472,  1.0922],
          [-0.7587, -0.9419, -0.6670,  ...,  1.2388,  0.8723,  1.0739],
          [-0.1539, -1.0702, -0.6121,  ...,  1.4220,  0.6157,  0.7806],
          ...,
          [-2.0414, -2.0414, -2.1330,  ..., -0.4838, -0.4838, -0.3738],
          [-2.2063, -2.2247, -2.2247,  ..., -0.3555, -0.4471, -0.3922],
          [-2.2247, -2.2063, -2.2063,  ..., -0.1723, -0.3372, -0.3922]],

         [[-0.5303, -0.5682, -0.8334,  ...,  1.1748,  0.9853,  0.9474],
          [-0.9470, -1.1176, -0.8334,  ...,  1.0800,  0.7011,  0.9285],
          [-0.3029, -1.2502, -0.7576,  ...,  1.2695,  0.4170,  0.6064],
          ...,
          [-2.1974, -2.1974, -2.2921,  ..., -0.9092, -0.9281, -0.8144],
          [-2.3679, -2.3869, -2.3869,  ..., -0.7955, -0.8902, -0.8334],
          [-2.3869, -2.3679, -2.3679,  ..., -0.6060, -0.7765, -0.8334]],

         [[-0.5760, -0.6329, -0.8981,  ...,  0.9396,  0.7501,  0.6933],
          [-0.9928, -1.1633, -0.8981,  ...,  0.8259,  0.4659,  0.6743],
          [-0.3676, -1.3149, -0.8223,  ...,  1.0153,  0.1818,  0.3712],
          ...,
          [-2.3947, -2.4137, -2.4894,  ..., -1.0686, -1.0875, -0.9739],
          [-2.5273, -2.5463, -2.5652,  ..., -0.9549, -1.0496, -0.9928],
          [-2.5273, -2.5084, -2.5084,  ..., -0.7655, -0.9360, -0.9928]]],


        [[[-0.1539, -0.2456, -0.2272,  ..., -1.4184, -1.4733, -1.4917],
          [-0.2089, -0.3005, -0.3189,  ..., -1.3817, -1.4000, -1.3817],
          [-0.4288, -0.4288, -0.4288,  ..., -1.1618, -1.1985, -0.9419],
          ...,
          [-2.1880, -2.2247, -2.2430,  ..., -1.6932, -1.7116, -1.7116],
          [-1.9864, -2.1514, -2.2247,  ..., -1.6932, -1.7482, -1.7849],
          [-1.8948, -2.0048, -2.0964,  ..., -1.6566, -1.7116, -1.7665]],

         [[-0.8334, -0.7008, -0.3408,  ..., -1.8564, -1.8375, -1.7996],
          [-0.7197, -0.6629, -0.5113,  ..., -1.8753, -1.9132, -1.8564],
          [-0.7576, -0.6818, -0.6629,  ..., -1.7048, -1.8185, -1.5343],
          ...,
          [-2.2732, -2.3111, -2.3300,  ..., -1.5912, -1.6101, -1.6101],
          [-2.0080, -2.1974, -2.2732,  ..., -1.5722, -1.6480, -1.6859],
          [-1.7427, -1.8943, -2.0648,  ..., -1.5722, -1.6291, -1.6859]],

         [[-1.5990, -1.3149, -0.7844,  ..., -2.3189, -2.3568, -2.4137],
          [-1.5611, -1.3906, -1.0496,  ..., -2.4137, -2.4705, -2.5084],
          [-1.6559, -1.4854, -1.3149,  ..., -2.2810, -2.4705, -2.2621],
          ...,
          [-2.4137, -2.4515, -2.4705,  ..., -1.8832, -1.9211, -1.9022],
          [-2.1674, -2.3568, -2.4326,  ..., -1.8832, -1.9590, -1.9969],
          [-1.9211, -2.0727, -2.2242,  ..., -1.8264, -1.8832, -1.9400]]],


        ...,


        [[[-0.9419, -1.1068, -0.8869,  ...,  0.1209,  0.0843,  0.0293],
          [-0.5571, -1.0335, -0.8503,  ...,  0.0293, -0.0073, -0.0623],
          [-0.1906, -0.8503, -0.7953,  ..., -0.0806, -0.1539, -0.3005],
          ...,
          [-1.5283, -1.3451, -1.0152,  ...,  2.0084,  2.0084,  1.9901],
          [-1.5466, -1.3634, -1.0519,  ...,  2.0268,  2.0084,  2.0268],
          [-1.5650, -1.4000, -1.0702,  ...,  2.0268,  2.0084,  2.0084]],

         [[-0.8902, -1.0797, -0.9092,  ...,  0.1517,  0.1139,  0.0570],
          [-0.4355, -0.9660, -0.8334,  ...,  0.0570,  0.0191, -0.0377],
          [-0.0188, -0.7197, -0.7197,  ..., -0.0377, -0.1324, -0.2840],
          ...,
          [-1.5722, -1.3828, -1.0797,  ...,  2.3304,  2.3304,  2.2925],
          [-1.5912, -1.4207, -1.1176,  ...,  2.3493,  2.3304,  2.3115],
          [-1.6101, -1.4586, -1.1365,  ...,  2.3304,  2.3304,  2.2925]],

         [[-0.7465, -0.8981, -0.7086,  ...,  0.2197,  0.1818,  0.1249],
          [-0.3108, -0.8034, -0.6518,  ...,  0.1249,  0.0871,  0.0302],
          [ 0.0871, -0.5950, -0.5760,  ...,  0.0113, -0.0645, -0.2161],
          ...,
          [-1.6180, -1.3906, -1.0307,  ...,  2.2089,  2.2278,  2.2278],
          [-1.6180, -1.3906, -1.0117,  ...,  2.1899,  2.1899,  2.1899],
          [-1.6369, -1.4285, -1.0307,  ...,  2.1710,  2.1899,  2.1710]]],


        [[[ 1.8802,  2.0084,  2.1184,  ...,  1.9351,  1.9351,  1.9351],
          [ 1.7885,  1.8985,  1.9718,  ...,  1.9535,  1.9901,  2.0084],
          [ 1.8985,  1.9168,  1.9351,  ...,  1.9535,  1.9168,  1.9351],
          ...,
          [ 1.8802,  1.8985,  1.9351,  ...,  1.6053,  1.6053,  1.5503],
          [ 1.8435,  1.8985,  1.9535,  ...,  1.6053,  1.5870,  1.4953],
          [ 1.8069,  1.9351,  2.0451,  ...,  1.6603,  1.7152,  1.6969]],

         [[ 1.7999,  1.9326,  2.0462,  ...,  1.8568,  1.8568,  1.8568],
          [ 1.7052,  1.8189,  1.8947,  ...,  1.8757,  1.9136,  1.9326],
          [ 1.8189,  1.8378,  1.8568,  ...,  1.8757,  1.8378,  1.8568],
          ...,
          [ 1.7999,  1.8189,  1.8568,  ...,  1.4779,  1.5158,  1.4589],
          [ 1.7621,  1.8189,  1.8757,  ...,  1.5158,  1.4968,  1.4021],
          [ 1.7242,  1.8568,  1.9704,  ...,  1.5726,  1.6294,  1.6105]],

         [[ 1.4700,  1.6026,  1.7163,  ...,  1.5269,  1.5269,  1.5269],
          [ 1.3753,  1.4890,  1.5647,  ...,  1.5458,  1.5837,  1.6026],
          [ 1.4890,  1.5079,  1.5269,  ...,  1.5458,  1.5079,  1.5269],
          ...,
          [ 1.4700,  1.4890,  1.5269,  ...,  1.1290,  1.1669,  1.1290],
          [ 1.4321,  1.4890,  1.5458,  ...,  1.1858,  1.1669,  1.0722],
          [ 1.3942,  1.5269,  1.6405,  ...,  1.2427,  1.2995,  1.2806]]],


        [[[-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          [-2.2430, -2.2430, -2.2430,  ..., -2.2430, -2.2430, -2.2430],
          ...,
          [-1.6016, -1.6016, -1.6016,  ..., -1.3084, -1.3084, -1.3084],
          [-1.6016, -1.6016, -1.6016,  ..., -1.3267, -1.3267, -1.3267],
          [-1.6383, -1.6383, -1.6383,  ..., -1.3451, -1.3451, -1.3451]],

         [[-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          [-2.4058, -2.4058, -2.4058,  ..., -2.4058, -2.4058, -2.4058],
          ...,
          [-2.1216, -2.1216, -2.1216,  ..., -1.7996, -1.7996, -1.7996],
          [-2.1216, -2.1216, -2.1216,  ..., -1.8185, -1.8185, -1.8185],
          [-2.1595, -2.1595, -2.1595,  ..., -1.8375, -1.8375, -1.8375]],

         [[-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          [-2.5652, -2.5652, -2.5652,  ..., -2.5652, -2.5652, -2.5652],
          ...,
          [-2.3000, -2.3000, -2.3000,  ..., -2.0916, -2.0916, -2.0916],
          [-2.3000, -2.3000, -2.3000,  ..., -2.1105, -2.1105, -2.1105],
          [-2.3379, -2.3379, -2.3379,  ..., -2.1295, -2.1295, -2.1295]]]],
       device='cuda:0'), tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 5,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'), tensor([[[[ 1.9168,  1.6969,  1.8252,  ..., -0.9419, -1.8215, -2.0048],
          [ 1.5137,  1.5686,  1.7702,  ..., -1.3267, -1.9498, -2.1330],
          [-0.3189, -0.0623,  0.2492,  ..., -1.6383, -2.0597, -2.1330],
          ...,
          [-1.6566, -1.7116, -1.8032,  ..., -0.2272, -0.2456, -0.7770],
          [-1.4550, -1.3267, -1.4550,  ..., -0.2272, -0.4471, -0.1539],
          [-1.1618, -0.9236, -0.6121,  ..., -0.3005,  0.0476, -0.6854]],

         [[ 1.8757,  1.6105,  1.7431,  ..., -1.0228, -1.7996, -2.0080],
          [ 1.4589,  1.4779,  1.6863,  ..., -1.3070, -1.8753, -2.0837],
          [-0.3787, -0.1324,  0.1707,  ..., -1.5912, -1.9701, -2.1027],
          ...,
          [-1.7238, -1.7996, -1.9322,  ..., -1.7806, -2.1216, -2.1595],
          [-1.4964, -1.3828, -1.6670,  ..., -1.8753, -2.0458, -2.1595],
          [-1.2312, -0.9849, -0.8902,  ..., -1.8753, -1.9322, -2.1216]],

         [[ 1.5458,  1.2237,  1.3564,  ..., -0.9928, -1.4096, -1.6748],
          [ 1.0532,  1.0532,  1.2806,  ..., -1.0686, -1.4285, -1.7127],
          [-0.7086, -0.5002, -0.2161,  ..., -1.2959, -1.5233, -1.6938],
          ...,
          [-1.7695, -1.9211, -2.1295,  ..., -2.1105, -2.2621, -2.2053],
          [-1.5422, -1.4475, -1.9211,  ..., -2.1295, -2.1484, -2.3000],
          [-1.2580, -0.9928, -1.1065,  ..., -2.0348, -2.1295, -2.2053]]],


        [[[-0.3738, -0.1539, -0.2272,  ...,  0.7806,  1.2571,  1.3121],
          [-0.3189, -0.1173, -0.1723,  ...,  0.1576,  0.5241,  0.7440],
          [-0.4471, -0.3738, -0.3555,  ...,  1.0739,  0.9822,  0.9639],
          ...,
          [-0.6304, -0.6854, -1.1801,  ...,  0.2126,  0.0843,  0.4141],
          [-0.8869, -0.8869, -0.9419,  ...,  0.0293, -0.4471,  0.2675],
          [-1.4550, -1.0702, -1.1618,  ..., -0.2822, -0.9053, -0.2089]],

         [[-0.5303, -0.3029, -0.3977,  ...,  0.6822,  1.1558,  1.1558],
          [-0.4734, -0.2650, -0.3408,  ..., -0.0188,  0.3791,  0.5685],
          [-0.6250, -0.5492, -0.5113,  ...,  0.9664,  0.8527,  0.8148],
          ...,
          [-0.7576, -0.7955, -1.3070,  ..., -0.1135, -0.2082,  0.0381],
          [-1.0039, -1.0039, -1.0797,  ..., -0.3598, -0.7765, -0.0566],
          [-1.5533, -1.1744, -1.2881,  ..., -0.6818, -1.2312, -0.5871]],

         [[-0.5950, -0.3676, -0.4623,  ...,  0.5986,  0.9396,  0.9017],
          [-0.5381, -0.3108, -0.3866,  ..., -0.1403,  0.1818,  0.2954],
          [-0.6707, -0.5950, -0.5760,  ...,  0.7501,  0.5986,  0.5796],
          ...,
          [-0.7844, -0.8223, -1.4285,  ..., -0.3487, -0.5002, -0.2918],
          [-1.0307, -1.0686, -1.1633,  ..., -0.6139, -1.0875, -0.3676],
          [-1.6559, -1.2580, -1.3906,  ..., -0.9928, -1.5990, -0.8223]]],


        [[[-0.7953, -0.7037, -0.5754,  ...,  0.7806, -0.3555,  0.0476],
          [ 0.8356,  0.1393,  0.4508,  ...,  1.1288,  0.5607,  1.2754],
          [ 1.6419,  1.5137,  0.6890,  ...,  0.7257,  1.3121,  1.4770],
          ...,
          [-2.2247, -2.2247, -2.2247,  ..., -0.7770, -2.0597, -1.4733],
          [-2.2247, -2.2063, -2.2063,  ...,  0.1026, -1.2901, -1.5833],
          [-2.2063, -2.1880, -2.2247,  ...,  0.2309, -0.2639, -1.6199]],

         [[-1.1176, -1.1176, -1.0418,  ...,  0.6822, -0.6629, -0.1703],
          [ 0.6064, -0.2082,  0.1328,  ...,  1.0232,  0.4170,  1.2127],
          [ 1.4968,  1.3642,  0.4170,  ...,  0.5496,  1.2884,  1.4968],
          ...,
          [-2.3111, -2.2732, -2.2542,  ..., -0.7008, -2.1027, -1.3070],
          [-2.3111, -2.2542, -2.2921,  ...,  0.2654, -1.1933, -1.4207],
          [-2.2732, -2.2353, -2.2921,  ...,  0.3980, -0.0756, -1.4775]],

         [[-1.6938, -1.8074, -1.7885,  ...,  0.2576, -1.1822, -0.6897],
          [ 0.1249, -0.7086, -0.4055,  ...,  0.5796, -0.0835,  0.6175],
          [ 1.0911,  0.9396, -0.0645,  ...,  0.0492,  0.7691,  0.9775],
          ...,
          [-2.4515, -2.4326, -2.4137,  ..., -0.9928, -2.2810, -1.5611],
          [-2.4515, -2.4137, -2.4326,  ..., -0.1403, -1.4664, -1.6938],
          [-2.4326, -2.3947, -2.4515,  ..., -0.0456, -0.4434, -1.7695]]],


        ...,


        [[[-0.5388, -0.3189,  0.1026,  ...,  0.1942,  0.0293, -0.3372],
          [-0.3738, -0.2089, -0.0257,  ..., -0.1356, -0.4838, -0.5754],
          [ 0.3042, -0.0806, -0.0806,  ..., -0.3922, -0.2822, -0.2272],
          ...,
          [-1.3817, -1.4000, -1.3817,  ...,  2.2283,  2.2100,  2.1734],
          [-1.0152, -1.0519, -0.8869,  ...,  2.2283,  2.1917,  2.1550],
          [-0.9969, -0.9419, -0.4655,  ...,  2.1917,  2.1367,  2.0634]],

         [[-0.4924, -0.2650,  0.2086,  ...,  0.2465,  0.0760, -0.3029],
          [-0.2650, -0.1514,  0.0381,  ..., -0.1324, -0.5113, -0.5871],
          [ 0.4170, -0.0377, -0.0756,  ..., -0.3787, -0.2461, -0.2082],
          ...,
          [-1.5154, -1.5343, -1.5154,  ...,  2.3683,  2.3683,  2.3683],
          [-1.1176, -1.1933, -0.9849,  ...,  2.3683,  2.3683,  2.3683],
          [-1.0228, -0.9470, -0.3977,  ...,  2.3493,  2.3493,  2.3493]],

         [[-0.4055, -0.2350,  0.2197,  ...,  0.2954,  0.1439, -0.2350],
          [-0.1403, -0.0835,  0.0871,  ..., -0.1213, -0.5002, -0.5760],
          [ 0.5417,  0.0681, -0.0077,  ..., -0.3297, -0.2350, -0.1971],
          ...,
          [-1.4854, -1.5233, -1.4854,  ...,  2.2089,  2.2089,  2.2089],
          [-1.1065, -1.1822, -0.9549,  ...,  2.2089,  2.2089,  2.2089],
          [-0.9928, -0.8791, -0.2350,  ...,  2.2278,  2.2089,  2.2089]]],


        [[[ 2.0634,  2.1367,  1.9351,  ...,  1.7336,  1.9168,  1.9351],
          [ 2.1550,  2.1367,  2.0451,  ...,  1.8435,  1.9168,  1.9718],
          [ 2.1917,  2.1367,  2.1367,  ...,  1.9168,  1.9901,  1.9535],
          ...,
          [ 0.8173,  0.8356,  1.0372,  ..., -0.8869, -0.9969, -0.9053],
          [ 1.5686,  1.5686,  1.6603,  ..., -0.5571, -0.3738, -0.3372],
          [ 1.7519,  1.8069,  1.8618,  ...,  0.7623,  0.7257,  0.8173]],

         [[ 1.9894,  2.0652,  1.8568,  ...,  1.6484,  1.8378,  1.8568],
          [ 2.0841,  2.0652,  1.9704,  ...,  1.7621,  1.8378,  1.8947],
          [ 2.1220,  2.0652,  2.0652,  ...,  1.8378,  1.9136,  1.8757],
          ...,
          [ 0.7011,  0.7201,  0.9285,  ..., -1.0228, -1.0986, -1.0418],
          [ 1.4779,  1.4968,  1.5726,  ..., -0.6439, -0.4545, -0.4355],
          [ 1.6673,  1.7242,  1.7810,  ...,  0.7011,  0.6633,  0.7201]],

         [[ 1.6595,  1.7352,  1.5269,  ...,  1.3185,  1.5079,  1.5269],
          [ 1.7542,  1.7352,  1.6405,  ...,  1.4321,  1.5079,  1.5647],
          [ 1.7921,  1.7352,  1.7352,  ...,  1.5079,  1.5837,  1.5458],
          ...,
          [ 0.4091,  0.4281,  0.5986,  ..., -1.2580, -1.3528, -1.2770],
          [ 1.1480,  1.1480,  1.2427,  ..., -0.8981, -0.7086, -0.6707],
          [ 1.3374,  1.3942,  1.4511,  ...,  0.4470,  0.4281,  0.4470]]],


        [[[-2.0414, -1.4000, -1.7116,  ..., -1.6932, -1.6199, -1.8582],
          [-1.6383, -1.4000, -1.4367,  ..., -1.4367, -1.4550, -1.7116],
          [-1.4367, -0.7953, -0.5388,  ..., -0.8320, -0.9602, -1.5833],
          ...,
          [-1.6383, -1.5833, -1.4733,  ..., -0.5388, -0.9786, -1.3084],
          [-1.6199, -1.4733, -1.2901,  ..., -1.2534, -1.2718, -1.3084],
          [-1.6199, -1.6016, -1.5833,  ..., -1.3084, -1.3084, -1.3084]],

         [[-2.1974, -1.5533, -1.8564,  ..., -1.8564, -1.7617, -2.0080],
          [-1.8753, -1.6291, -1.6670,  ..., -1.7048, -1.7048, -1.9890],
          [-1.7238, -1.0418, -0.7765,  ..., -1.1365, -1.2881, -1.9701],
          ...,
          [-2.1216, -2.0458, -1.8753,  ..., -0.8334, -1.3449, -1.7238],
          [-2.1406, -1.8185, -1.4964,  ..., -1.7048, -1.7238, -1.7427],
          [-2.1406, -2.0648, -2.0080,  ..., -1.7996, -1.7996, -1.7806]],

         [[-2.3568, -1.7127, -2.0348,  ..., -2.0158, -1.9211, -2.1674],
          [-2.1863, -1.9400, -1.9590,  ..., -1.9211, -1.9400, -2.2053],
          [-2.0916, -1.3717, -1.0875,  ..., -1.3338, -1.4854, -2.1863],
          ...,
          [-2.3568, -2.2810, -2.2242,  ..., -1.0686, -1.5990, -1.9779],
          [-2.3379, -2.3000, -2.2621,  ..., -1.9969, -2.0158, -2.0348],
          [-2.3189, -2.3000, -2.3000,  ..., -2.0916, -2.0916, -2.0727]]]],
       device='cuda:0'), tensor([[   1,    7,    3,  ...,    0,    0,    0],
        [   1,    7,    3,  ...,    0,    0,    0],
        [   1,  799,   25,  ...,    0,    0,    0],
        ...,
        [   1, 1116, 4493,  ...,    0,    0,    0],
        [   1,    7, 1134,  ...,    0,    0,    0],
        [   1,    7,    3,  ...,    0,    0,    0]], device='cuda:0'), tensor([[19],
        [22],
        [27],
        [19],
        [16],
        [19],
        [14],
        [ 8],
        [21],
        [11],
        [ 7],
        [16],
        [14],
        [20],
        [13],
        [13],
        [23],
        [34],
        [15],
        [20],
        [18],
        [11],
        [22],
        [23],
        [15],
        [15],
        [28],
        [17],
        [22],
        [17],
        [ 6],
        [27],
        [15],
        [15],
        [20],
        [21],
        [14],
        [20],
        [15],
        [23]], device='cuda:0'), tensor([49, 51, 51, 56, 34, 52, 34, 27, 46, 27, 18, 50, 33, 48, 57, 37, 38, 59,
        30, 36, 41, 29, 51, 47, 45, 50, 62, 39, 45, 50, 19, 56, 36, 32, 52, 52,
        39, 32, 46, 51], device='cuda:0'))
Traceback (most recent call last):
  File "main_ct.py", line 999, in <module>
    main()
  File "main_ct.py", line 802, in main
    args, model, dev_dataset)
  File "main_ct.py", line 255, in evaluate
    elif j == input_lens[i] - 1:
TypeError: unsupported operand type(s) for -: 'list' and 'int'
