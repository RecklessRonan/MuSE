nohup: ignoring input
03/21/2023 17:02:57 - INFO - __main__ -   label_list: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'X', '[START]', '[END]'], length: 12
The number of samples: 4000
The number of images: 4000
The number of samples: 1000
The number of images: 1000
The number of samples: 3257
The number of images: 3257
03/21/2023 17:02:57 - INFO - root -   Constructing vocabulary for image caption
03/21/2023 17:02:58 - INFO - __main__ -    The size of vocabulary = 4736
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
03/21/2023 17:03:11 - INFO - __main__ -   Finish loading model [204M] parameters
03/21/2023 17:03:14 - INFO - __main__ -   Args: Namespace(adam_epsilon=1e-08, alpha=0.001, bert_type='uncased', beta=0.001, cls_init=0, crf_dropout=0.5, crf_learning_rate=5e-05, crop_size=224, cross_dropout=0.2, data_dir='../data/twitter2015', device=device(type='cuda'), do_eval=True, do_predict=False, do_train=True, drop_last=True, eval_all_checkpoints=False, evaluate_during_training=True, gradient_accumulation_steps=1, hidden_size=768, id2label={0: 'O', 1: 'B-MISC', 2: 'I-MISC', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC', 9: 'X', 10: '[START]', 11: '[END]'}, image_dir='../data/twitter2015_images', image_dropout=0.0, label2id={'O': 0, 'B-MISC': 1, 'I-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'X': 9, '[START]': 10, '[END]': 11}, learning_rate=5e-05, load_image_checkpoint=False, load_text_checkpoint=False, local_rank=-1, logging_steps=100, markup='bio', max_grad_norm=1.0, max_seq_length=64, n_gpu=1, num_layers=6, num_train_epochs=10, num_workers=8, output_dir='../outputs_ct/twitter15_output/alpha0.001_beta0.001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr5e-5_clr5e-5_uncased_cd0.2_last/', per_gpu_eval_batch_size=40, per_gpu_train_batch_size=40, predict_checkpoints=0, replace_end=3, replace_start=1, resnet_pretrained_dir='../models/resnet152-b121ed2d.pth', save_steps=100, seed=42, sigma=1.0, skip_connection=True, task='twitter15', test_batch_size=1, text_dropout=0.0, theta=0.1, ti_crop_size=32, train_batch_size=40, use_quantile=True, use_xlmr=False, warmup_proportion=0.1, weight_decay=0.01)
03/21/2023 17:03:14 - INFO - __main__ -   Summary dir: ../outputs_ct/twitter15_output/alpha0.001_beta0.001_theta0.1_sigma1.0_rs1_re3_cls0_l6_lr5e-5_clr5e-5_uncased_cd0.2_last/summary_1679389394
/home/ubuntu/.conda/envs/muse/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
03/21/2023 17:03:14 - INFO - __main__ -   ***** Running training *****
03/21/2023 17:03:14 - INFO - __main__ -     Num examples = 4000
03/21/2023 17:03:14 - INFO - __main__ -     Num Epochs = 10
03/21/2023 17:03:14 - INFO - __main__ -     Gradient Accumulation steps = 1
03/21/2023 17:03:14 - INFO - __main__ -     Total optimization steps = 1000

Epoch: 0/10
/home/ubuntu/multimodal-fusion/MuSE/code/models.py:1235: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:333.)
  score = torch.where(mask[i].unsqueeze(1), next_score, score)
[Training] 1/100 [..............................] - ETA: 3:28  [ loss=115.3319 ][Training] 2/100 [..............................] - ETA: 3:07  [ loss=108.6240 ][Training] 3/100 [..............................] - ETA: 2:50  [ loss=113.0002 ][Training] 4/100 [>.............................] - ETA: 2:40  [ loss=109.9743 ][Training] 5/100 [>.............................] - ETA: 2:34  [ loss=109.1787 ][Training] 6/100 [>.............................] - ETA: 2:31  [ loss=105.4402 ][Training] 7/100 [=>............................] - ETA: 2:27  [ loss=101.9241 ][Training] 8/100 [=>............................] - ETA: 2:24  [ loss=93.2805 ][Training] 9/100 [=>............................] - ETA: 2:20  [ loss=87.1344 ][Training] 10/100 [==>...........................] - ETA: 2:18  [ loss=77.4942 ][Training] 11/100 [==>...........................] - ETA: 2:16  [ loss=72.0667 ][Training] 12/100 [==>...........................] - ETA: 2:14  [ loss=67.5267 ][Training] 13/100 [==>...........................] - ETA: 2:11  [ loss=62.2812 ][Training] 14/100 [===>..........................] - ETA: 2:09  [ loss=53.0198 ][Training] 15/100 [===>..........................] - ETA: 2:08  [ loss=49.3032 ][Training] 16/100 [===>..........................] - ETA: 2:06  [ loss=48.1651 ][Training] 17/100 [====>.........................] - ETA: 2:04  [ loss=47.7753 ][Training] 18/100 [====>.........................] - ETA: 2:02  [ loss=41.3070 ][Training] 19/100 [====>.........................] - ETA: 2:01  [ loss=43.5898 ][Training] 20/100 [=====>........................] - ETA: 1:59  [ loss=45.0032 ][Training] 21/100 [=====>........................] - ETA: 1:58  [ loss=41.3788 ][Training] 22/100 [=====>........................] - ETA: 1:56  [ loss=39.4672 ][Training] 23/100 [=====>........................] - ETA: 1:55  [ loss=40.4024 ][Training] 24/100 [======>.......................] - ETA: 1:53  [ loss=40.0729 ][Training] 25/100 [======>.......................] - ETA: 1:51  [ loss=38.3756 ][Training] 26/100 [======>.......................] - ETA: 1:50  [ loss=35.9367 ][Training] 27/100 [=======>......................] - ETA: 1:48  [ loss=39.0805 ][Training] 28/100 [=======>......................] - ETA: 1:47  [ loss=41.8123 ][Training] 29/100 [=======>......................] - ETA: 1:45  [ loss=35.2549 ][Training] 30/100 [========>.....................] - ETA: 1:43  [ loss=31.6345 ][Training] 31/100 [========>.....................] - ETA: 1:42  [ loss=30.6042 ][Training] 32/100 [========>.....................] - ETA: 1:40  [ loss=28.4432 ][Training] 33/100 [========>.....................] - ETA: 1:39  [ loss=26.6065 ][Training] 34/100 [=========>....................] - ETA: 1:37  [ loss=26.4328 ][Training] 35/100 [=========>....................] - ETA: 1:36  [ loss=22.3860 ][Training] 36/100 [=========>....................] - ETA: 1:34  [ loss=21.3178 ][Training] 37/100 [==========>...................] - ETA: 1:33  [ loss=21.9979 ][Training] 38/100 [==========>...................] - ETA: 1:31  [ loss=22.0729 ][Training] 39/100 [==========>...................] - ETA: 1:30  [ loss=16.7792 ][Training] 40/100 [===========>..................] - ETA: 1:28  [ loss=18.4209 ][Training] 41/100 [===========>..................] - ETA: 1:27  [ loss=14.9277 ][Training] 42/100 [===========>..................] - ETA: 1:25  [ loss=16.8457 ][Training] 43/100 [===========>..................] - ETA: 1:24  [ loss=15.0178 ][Training] 44/100 [============>.................] - ETA: 1:22  [ loss=14.8222 ][Training] 45/100 [============>.................] - ETA: 1:21  [ loss=13.5047 ][Training] 46/100 [============>.................] - ETA: 1:19  [ loss=14.0502 ][Training] 47/100 [=============>................] - ETA: 1:18  [ loss=13.9876 ][Training] 48/100 [=============>................] - ETA: 1:16  [ loss=12.6799 ][Training] 49/100 [=============>................] - ETA: 1:15  [ loss=12.9163 ][Training] 50/100 [==============>...............] - ETA: 1:13  [ loss=13.6740 ][Training] 51/100 [==============>...............] - ETA: 1:12  [ loss=13.0183 ][Training] 52/100 [==============>...............] - ETA: 1:10  [ loss=13.2604 ][Training] 53/100 [==============>...............] - ETA: 1:09  [ loss=12.1840 ][Training] 54/100 [===============>..............] - ETA: 1:07  [ loss=8.8745 ][Training] 55/100 [===============>..............] - ETA: 1:06  [ loss=12.6936 ][Training] 56/100 [===============>..............] - ETA: 1:04  [ loss=10.0792 ][Training] 57/100 [================>.............] - ETA: 1:03  [ loss=9.7704 ][Training] 58/100 [================>.............] - ETA: 1:01  [ loss=8.8211 ][Training] 59/100 [================>.............] - ETA: 1:00  [ loss=10.4034 ][Training] 60/100 [=================>............] - ETA: 58s  [ loss=9.3297 ][Training] 61/100 [=================>............] - ETA: 57s  [ loss=9.6055 ][Training] 62/100 [=================>............] - ETA: 56s  [ loss=8.2701 ][Training] 63/100 [=================>............] - ETA: 54s  [ loss=10.1006 ][Training] 64/100 [==================>...........] - ETA: 53s  [ loss=8.2655 ][Training] 65/100 [==================>...........] - ETA: 51s  [ loss=8.7280 ][Training] 66/100 [==================>...........] - ETA: 50s  [ loss=10.7810 ][Training] 67/100 [===================>..........] - ETA: 48s  [ loss=9.1445 ][Training] 68/100 [===================>..........] - ETA: 47s  [ loss=8.5774 ][Training] 69/100 [===================>..........] - ETA: 45s  [ loss=6.9750 ][Training] 70/100 [====================>.........] - ETA: 44s  [ loss=7.0539 ][Training] 71/100 [====================>.........] - ETA: 42s  [ loss=8.9839 ][Training] 72/100 [====================>.........] - ETA: 41s  [ loss=7.3244 ][Training] 73/100 [====================>.........] - ETA: 39s  [ loss=7.4763 ][Training] 74/100 [=====================>........] - ETA: 38s  [ loss=6.3437 ][Training] 75/100 [=====================>........] - ETA: 36s  [ loss=6.9315 ][Training] 76/100 [=====================>........] - ETA: 35s  [ loss=6.1613 ][Training] 77/100 [======================>.......] - ETA: 33s  [ loss=7.4540 ][Training] 78/100 [======================>.......] - ETA: 32s  [ loss=7.5639 ][Training] 79/100 [======================>.......] - ETA: 30s  [ loss=7.1909 ][Training] 80/100 [=======================>......] - ETA: 29s  [ loss=6.7322 ][Training] 81/100 [=======================>......] - ETA: 27s  [ loss=6.9378 ][Training] 82/100 [=======================>......] - ETA: 26s  [ loss=5.3551 ][Training] 83/100 [=======================>......] - ETA: 25s  [ loss=6.6441 ][Training] 84/100 [========================>.....] - ETA: 23s  [ loss=5.1089 ][Training] 85/100 [========================>.....] - ETA: 22s  [ loss=6.2122 ][Training] 86/100 [========================>.....] - ETA: 20s  [ loss=7.1277 ][Training] 87/100 [=========================>....] - ETA: 19s  [ loss=6.1725 ][Training] 88/100 [=========================>....] - ETA: 17s  [ loss=5.4508 ][Training] 89/100 [=========================>....] - ETA: 16s  [ loss=5.8233 ][Training] 90/100 [==========================>...] - ETA: 14s  [ loss=5.8095 ][Training] 91/100 [==========================>...] - ETA: 13s  [ loss=6.6286 ][Training] 92/100 [==========================>...] - ETA: 11s  [ loss=5.7515 ][Training] 93/100 [==========================>...] - ETA: 10s  [ loss=4.5638 ][Training] 94/100 [===========================>..] - ETA: 8s  [ loss=4.4195 ][Training] 95/100 [===========================>..] - ETA: 7s  [ loss=7.0683 ][Training] 96/100 [===========================>..] - ETA: 5s  [ loss=6.2964 ][Training] 97/100 [============================>.] - ETA: 4s  [ loss=4.7820 ][Training] 98/100 [============================>.] - ETA: 2s  [ loss=6.0190 ][Training] 99/100 [============================>.] - ETA: 1s  [ loss=6.0861 ][Training] 100/100 [==============================] 1.5s/step  [ loss=5.7757 ]
 
03/21/2023 17:05:41 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 17:05:41 - INFO - __main__ -     Num examples = 1000
03/21/2023 17:05:41 - INFO - __main__ -     Batch size = 40
[Evaluating] 1/25 [>.............................] - ETA: 39s  [ loss=5.7586 ]Traceback (most recent call last):
  File "main_ct.py", line 998, in <module>
    main()
  File "main_ct.py", line 801, in main
    args, model, dev_dataset)
  File "main_ct.py", line 254, in evaluate
    elif j == input_lens[i] - 1:
TypeError: unsupported operand type(s) for -: 'list' and 'int'
